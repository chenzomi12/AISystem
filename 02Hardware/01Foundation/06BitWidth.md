# 计算之比特位宽

在前面的深度学习计算模式里面我们提到了模型的量化操作，通过建立一种有效的数据映射关系，使得模型以较小的精度损失获得更好的模型执行效率的收益。模型量化的具体操作就是将高比特的数据转换为低比特位宽表示。本节我们将对计算的比特位宽概念做一个更具体的了解。

## 比特位宽的定义

在计算机科学中，整数和浮点数是两种基本的数据类型，它们在计算机中可以用不同长度的比特表示，也就是比特位宽，比特位宽决定了它们的表示范围和数据精度。下面我们一起回顾一下计算机中整数和浮点数的表示定义。

**整数类型**

在计算机中，整数类型的表示通常采用二进制补码形式。二进制补码是一种用来表示有符号整数的方法，它具有以下特点：

- 符号位：整数的最高位（最左边的位）通常用作符号位，0表示正数，1表示负数。
- 数值表示：
  - 对于正数，其二进制补码与其二进制原码相同。
  - 对于负数，其二进制补码是其二进制原码取反（除了符号位，每一位取反，0变为1，1变为0），然后再加1。
- 范围：对于n位比特位宽的整数类型，其表示范围为-2^(n-1)到2^(n-1) - 1，其中有一位用于表示符号位。

举例说明，对于一个8位二进制补码整数：

- 0110 1100 表示正数108。
- 1111 0011 表示负数-13。其二进制原码为1000 1101，取反得到 1111 0010，加1得到补码。

**浮点数类型**

在计算机中，浮点数据类型的表示通常采用IEEE 754标准，该标准定义了两种精度的浮点数表示：单精度和双精度。

一个单精度浮点数通常由32位二进制组成，按照IEEE 754标准的定义，这32位被划分为三个部分：符号位、指数部分和尾数部分。

- 符号位：占用1位，表示数值的正负。
- 指数部分：占用8位，用于表示数值的阶码（数据范围）。
- 尾数部分：占用23位，用于表示数值的有效数字部分（小数精度）。

一个双精度浮点数通常由64位二进制组成，同样按照IEEE 754标准的定义，这64位被划分为三个部分：符号位、指数部分和尾数部分。同单精度不同的是，指数部分位宽是11位，尾数部分位宽是52位。单精度和双精度浮点数的取值范围和精度有所不同，双精度浮点数通常具有更高的精度和更大的取值范围。

IEEE 754标准中浮点数根据指数的值会分为规格化，非规格化和特殊值(零值、无穷大和NaN (Not a Number)，它们的定义如下。规格化浮点数用于表示较大的数值，指数部分至少有一位为1，可以表示更大的数值范围和更高的精度。非规格化浮点数用于表示接近于零的数值，指数部分全为0，可以表示比规格化浮点数更接近于零的小数值。

- 规格化浮点数：指数部分至少有一位为1，且不能全为1。在规格化浮点数中，尾数部分的最高位总是1，并且在存储时通常省略了这个最高位，以节省存储空间。规格化浮点数的指数部分表示了数值的阶码，而尾数部分表示了数值的有效数字。例如，对于单精度浮点数，规格化浮点数的指数范围是1到254。
- 非规格化浮点数：指数部分全为0，尾数部分不全是0。在非规格化浮点数中，尾数部分的最高位不再强制为1，而是可以为0，这样可以表示非常接近于零的数值。非规格化浮点数可以用来增加浮点数的精度范围，但通常会失去一些精度。例如，在单精度浮点数中，当指数部分全为0时，尾数部分的最高位可以为0，这时表示的数值接近于零。
- 特殊值
  - 零值：指数部分和尾数部分都为0，符号位可以为0或1，表示正零或负零，零值的真实值为0。
  - 无穷大：指数部分全为1，尾数部分全为0，符号位分别为0和1。无穷大的真实值为正无穷大和负无穷大。
  - NaN： 指数部分全为1，尾数部分至少有一位为1，符号位可以是任意值。NaN表示浮点数的无效操作或不确定结果，NaN的真实值通常没有实际意义。

![](./images/06BitWidth04.png)

对于规格化和非规格化浮点数，其真实值的计算公式定义如下，其中S是符号位，取指0或1，表示浮点数的正负；E是指数部分，决定了数值的范围；M是尾数部分，决定了小数点的有效位数；E_bw表示指数的位宽；B表示指数的偏移量，B的取值取决于浮点数的指数位宽，比如E的位宽是8， B=2^7-1=127。

$$
\begin{align}
&规格化浮点数： (-1 )^S \times 2^{E-B} \times (1+M)\\
&非规格化浮点数： (-1 )^S \times 2^{1-B} \times M\\
&B=2^{E_{bw}-1}-1
\end{align}
$$

举例说明，对一个单精度浮点数的表示：

```
0 10000011 10100000000000000000000
```

- 符号位：0，S=0，表示正数，(-1)^0=1。
- 指数部分：10000011，E-B=130-127=3，表示指数为2^3=8。
- 尾数部分：10100000000000000000000，表示尾数为1.101，即1.625（二进制转换为十进制）。

所以，这个单精度浮点数表示的数值为8.625。
$$
(-1)^0 \times 2^{130-127} \times (1+0.625)=8.625
$$

## AI模型中的数据类型及应用

在AI模型中常用数据位宽有8bit, 16bit, 32bit，根据不同的应用场景和模型训练推理阶段需求，可以选择不同位宽的数据类型。下图是现有AI模型中出现过的数据类型位宽和定义，可以看到关于浮点数据类型在E和M位宽有很多种设计，比如对同样的16bit和8bit位宽的浮点数，出现了不同的E，M位宽设计，这些数据类型的出现也是AI领域在具体实践应用中，对软件和硬件设计不断优化的表现。

![](images/06BitWidth00.png)

**FP32:** 单精度浮点数格式 ，FP32 是一种广泛使用的数据格式，其可以表示很大的实数范围，足够深度学习训练和推理中使用。每个数据占 4 个字节。

**TF32:** Tensor Float 32 是 Tensor Core 支持的新的数据类型，从 NVIDIA A100 中开始支持。TF32比FP32减少了13位的小数部分精度位宽，所以其峰值计算速度相关 FP32 有了很大的提升。

**FP16:** P16 是一种半精度浮点格式，因为神经网络具有很强的冗余性，降低数据位宽的计算对于模型性能来说影响不大，FP16和FP32混合精度训练模式被大量使用。

**BF16:** FP16 设计时并未考虑深度学习应用，其动态范围太窄。由 Google 开发的 16 位浮点格式称为“Brain Floating Point Format”，简称“bfloat16”，bfloat16 解决了FP16动态范围太窄的问题，提供与 FP32 相同的动态范围。其可以认为是直接将 FP32 的前 16 位截取获得的，在Transformer架构模型中有很好的表现。

**FP8**: FP8是INIDIA的H100 GPU产品中推出的一种8bit位宽浮点数据类型，有E4M3和E5M2两种设计，其中E5M2保持了FP15的数据范围，而E4M3则对数据精度有更好的支持。FP8数据类型可以在保持与FP16/BF16相似的模型精度下，节省模型的内存占用以及提升吞吐量。基于FP8的类型在LLM领域的表现正在被加速探索中。

**Int**: Int数据类型一般在AI模型中的特定应用场景中被使用，比如大量整数类型的任务场景，或者对资源受限的硬件平台进行模型量化，Int8是一个不错的选择。

### FP8数据类型的实践应用

[FP8数据类型](https://arxiv.org/pdf/2209.05433)是近两年才出现的浮点数，随着2022年英伟达H100 GPU产品对其支持的推出，目前已经有了很多的探索应用。下面我们来更具体的了解一下它的定义和应用情况。

FP32， FP16， BF16， FP8 E5M2的浮点数表示都遵循前面提到的IEEE754浮点数据标准，FP8的E4M3则不完全遵循IEEE754标准约定，主要不同在于当指数位全为1时候，去掉了无穷大类型的表示，约定仅当尾数全为1表示NaN，否则仍然来表示规格化数据的值，这样可以增加数据的表示范围。比如当二进制序列为0 1111 110时候，表示的值为1 * 2^8 * 1.75 =448。如果仍完全采用IEEE754标准，支持的最大的数据是240。下表展示了FP8两种类型的数据范围一些细节。

![](images/06BitWidth05.png)

FP8的两种数据类型在神经网络训练的不同部分有不同的用途，在DNN训练过程中，这两种类型都都可能被使用。通常，前向传播的Activations和Weights需要更高的精度，因此在前向传播过程中最好使用E4M3数据类型；在反向传播过程中，通过网络传播的Gradients通常对精度损失不太敏感，但需要更高的动态范围，最好使用E5M2数据格式来存储它们。

AI模型在业界长期依赖于FP16和FP32数据类型的训练，后来BF16数据类型解决了GPT模型计算过程中数据溢出问题也被广泛使用。随着NVIDIA的GPU产品开始推出FP8数据类型，除了NVIDIA本身技术团队对FP8应用的实践与支持，越来越多的机构也开始探索FP8在LLM上的性能表现。比如微软团队在2023年的一篇[论文](https://arxiv.org/pdf/2310.18313)提出一种用于训练 LLM 的 FP8 混合精度框架 FP8-LM，将 FP8 尽可能应用在大模型训练的计算、存储和通信中，使用 H100 训练 GPT-175B 的速度比 BF16 快 64%，节省 42% 的内存占用，给以后大规模LLM模型应用带来了很大的鼓舞。

数据位宽的降低可以带来了更大的吞吐和更高的计算性能，虽然精度有所降低，但是在 LLM 场景下，采用技术和工程手段，FP8 能够提供与更高精度类型相媲美的结果，同时带来显著的性能提升和能效改善，未来肯定会产生更多的[应用实践](https://developer.nvidia.com/zh-cn/blog/fp8-precision-performance/)，以后AI芯片的设计也要考虑对FP8数据类型的支持。

## 为什么要降低比特位宽？

似乎AI模型设计中绕不开对低比特位宽数据的探索，在计算资源有限，成本有限的大环境背景下，这是一个必然的选择。高比特的数据位宽，可以保证模型的精度，但是硬件的计算和存储成本也会更高，而对不同的场景，有不同的模型精度需求，所以需要对不同的场景，设计使用不同精度的数据类型，以降低硬件执行的成本。

降低比特位宽其实就是降低数据的精度，对于AI芯片来说，降低比特位宽可以带来如下好处：

1. 降低MAC的输入和输出数据位宽，能够有效减少数据的搬运和存储开销。更小的内存搬移带来更低的功耗开销
2. 减少 MAC 计算的开销和代价，比如，两个int8数据类型的相乘，累加和使用16bit位宽的寄存器即可，而FP16数据类型的相乘，累加和需要设计32位宽的寄存器。 8bit和16bit计算对硬件电路设计的复杂度影响也很大。

![](./images/06BitWidth01.png)

上面表格展示了降低位宽对芯片的功耗和面积影响程度。左图是对功耗的比较，随着比特位宽的增加，对应乘加操作的能耗在逐渐增加，从SRAM的数据搬移过程是功耗的主要来源；右图是对应芯片面积的比较，可以看到随着数据位宽的增加，需要的芯片面积也在成倍的增加。

针对AI芯片不同阶段的精度需求，市场上已经推出了8-bit的推理芯片产品和16-bit浮点数据的训练芯片产品。比如华为升腾910和NVIDIA的A100。

![](./images/06BitWidth02.png)

## AI芯片设计的思考

结合AI计算模式，通过上面低比特位宽数据的一些理解，可以引发如下方面对AI芯片设计的思考。

1. 当降低位宽的时候，主要对尾数M和动态范围指数E两个值进行调整，这两个值具有不同的影响方向。
   - 尾数M的调整： 对数据的精度产生影响。由于AI模型具有很强的容错性，降低M的比特值对模型精度的影响很小。比如NVIDIA中TensorCore采用的TF32数据类型，具有和FP32相同的动态数据范围E，保持了FP16的尾数精度，总数据位宽从32bit降低到19bit。
   - 指数E的调整： 指数的调整会影响数据的表达范围。在模型训练和推理过程中，不同阶段有不同的数据分布情况，可以根据实际数据的动态范围来动态调整指数 E 的位数，以减少存储空间和计算复杂度。
2. 不同比特位宽的选择时候，需要考虑对模型精度的影响，不同的数据集和不同的任务会有不一样的性能表现。比如NLP和CV的数据集，分类和检测的任务类型，需要进行全面的评测。
3. 训练和推理阶段可以采用不同的数据位宽类型。比如训练阶段采用FP16，BF16，TF32类型；推理阶段CV任务以int8为主，NLP以FP16为主，大模型以int8/FP16混合的方式
4. 权衡硬件的成本开销。额外的数据位宽引入了更多的电路，那么额外的位宽和低比特下损失的模型精度之间需要做一个取舍衡量。

AI计算模式和芯片设计的关系是一个庞大的系统工程，量化算法的落地，低比特的硬件指令设计等软件上的优化在硬件架构设计时候，需要综合考虑很多因素，尤其需要软件和硬件协同合作，从来不是一蹴而就的。

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=483611067&bvid=BV1WT411k724&cid=1054068788&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
