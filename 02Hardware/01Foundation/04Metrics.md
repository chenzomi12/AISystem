# 关键设计指标

前面我们已经对深度学习的计算模式有了初步的认识，那么这些计算模式具体是如何和AI芯片设计结合起来的呢？接下来我们将从AI芯片关键设计指标的角度来进一步拓展对AI计算体系的思考。

## 计算单位

市场上当一款AI芯片产品发布时候，经常会通过一些指标数据说明产品的能力，比如芯片制程，内存大小，核心数，带宽，算力等，这些指标体现了AI产品的核心竞争力。为了帮助理解这些指标，我们先来了解一下AI算法领域常用的计算单位。

**OPS**

OPS，Operations Per Second, 每秒操作数。 1 TOPS 代表处理器每秒进行一万亿次（$10^12$）计算。

OPS/W：每瓦特运算性能。TOPS/W 评价处理器在1W功耗下运算能力的性能指标。

**MACs**

Multiply-Accumulate Operations，乘加累计操作。 1 MACs包含一个乘法操作与一个加法操作，通常1MACs = 2FLOPs。

**FLOPs**

Floating Point Operations, 浮点运算次数，用来衡量模型计算复杂度，常用作神经网络模型速度的间接衡量标准。对于卷积层来说，FLOPs的计算公式如下：

$$
FLOPs = 2 \cdot H \cdot W \cdot C_{in} \cdot  K \cdot K \cdot C_{out}
$$

**MAC**

Memory Access Cost，内存占用量，用来衡量模型在运行时的内存占用情况。对卷积层来说，MAC的计算公式如下：

$$
H_{in} \cdot W_{in} \cdot C_{in}+ H_{out} \cdot W_{out} \cdot C_{out}+K \cdot K \cdot C_{in} \cdot  C_{out}
$$

## AI芯片关键指标

AI芯片设计的目标是低成本高效率的执行AI模型，所以衡量AI芯片的关键指标涉及AI模型软件应用层面的指标和AI芯片硬件市场竞争力指标两个方面，展开如下：

- 精度Accuracy

  在AI芯片中，精度是一个非常关键的指标，它指的是模型在处理任务时输出结果与实际情况之间的接近程度。理解AI芯片的精度指标可以从以下两个角度：

  - 计算精度， 比如支持计算支持的位宽，FP32/FP16等，可以保证多少位宽内的计算结果无误差。
  - 模型效果精度，AI模型不同的任务有不同的模型效果评价标准，比如ImageNet图像识别任务的准确率。回归任务的均方误差等。

- 吞吐量Throughput

  吞吐量指芯片在单位时间内能处理的数据量。对于具有多核心的芯片，可以处理更多并行任务，吞吐量往往更高。在不同的应用场景，对精度和吞吐量的需求是不同的。

- 时延Latency

  AI芯片的时延是指从输入数据传入芯片开始，到输出结果产生的时间间隔。对于需要快速响应的应用场景，如自动驾驶、智能监控等，较低的推理时延是至关重要的。但是AI芯片在执行时候往往是通过应用程序来和用户交互，而在交互应用程序（TTA）中，时延指的是用户输入某个操作或请求后，系统完成相应处理并产生输出结果之间的时间间隔。因此在TTA环境中，时延的影响尤为重要，因为用户通常期望系统能够快速响应他们的操作，以提供流畅的用户体验。优化时延可以通过多方面的手段，包括优化系统架构、加速处理流程、减少网络延迟等，从而提高系统的响应速度和性能表现。

- 能耗Energy

  AI芯片的能耗指的是在执行人工智能任务时芯片所消耗的能量。随着人工智能应用的广泛普及，对于AI芯片的能效和能耗成为了重要关注的焦点之一。

  在AI任务中，通常需要大量的计算资源来执行复杂的算法，例如深度学习模型的训练和推断。因此，AI芯片的能耗通常与其性能密切相关。高性能的AI芯片通常会消耗更多的能量，而低功耗的设计则可以减少能源消耗并延长电池寿命，这对于移动设备和物联网设备等场景尤为重要。

  AI芯片的能耗取决于多个因素，包括芯片架构、制造工艺、工作负载和优化程度等。一些创新的设计和技术可以帮助降低AI芯片的能耗，例如专门针对AI计算任务进行优化的架构、低功耗制造工艺、智能功耗管理等。

  在选择AI芯片时，通常需要权衡性能和能效之间的平衡AI，以满足具体应用场景的需求。对于一些需要长时间运行或依赖于电池供电的设备，低能耗的AI芯片可能更具吸引力，而对于需要高性能计算的场景，则可能更关注芯片的计算能力和性能表现。

- 系统价格System Cost

  价格是市场选择AI产品时的重要考量指标。对搭建一个AI系统来说，要综合考虑硬件成本以及与之相关的系统集成和全栈生态系统的成本。只有综合考虑这些方面，才能更好地评估AI芯片的实际成本和性能表现，从而为实际应用场景做出合适的选择。

  硬件自身价格：这是指AI芯片本身的制造成本，包括芯片设计、制造、封装、测试等环节的费用。硬件自身价格直接影响到芯片的成本效益比，对于消费市场和大规模部署的场景尤为重要。较低的硬件价格可以降低设备制造成本，提高产品的竞争力。

  系统集成上下游全栈等成本：除了硬件本身的成本外，还需要考虑与AI芯片相关的系统集成和全栈生态系统的成本。这包括软件开发、算法优化、系统集成、测试验证、软件支持等方面的成本。在实际应用中，AI芯片往往需要与其他硬件设备、软件系统以及云端服务进行集成，这些集成成本也需要被考虑进来。

- 易用性Flexibility

  一个好的AI芯片产品应该提供完善的软硬件支持、丰富的文档和教程、灵活的编程语言和框架支持，以及便捷的硬件接口和集成支持，从而满足开发者在不同应用场景下的需求，提高开发效率和用户体验。 AI芯片的易用性的具体理解为：

  - 文档和教程：良好的文档和教程能够帮助用户更好地了解AI芯片的特性、功能和使用方法，降低学习成本，提高开发效率。
  - 软件支持和开发工具：一个易于使用的AI芯片应该提供完善的软件开发工具链，包括丰富的API、SDK、开发环境等，使开发者可以快速上手并进行应用程序的开发和调试。

  - 硬件接口和集成支持：AI芯片应该提供标准化的接口和通信协议，便于与其他硬件设备和系统进行集成，从而实现更广泛的应用场景。

  - 性能优化和调试工具：AI芯片应该提供丰富的性能分析和调试工具，帮助开发者对应用程序进行性能优化和故障排查，提高系统的稳定性和可靠性。

## 关键设计点

AI芯片设计的关键点围绕着如何提高吞吐量和降低时延，以及低时延和Batch Size之间权衡。具体的实现策略主要表现在MACs和PE两个方向。

**MACs**

减少MACs：MACs是指在神经网络推理过程中进行的一种常见的计算操作，在AI芯片设计中，去掉没有用的MACs意味着优化计算资源的利用，以提高性能和效率。通过减少网络的MACs，芯片上对应增加稀疏数据的硬件结构，提升控制流和数据传输执行效率，达到节省时钟周期的效果。

降低MAC执行时间：硬件上单次MAC的执行时间和时钟频率和指令开销有关，所以还可以通过增加时钟频率和减少指令开销来降低单次MAC的执行时间。

 **PE**

PE，处理单元（Processing Element），PE是芯片中负责执行计算任务的基本单元，每个处理单元通常包含多个算术逻辑单元（ALU）和寄存器等计算资源，可以并行地执行多个计算任务。PE在神经网络推理和训练中起着至关重要的作用，其数量和性能直接影响着芯片的计算能力和效率。设计高效的处理单元是提升AI芯片性能的重要手段之一。关于PE的优化设计方向有两个方面：

- 增加PE的核心数量。增加PE数量意味着更多的MACs并发，可以通过采用更高纳米制程技术，可以增加单位面积的芯片上的PE密度。

- 增加PE利用率。实际硬件执行中由于指令调度，数据传输通信等一些限制，PE的利用率一般并不高，通过增加PE利用率也能达到提高吞吐量和降低时延的效果。增加PE利用率既包括硬件设计方面的优化，也包括软件算法方面的改进。以下是一些可以考虑的方面：

  1. 并行计算：设计支持高效并行计算的硬件结构，使得多个处理单元能够同时执行计算任务，提高处理单元的利用率。比如使用并行处理器架构、硬件流水线设计等方式。

  2. 负载均衡：在设计深度学习模型时，合理分配计算任务到不同的处理单元上，确保各个处理单元的负载均衡，避免某些处理单元空闲或过载。比如通过动态调度算法和任务分配策略来实现负载均衡。

  3. 数据重用：利用数据重用技术，减少数据在处理单元之间的传输次数，提高数据在处理单元内的重复利用率。比如通过设计高速缓存结构、优化数据存取模式等方式来实现。

## 计算性能仿真

当我们根据关键指标完成了AI芯片的设计之后，不同的AI模型在这个芯片上的执行性能都一样吗？或者怎如何评估AI模型在这款AI芯片上的执行情况？如果一个模型在AI芯片上因为芯片的内部cache空间有限导致性能无法提升，认为该模型属于内存受限模型；如果一个模型在AI芯片上因为芯片的计算单元有限导致性能无法提升，则认为该模型属于算力受限模型。

**算术强度和操作字节比的概念**

一个模型在AI芯片的执行过程大概可以分为三步：从外部存储搬移数据到计算单元，计算算元进行计算，把结果搬回外部存储空间。再精简的说，就是搬移数据和计算这两件事情。对硬件平台AI芯片来说，数据搬移的带宽和计算单元（算力）是固定的值，所以当我们拿到一个AI模型时候，可以根据上面提到的FLOPs和MACs概念，统计出该模型的总FLOPs(浮点运算次数)和总MAC(内存占用量)需求，假设用bytes表指代内存占用量，用ops指代浮点运算次数，用bw表示AI芯片的数据搬移带宽，用π表示AI芯片的PE个数，也就是算力。那么搬移数据的时间是t1= bytes / bw ,  计算的时间是 t2= ops / π 。对AI芯片的执行来说，搬移数据和计算是两件不同的事情，硬件内部进行pipeline流水执行时候，可以认为是并行的过程，所以当 t1> t2时候，模型搬移时间大于计算时间，AI模型在这个AI芯片上最终一定是内存受限的；当t1 < t2 时候，模型搬移时间小于计算时间，AI模型在这个芯片上最终是属于计算受限。

bw和π和AI芯片有关，bytes和ops和AI模型有关，当模型是内存受限时候 , t1 > t2，具体的参数代入，并将不等号两边进行位置调换，那么就变成了下面公式的模样，不等号左侧是AI芯片计算带宽和内存带宽的比值，称为操作字节比，不等号右侧是AI模型的运算次数和内存占用量的比值，称为算术强度（arithmetic intensity）。

$$
t1 > t2 \\
\rightarrow bytes / bw > ops / π \\
\rightarrowπ/bw > ops/bytes
$$

根据算术强度和操作字节比的概念，我们很容易评估出一个AI模型在指定AI芯片上的理论性能情况。下面展示一个具体的示例。

以V100 GPU执行GEMM为例，V100的FP16峰值计算性能是125TFLOPS，片外存储带宽约为900GB/s， 片上L2带宽为3.1TB/s。

- 如果输入数据来自片外存储器，操作字节比约为125/0.9≈138.9
- 如果输入数据来自片上存储器，操作字节比约为125/3.1≈40

对于FP16数据类型，(M, K ,N)形状的一个矩阵乘来说，算术强度为：

$$
\frac{2 \times M \times N \times K}{2\times (M \times K + K \times N + M \times N)}=\frac{M \times N \times K}{(M \times K + K \times N + M \times N)}
$$

当矩阵乘(M, K, N)的值是(8192, 128, 8192)时候，算术强度是124.1，低于V100的操作字节比138.9，该算子操作为内存受限型。

当矩阵乘(M, K, N)的值是(8192, 8192, 8192)时候，算术强度是2730，远高于V100的操作字节比138.9，该算子操作是计算受限型。

**Roofline Model性能评估**

实际上不同模型在特定硬件平台的执行效率情况，可以利用[Roofline Model](https://crd.lbl.gov/assets/Uploads/roofline-intro.pdf)建模进行预估。Roofline model建模是指通过简化硬件计算平台架构，根据计算平台的算力和带宽上限这两个参数和算子的算术强度信息，评估出其能达到的最大性能。如下图所示，横坐标是算子的算术强度，纵坐标表示该算子能达到的最高浮点运算性能，则该算子能达到的最大理论性能公式为：

$$
P = min(peak_{performance}, ops/byte * bw)
$$

当一个算子的算术强度值落在红色区域位置时候，该算子表现为内存受限；而落在绿色区域时候，该算子表现为算力受限。所以最好的情况就是x轴的算术强度值是在红色和绿色交接的那条绿色直线上的时候，该计算平台的带宽和算力得到了一个很好的平衡。

![](./images/04metrics01.png)

通过对AI芯片进行性能仿真可以帮助我们确认性能瓶颈，并激励软件优化。如下图是根据roofline model进行计算性能仿真的[示意图](https://arxiv.org/pdf/1807.07928)，Step1-Step7展示了七种由于软件任务或者硬件设计导致不同性能表现的情况，根据这些表现分析，开发人员通过调整相应的软件策略或者改善硬件设计，来进一步提高计算平台的仿真性能。

- Step1, Step2, 当计算平台的资源没有限制，通过软件最大化配置任务负载或者数据并行策略，来达到最好的执行性能。这时候性能瓶颈在于软件调度策略。

- Step3, Step4, 由于固定的PE 维度或者size, 导致有的PE在任务周期的不是100%被激活。比如有7个计算任务，分给4个PE执行，则需要2个cycle，但是其中有个PE的激活率是50%，这种情况下的计算性能就没有达到峰值性能。

- Step5, 当到计算单元的内存容量有限时候，即使计算所需的数据被很快的送到，也没有足够的地方存放，继而到达一个算力性能瓶颈。

- Step6, Step7, 当计算平台自身提供的带宽有限，即使算力很多，内存空间很多，实际的仿真性能也不能更高了。

![](./images/04metrics02.png)

## 小结

最后，我们再梳理一下AI芯片的关键指标与计算体系之间的一个思考。

1. 精度：AI芯片需要能够处理各类型的无规则数据，以及应对复杂网络模型结构带来的计算冗余性。

2. 吞吐量：除了峰值算力，还需要考虑处理器单元（PE）的平均利用率，实现负载均衡。关注SOTA网络模型的运行时间，基于MLPerf benchmarks评测模型整体性能情况。

3. 时延：AI芯片需要考虑通信时延对MACs的影响，时延越小越好的原则，在Batch Size和内存大小之间达到平衡，可以通过多级缓存设计来降低时延。

4. 能耗：AI芯片在执行SOTA网络模型时需要考虑Ops/W，根据实际部署场景选择降低能耗的程度。此外，还需要降低内存读写功耗 (e.g., DRAM)。

5. 系统价格：AI芯片的系统价格可以通过优化片内多级缓存Cache大小、处理器单元数量、芯片大小和纳米制程等电路设计因素来控制。

6. 易用性：AI芯片需要对主流AI框架（如PyTorch）提供支持，提供丰富的软件栈工具，以便用户更加灵活地使用。

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=438473591&bvid=BV1qL411o7S9&cid=1050719648&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
