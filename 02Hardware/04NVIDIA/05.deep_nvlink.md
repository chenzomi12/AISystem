# NVLink原理剖析

随着人工智能技术的飞速发展，大型模型的参数量已经从亿级跃升至万亿级，这一变化不仅标志着人工智能能力的显著提升，也对支持这些庞大模型训练的底层硬件和网络架构提出了前所未有的挑战。为了有效地训练这些复杂的模型，需要依赖于大规模的GPU服务器集群，它们通过高速网络相互连接，以便进行快速、高效的数据交换。但是，即便是最先进的GPU也可能因为网络瓶颈而无法充分发挥其计算潜力，导致整个算力集群的性能大打折扣。这一现象凸显了在构建大规模GPU集群时，仅仅增加GPU数量并不能线性增加集群的总体算力。相反，随着集群规模的扩大，网络通信的额外开销也会成倍增加，严重影响计算效率。

在这种背景下，算存互连（即计算与存储之间的连接）和算力互连（即计算单元之间的连接）的重要性变得日益突出。这些互连技术是实现高效大规模并行计算的关键，它们确保数据可以迅速在处理单元和存储设备间传输，最大限度地减少通信延迟，提高整体系统性能。

在解决这一挑战的过程中，PCIe (Peripheral Component Interconnect Express) 和NVIDIA的NVLink、NVSwitch等通信技术扮演了重要角色，本文将深度的介绍这几种技术，我们先来简单了解一下。

PCIe是一种高速串行计算机扩展总线标准，广泛应用于连接服务器中的GPU、SSD等设备。它通过提供高带宽和低延迟的数据传输，支持了复杂计算任务的需求。然而，随着计算需求的不断增长，PCIe的带宽可能成为限制因素。

NVIDIA的NVLink技术则为GPU之间提供了更高速度的数据交换能力，其传输速度远超传统的PCIe连接，使得数据在GPU之间的传输更加高效。此外，NVSwitch技术进一步扩展了这种能力，允许多达数十个GPU之间实现高速、高带宽的直接连接。这种先进的互连技术极大地提高了大规模GPU集群处理复杂模型时的数据交换效率，降低了通信延迟，从而使得万亿级别的模型训练成为可能。

## PCIe互联与现代GPU架构

在NVIDIA推出其创新的NVLink和NVSwitch互联技术之前，构建强大计算节点的常规方法是通过PCIe交换机将多个GPU直接连接到CPU，如下图所示。这种配置方式依赖于PCIe标准，尤其是PCIe 3.0版本，它为每个通道提供了大约32GB/s的双向带宽。虽然这在当时被视为高效的数据传输方式，但随着人工智能和机器学习领域的快速发展，数据集和模型的规模呈指数级增长，这种传统的GPU-CPU互联方式很快成为了系统性能提升的瓶颈。

随着新一代GPU性能的显著提升，它们处理数据的能力大幅增强，但如果互联带宽没有相应的提升，那么这些GPU就无法充分发挥其性能潜力。数据传输速度不足意味着GPU在处理完当前数据之前，需要等待下一批数据的到来，这导致了计算效率的显著下降。在这种情况下，即使是最先进的GPU也无法满足日益增长的计算需求，限制了大规模并行计算系统的整体性能。

![PCle互联](images/05.deep_nvlink_01.png)

正是为了解决这一挑战，NVIDIA开发了NVLink技术，它提供了比PCIe 3.0更高的数据传输速率，极大地减少了数据在GPU之间传输的时间。NVLink通过提供更快的数据交换能力，使得多个GPU之间可以更高效地共享数据，从而提高了整体的计算性能和效率。

### 现代GPU架构

![现代GPU架构](images/05.deep_nvlink_02.png)

如上图所示，在现代GPU架构中，单个GPU内部包含了多个流多处理器（SM）核心，这些核心是实现并行计算的基石。通过CUDA（Compute Unified Device Architecture）技术，开发者能够编写程序来驱动这些硬件单元并行执行复杂的计算任务。CUDA不仅为程序员提供了一种高效的方式来利用GPU的并行处理能力，还极大地简化了并行计算程序的开发过程。

而在GPU内部，工作任务被划分并分配给每个图形处理簇（GPC）和流多处理器（SM）核心。这种工作分配机制确保了GPU的计算资源得到充分利用，每个核心都在执行计算任务，从而实现了高效的并行处理。为了支持这种高速计算，GPU通常配备有高带宽内存（HBM2），它为GPC/SM核心提供了快速访问大量数据的能力，从而保证了数据密集型任务的高效执行。

HBM2（High Bandwidth Memory 2）是一种堆叠式内存技术，它通过宽接口和高传输速率显著提升了内存带宽。这对于处理大规模数据集和复杂计算尤为重要，因为它确保了数据能够迅速地供给到每个GPC/SM核心进行处理。此外，GPC/SM核心之间能够共享HBM2中的数据，这一特性使得数据交换更为高效，进一步提升了整体的计算性能。

从上面可以看出，在现代GPU架构中，主要涉及GPU之间的通信和数据交换通常涉及以下几个方面：

![GPU间PCle互联](images/05.deep_nvlink_03.png)

1. PCIe通信：当多个GPU在没有专用高速互连技术（如NVLink）的系统中协同工作时，它们之间的通信通常是通过PCI Express（PCIe）总线进行的。PCIe是一种高速串行计算机扩展总线标准，用于连接主板上的硬件设备。但是，由于PCIe的带宽有限，它可能成为GPU之间高速数据传输的瓶颈。

2. 对HBM2的访问：如果一个GPU需要直接访问另一个GPU的HBM2内存，数据必须通过PCIe总线传输，这会受到PCIe带宽的限制。这种通信方式比GPU内部访问HBM2的速度慢得多，因为PCIe的带宽远低于HBM2的内存带宽。

3. 通过CPU的调度：在没有直接GPU对GPU通信能力的系统中，CPU充当数据交换的中介。CPU负责在多个GPU之间分配和调度计算任务，以及管理数据在GPU和系统内存之间的传输。

这就使得PCIe的带宽限制成为多GPU系统中一个限制因素。特别是当工作负载需要频繁的GPU间通信时，在数据传输密集型的应用中，这种限制可能导致性能下降。

![GPU间Nvlink互联](images/05.deep_nvlink_04.png)

NVLink的出现为GPU间的互联提供了一种革命性的方式，使得不同GPU之间的通信和数据共享变得更加高效和直接。

通过NVLink，GPU的图形处理簇（GPCs）可以直接访问连接在同一系统中其他GPU上的高带宽内存（HBM2）数据。这种直接的内存访问机制显著降低了数据交换的延迟，并提高了数据处理的速度。同时，NVLink支持多条链路同时操作，这意味着可以通过多条NVLink同时对其他GPU内的HBM2数据进行访问，极大地增加了带宽和通信速度。每条NVLink链路都提供了远高于PCIe的数据传输速率，多条链路的组合使得整体带宽得到了成倍增加。

此外，NVLink不仅仅是一种点对点的通信协议，它还可以通过连接到GPU内部的交换机（XBARs）来实现更复杂的连接拓扑。这种能力使得多GPU系统中的每个GPU都能以极高的效率访问其他GPU的资源，包括内存和计算单元。而且，NVLink并不是要取代PCIe，而是作为一种补充和增强。在某些情况下，系统中可能同时使用NVLink和PCIe，其中NVLink用于高速GPU间通信，而PCIe则用于GPU与其他系统组件（如CPU、存储设备）之间的通信。这种设计允许系统根据不同的通信需求灵活选择最合适的技术，从而最大化整体性能和效率。

![多GPU间Nvlink互联](images/05.deep_nvlink_05.png)

如上图所示，NVLink技术的引入不仅仅是为了加速GPU间的通信，它还极大地扩展了多GPU系统的潜力。

1. 多GPU互联能力的提升：NVLink极大地提高了多GPU之间的互联能力，使得更多的GPU可以高效地连接在一起。这种增强的互联能力不仅提升了数据传输的速度和效率，而且还使得构建大规模GPU集群成为可能。在深度学习、科学模拟等领域，这意味着可以处理更复杂的问题，实现更高的计算性能。

2. 单一GPU驱动进程的全局控制：通过NVLink，单个GPU驱动进程可以控制所有GPU的计算任务，实现任务的高效分配和管理。这种集中式控制机制简化了多GPU系统的编程和使用，使得开发者能够更容易地利用系统中所有GPU的计算能力，从而加速复杂计算任务的处理。

3. 无干扰的HBM2内存访问：NVLink还允许GPU在不受其他进程干扰的情况下直接访问其他GPU的HBM2内存。通过使用LD/ST指令和远程直接内存访问（RDMA）技术，数据可以在GPU之间高效地传输，极大地提高了内存访问的速度和效率。这种无干扰的访问机制对于需要大量数据交换的应用至关重要，因为它减少了数据传输的延迟，提高了整体的计算性能。

4. XBAR的独立演进与带宽提升：GPU内部的交换机（XBAR）作为桥接器，可以独立于GPU核心演进发展，提供更高的带宽和更灵活的连接拓扑。这种设计使得NVLink不仅能够支持当前的高性能计算需求，而且还具备了未来进一步扩展和提升性能的潜力。随着XBAR技术的发展，我们可以期待NVLink将会支持更加复杂和高效的多GPU连接方案，进一步推动高性能计算的极限。

## Nvlink结构

为了克服传统PCIe通信带宽的限制，Nvidia开创性地推出了一种名为NVLink的高速互连架构。这项技术首次亮相于P100 GPU中，标志着高性能计算通信技术的一大飞跃。NVLink的设计初衷是为了超越传统的PCIe通道，实现GPU间以及GPU与CPU之间更高效率、更高带宽的数据传输。

NVLink的引入不仅仅是技术上的创新，它还代表了Nvidia对未来计算架构的深远考量。与PCIe相比，NVLink提供了显著更高的通信带宽和更低的延迟，这对于数据密集型的应用，如深度学习、科学计算和大规模模拟等领域，意味着巨大的性能提升。

值得一提的是，NVLink的设计也考虑到了CPU与GPU之间的高带宽通信需求。这一点尤其重要，因为它为异构计算提供了更加紧密和高效的集成方式。虽然基于x86架构的AMD和Intel可能不会直接采用NVLink，但Nvidia与IBM的合作展现了NVLink技术在非x86架构中的巨大潜力。通过在IBM的POWER微处理器上实现NVLink，展示了一种全新的、去除PCIe瓶颈的通信方式，为高性能计算系统提供了更加高效的数据交换路径。

### 第一代Nvlink结构详析

第一代NVLink技术采用了一种精巧的设计，每条NVLink是由一对双工双路信道组成，通过巧妙地将32条配线组合起来，形成了8对不同的配对。这种独特的结构使得每个方向上能够实现高效的数据传输，具体来说，就是通过2位双向传输（2bi）乘以8对配对（8pair）再乘以2条线（2wire），最终形成了32条线（32wire）的配置。

![第一代Nvlink结构详析](images/05.deep_nvlink_06.png)

如上图所示，在P100 GPU上，Nvidia搭载了4条这样的NVLink通道，每条通道能够提供双向总共40GB/s的带宽。这意味着，整个P100芯片能够达到惊人的160GB/s的总带宽，为数据密集型的应用提供了强大的数据处理能力。

通过这种技术，不仅解决了传统PCIe通信带宽瓶颈的问题，而且还为GPU之间以及GPU与CPU之间的通信提供了一条更快、更高效的数据传输路径。为处理更加复杂的计算任务，构建更加强大的计算系统铺平了道路。

下面我们来解析下Nvlink连接的技术细节：

![Nvlink连接](images/05.deep_nvlink_07.png)

首先，Nvidia的P100 GPU在其设计中融入了四条NVLink通道，这一创新不仅提升了数据传输的速度，还极大地增强了系统的整体性能。P100通过这些高速通道，实现了高达94%的带宽效率，这一数字在当时是非常令人印象深刻的，它意味着几乎所有的数据传输都能以极高的效率完成，极大地减少了数据在传输过程中的损耗。

其次，更为重要的是，NVLink不仅支持GPU之间的数据读写操作，还支持原子操作到对等GPU，这为复杂的计算任务和数据处理提供了更加灵活和强大的支持。此外，P100还能够通过NVLink与支持NVLink的CPU进行数据读写操作，这一特性极大地提升了CPU与GPU之间的协同工作效率，为异构计算环境中的数据共享和任务协调提供了更加高效的解决方案。

最后，NVLink的另一个显著特点是其链接可以被“捆绑”起来以实现更高的带宽。通过将多条NVLink通道组合使用，可以进一步提升数据传输速度，满足那些对数据传输速度有极高要求的应用场景。这种灵活的配置方式，使得P100能够根据具体的应用需求和工作负载，动态调整数据传输策略，从而优化性能表现。

我们再深入Nvlink协议的细节中进行分析。

![Nvlink连接架构](images/05.deep_nvlink_08.png)

在NVLink的链接架构中，一个关键的概念是“Brick”，它指的是NVLink通道的基本单元。如上图所示，每个NVLink是一个双向接口，由8个差分对组成，总计32条线。这些差分对采用直流耦合（DC coupled）技术，并配置有85欧姆的差分终端，以优化信号传输质量。

同时为了进一步提高设计的灵活性和兼容性，NVLink引入了通道反转（Channel Reversal）和通道极性（Channel Polarity）的概念。这意味着，设备间的物理通道顺序和极性可以根据实际布线和设计需要进行调整，从而简化了物理布局和路由的复杂度。

![Nvlink数据包](images/05.deep_nvlink_09.png)

如上图所示，在数据传输方面，NVLink采用了基于flit（flow control digit）的数据包结构。一个单向的NVLink数据包可以包含1到18个flit，每个flit包含128位。这种设计允许在单个数据包中传输不同大小的数据，从而提高了传输的灵活性和效率。例如，一个包含1个头部flit（header flit）加上16个数据有效载荷flit（Data payload flit）的数据包可以实现单向256字节的传输，达到94.12%的峰值带宽利用率。而一个包含1个头部flit加上4个数据有效载荷flit的数据包，则可以实现单向64字节的传输，带宽利用率为80%。

头部flit的结构设计包含了25位的循环冗余校验（CRC）、83位的传输字段（Transaction field）和20位的数据链路字段（Data link）。传输字段中包括了请求类型、地址、流量控制位和标记标识符等信息，而数据链路字段则涉及到数据包长度、应用编号标签和确认标识符等内容。地址扩展（Address Extension）机制通过保留静态位，仅传输变化位来进一步优化了数据传输效率。

NVLink协议通过25位CRC实现了错误检测，确保了数据传输的可靠性。接收方（Receiver）负责将接收到的数据保存在重播缓冲区（Replay buffer）中，对数据包进行排序，并在确认CRC无误后将数据发送回源端。CRC字段的设计使得最大数据包能够容忍高达5个随机位错误，或者在差分对发生突发错误时，支持最多25个连续位错误的容错能力。

通过这种精心设计的电气和数据结构，NVLink不仅提高了数据传输的速度和效率，还确保了在高速传输过程中的稳定性和可靠性，为高性能计算提供了强有力的支持。

## Nvlink拓扑

为了实现GPU间的高效链接和协作计算，就需要基于Nvlink系统配置和性能成本要求，来合理的配置GPU之间的NVLink通道的物理布局和连接方式。

![Nvlink拓扑](images/05.deep_nvlink_11.png)

初代DGX-1通常采用了一种类似于上图的互联形式。不过，IBM在其基于Power8+微架构的Power处理器上引入了NVLink 1.0技术，这使得NVIDIA的P100 GPU可以直接通过NVLink与CPU相连，而无需经过PCIe总线。这一举措实现了GPU与CPU之间的高速、低延迟的直接通信，为深度学习和高性能计算提供了更强大的性能和效率。

通过与最近的Power8+ CPU相连，每个节点的4个GPU可以配置成一种全连接的mesh结构。这种结构使得GPU之间可以直接交换数据，并在深度学习和计算密集型任务中实现更高效的数据传输和协作计算。

![DGX1 Nvlink拓扑](images/05.deep_nvlink_12.png)

如上图所示，DGX-1集成了八块P100 GPU和两块志强E5 2698v4处理器。然而，由于每块GPU只有4条NVLink通道，因此GPU形成了一种混合的cube-mesh网络拓扑结构。在这种结构中，GPU被分成两组，每组四块GPU，并且在组内形成了一个mesh结构，在组间形成了一个cube结构。

![P100 Nvlink拓扑](images/05.deep_nvlink_13.png)

此外，由于GPU所需的PCIe通道数量超过了芯片组所能提供的数量，因此每一对GPU将连接到一组PCIe交换机上，然后再与志强处理器相连，如上图所示。随后，两块Intel处理器通过QPI总线相连。

这种配置确保了每个GPU都能获得足够的PCIe带宽，以便在深度学习和高性能计算任务中能够高效地进行数据传输和处理。同时，通过QPI总线连接的两块Intel处理器也为系统提供了高速的CPU之间通信通道，进一步提升了整个系统的性能和效率。

## 小结

随着深度学习模型的日益庞大，大规模并行计算集群中的数据传输效率成为了提升系统性能的关键因素。在这一背景下，算存互连和算力互连技术的作用变得尤为突出，它们对于降低通信延迟、增强整体计算性能至关重要。

尽管PCIe作为一项成熟的高速串行计算机扩展总线标准，在早期为GPU与CPU之间的数据传输提供了有效的解决方案，但其有限的带宽逐渐成为了制约性能进一步提升的瓶颈。

为突破这一限制，NVIDIA推出了创新的NVLink技术，该技术显著提高了GPU之间以及GPU与CPU之间的数据传输速率。本文深入探讨了NVLink的架构设计和工作原理，阐明了其如何通过实现高带宽、低延迟的数据传输，大幅提升了多GPU系统的计算效率。此外，文中还详细分析了初代NVLink的网络拓扑结构，特别是在高性能计算平台如DGX-1系统中，如何通过构建GPU间的全连接mesh结构，以及通过NVLink实现与CPU的高速、低延迟直接通信。这些设计不仅优化了数据传输的速度和效率，还确保了在高速传输过程中的稳定性与可靠性。

随着技术的进步，NVLink已发展至Blackwell架构的第五代。在接下来的章节中，我们将继续深入分析NVLink技术的发展历程，以及NVSwitch技术的迭代演进，进一步揭示这些尖端技术如何助力构建更高效、更强大的计算系统，推动人工智能和机器学习领域的未来发展。


## 本节视频

<html>
<iframe src="https://www.bilibili.com/video/BV1uP411X7Dr/?spm_id_from=333.337.search-card.all.click&vd_source=997b612028a4d9f90d4179eb93284d60" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
