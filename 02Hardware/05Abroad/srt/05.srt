1
00:00:06,900 --> 00:00:08,500
人生得意须尽欢

2
00:00:17,900 --> 00:00:19,100
特别是 AI 系列里面呢

3
00:00:19,100 --> 00:00:20,500
我们现在已经来到了

4
00:00:22,300 --> 00:00:23,600
谷歌 TPU 系列

5
00:00:23,700 --> 00:00:25,266
在谷歌 TPU 系列里面呢

6
00:00:30,000 --> 00:00:32,100
就离不开谷歌最重要的一个

7
00:00:32,100 --> 00:00:34,000
不管是 TPU1234 啊

8
00:00:34,266 --> 00:00:36,866
它里面的最核心的计算就是脉动阵列

9
00:00:36,900 --> 00:00:39,366
所以我们呢在谷歌 TPU 第一代里面呢

10
00:00:39,366 --> 00:00:43,666
详细的去展开脉动阵列的具体的细节

11
00:01:08,200 --> 00:01:09,900
从 Alpha Go 的第一代

12
00:01:10,000 --> 00:01:14,100
跟韩国的李世石和中国的柯洁去对战

13
00:01:18,366 --> 00:01:20,566
非常非常重要的一个应用

14
00:01:21,900 --> 00:01:23,200
在这些应用背后呢

15
00:01:23,200 --> 00:01:26,800
都离不开谷歌 TPU 第一代芯片的架构

16
00:01:33,500 --> 00:01:36,866
来到了 TPU 第一代的芯片的架构呢

17
00:01:36,866 --> 00:01:37,866
有点花花绿绿

18
00:01:48,966 --> 00:01:49,466
这里面

19
00:02:09,466 --> 00:02:12,066
虽然你叫它 mmumxu 也行啊

20
00:02:12,166 --> 00:02:15,366
在谷歌后面官网的宣称呢都会叫它 MXU

21
00:02:22,166 --> 00:02:25,333
每个时钟周期呢可以计算 256 个乘法

22
00:02:25,466 --> 00:02:29,133
计算的结果呢是一个半精度 FP16

23
00:02:36,866 --> 00:02:39,466
还有一个双缓存的单元

24
00:02:46,466 --> 00:02:47,933
回调的一个作用

25
00:02:49,000 --> 00:02:49,466
接下来呢

26
00:02:49,466 --> 00:02:52,866
我们看下一个内容就是 accumulators

27
00:02:54,700 --> 00:02:58,566
主要是用来收集刚才 MXU 计算之后

28
00:03:04,266 --> 00:03:05,666
实际呢这里面的计算呢

29
00:03:05,666 --> 00:03:06,866
还是很有意思

30
00:03:12,100 --> 00:03:13,500
而整个 MXU 里面去执行

31
00:03:21,466 --> 00:03:24,333
最后呢再执行 32 比特的累加

32
00:03:26,100 --> 00:03:28,300
activation 进行累加

33
00:03:28,366 --> 00:03:31,166
整体的运算呢就变成 4096*256*32b

34
00:03:42,700 --> 00:03:44,566
都会有控制的模块

35
00:03:46,900 --> 00:03:48,000
在区里面呢

36
00:03:48,066 --> 00:03:49,766
分开四级的流水

37
00:03:50,166 --> 00:03:51,066
控制单元呢

38
00:03:52,666 --> 00:03:53,566
那这些指令呢

39
00:03:53,600 --> 00:03:55,500
是通过 PCIe 的总线

40
00:03:59,666 --> 00:04:01,766
传到芯片里面

41
00:04:13,766 --> 00:04:16,333
也就是复杂指令集

42
00:04:18,600 --> 00:04:20,700
哎小新就有疑问了

43
00:04:23,900 --> 00:04:25,900
但是却用 CSIC 指令集呢

44
00:04:41,500 --> 00:04:42,566
所以这条指令呢

45
00:04:42,566 --> 00:04:44,166
就变得非常的复杂

46
00:04:44,166 --> 00:04:46,366
于是呢就使用了 CISC 指令

47
00:04:48,266 --> 00:04:50,866
而为了去控制里面的各种各样的单元

48
00:04:50,900 --> 00:04:52,066
特别是 MXU 啊

49
00:04:52,066 --> 00:04:54,333
UB 啊还有 AU 等模块呢

50
00:04:54,366 --> 00:04:55,866
里面呢就定义了十几个

51
00:04:55,866 --> 00:04:57,166
专门为神经网络推理

52
00:04:58,700 --> 00:04:59,566
都比较长

53
00:05:14,366 --> 00:05:15,966
主要是读数据

54
00:05:25,800 --> 00:05:27,500
所以 ZOMI 觉得这 5 条指令呢

55
00:05:35,366 --> 00:05:38,066
可以去这条链接里面去了解

56
00:05:38,100 --> 00:05:40,700
谷歌 TPU 里面的 instruction set Architecture

57
00:05:45,100 --> 00:05:47,700
我们看一下它的整体的芯片的布局啊

58
00:05:53,400 --> 00:05:55,500
就是 Local unified buffer

59
00:06:05,000 --> 00:06:08,166
其实只有 2%的芯片电路面积

60
00:06:10,766 --> 00:06:12,466
留下了更多的空间

61
00:06:12,500 --> 00:06:15,500
那这个呢就是 TPU 的芯片的布局了

62
00:06:19,500 --> 00:06:21,200
现在已经到凌晨 1 点钟了

63
00:06:24,500 --> 00:06:26,800
最重要的内容了

64
00:06:26,800 --> 00:06:30,000
那脉动阵列的英文叫做 Systolic array

65
00:06:38,166 --> 00:06:40,466
Img2col 的这种方式

66
00:06:40,466 --> 00:06:41,066
实际上呢

67
00:06:41,066 --> 00:06:42,066
我们知道啊

68
00:06:54,500 --> 00:06:56,966
或者对 feature map 进行卷积

69
00:06:56,966 --> 00:06:58,366
而是通过 Img2col

70
00:06:58,400 --> 00:07:00,466
的方式把卷积呢

71
00:07:02,766 --> 00:07:03,266
那 ZOMI 呢

72
00:07:08,766 --> 00:07:10,166
最终把原始的卷积

73
00:07:30,866 --> 00:07:32,766
那矩阵乘的就是一行乘以一列

74
00:07:34,066 --> 00:07:36,133
那大家看到这个内容没有

75
00:07:38,566 --> 00:07:40,733
我们把矩阵里面的每一行

76
00:07:40,800 --> 00:07:43,900
跟矩阵里面的每一列的方式拿出来

77
00:07:45,000 --> 00:07:46,866
每一次相乘相加

78
00:07:57,300 --> 00:07:59,000
那面向这种计算的结构呢

79
00:08:01,200 --> 00:08:03,100
可不可以用 1987 年发明

80
00:08:03,100 --> 00:08:04,566
脉冲阵列的方式呢

81
00:08:04,666 --> 00:08:07,566
去提供具体的矩阵的运算呢

82
00:08:07,600 --> 00:08:08,200
那于是呢

83
00:08:08,200 --> 00:08:10,500
这里面呢就引入了脉动阵列

84
00:08:11,700 --> 00:08:13,566
对数据呢进行计算

85
00:08:14,400 --> 00:08:15,066
控制模块呢

86
00:08:15,066 --> 00:08:15,766
把数据呢

87
00:08:15,800 --> 00:08:17,700
一波一波根据 FIFO 队列

88
00:08:25,300 --> 00:08:26,166
进行累加

89
00:08:28,266 --> 00:08:30,566
就像心脏的脉动血液一样啊

90
00:08:30,566 --> 00:08:32,333
每次时钟周期执行一次

91
00:08:46,100 --> 00:08:47,100
每次运算的结果呢

92
00:08:47,100 --> 00:08:47,900
都像脉动一样

93
00:08:47,900 --> 00:08:50,266
把上一次计算的结果呢缓存起来

94
00:08:54,666 --> 00:08:57,066
也是谷歌引入的概念了

95
00:09:01,266 --> 00:09:02,666
虽然 PPT 呢有点旧

96
00:09:05,200 --> 00:09:06,300
首先我们看到啊

97
00:09:06,300 --> 00:09:07,666
左边的这个图呢

98
00:09:08,966 --> 00:09:10,766
m 就是计算器 memory

99
00:09:18,700 --> 00:09:19,100
算一次

100
00:09:19,100 --> 00:09:21,400
从计算机里面储一次数据

101
00:09:25,566 --> 00:09:27,333
那脉动阵列的发明的时候呢

102
00:09:27,866 --> 00:09:30,066
我们数据可不可以不断不断地累加

103
00:09:34,400 --> 00:09:35,466
给下次 PE

104
00:09:35,500 --> 00:09:36,900
把下次 PE 的计算结果呢

105
00:09:38,300 --> 00:09:39,500
最终计算完之后呢

106
00:09:41,900 --> 00:09:42,900
通过这种方式呢

107
00:09:44,866 --> 00:09:45,533
那当时呢

108
00:09:47,066 --> 00:09:48,366
而整个脉动的阵列

109
00:09:51,300 --> 00:09:52,366
矩阵的计算

110
00:09:53,100 --> 00:09:55,500
一个 3 乘以 3 的矩阵

111
00:09:55,500 --> 00:09:57,500
去讲讲脉动的阵列

112
00:10:01,700 --> 00:10:04,500
我们虽然是一个 3*3 的数据

113
00:10:07,500 --> 00:10:08,300
这是 a 矩阵

114
00:10:10,100 --> 00:10:11,300
数据的排布呢

115
00:10:11,300 --> 00:10:13,400
并不是完全相同

116
00:10:13,466 --> 00:10:15,366
我们输进去脉动阵列的时候呢

117
00:10:15,366 --> 00:10:16,966
数据进行要错开

118
00:10:26,100 --> 00:10:27,266
第一次计算的时候呢

119
00:10:27,266 --> 00:10:30,166
是把我们队列里面的 A00 呢

120
00:10:32,600 --> 00:10:33,300
相乘之后呢

121
00:10:36,700 --> 00:10:40,700
我们下次呢会把 A01 跟 B10 进行相乘

122
00:10:40,700 --> 00:10:43,000
累加到我们第一个寄存器里面

123
00:10:49,766 --> 00:10:52,466
然后呢就得到我们现在的结果

124
00:10:52,500 --> 00:10:55,400
接着呢我们还会不断的循环累加

125
00:10:55,466 --> 00:10:57,766
直到呢把所有的数据呢

126
00:10:57,766 --> 00:11:01,133
都加载到整个脉动阵列里面

127
00:11:01,700 --> 00:11:05,000
就完成我们刚才的三个矩阵的运算

128
00:11:13,266 --> 00:11:14,466
最终经过八步之后呢

129
00:11:24,000 --> 00:11:26,166
不过呢大家有没有留意到呢

130
00:11:29,900 --> 00:11:31,666
一次一次的累加起来

131
00:11:31,666 --> 00:11:32,866
最后再获取

132
00:11:33,600 --> 00:11:34,566
计算的延迟呢

133
00:11:34,566 --> 00:11:36,866
对程序或者对性能来说

134
00:11:37,966 --> 00:11:39,933
就变得非常非常的重要

135
00:11:40,200 --> 00:11:41,700
在实际的程序过程当

136
00:11:41,700 --> 00:11:45,200
中呢其实并不感知 MXU 的脉动的实现

137
00:11:47,300 --> 00:11:49,700
就需要考虑到对应的延迟

138
00:11:50,500 --> 00:11:53,400
因为刚才讲到数据一级一级的去传导

139
00:11:53,466 --> 00:11:54,766
这意味着延迟呢

140
00:12:00,566 --> 00:12:01,366
那这种情况呢

141
00:12:04,100 --> 00:12:07,000
TPU 的 CISC 卡指令呢就使用了 4 级的流水

142
00:12:15,866 --> 00:12:17,166
准备数据的时候呢

143
00:12:36,900 --> 00:12:37,666
那其实呢

144
00:12:54,166 --> 00:12:57,466
以对角波的形式呢通过阵列进行输出

145
00:13:01,900 --> 00:13:04,300
输到脉动阵列里面

146
00:13:04,300 --> 00:13:07,266
不断不断的去移动累加进行计算

147
00:13:16,666 --> 00:13:18,066
肯定离不开 GPU

148
00:13:42,266 --> 00:13:44,666
更符合神经网络的计算

149
00:14:09,766 --> 00:14:11,066
那通过这个目的不一样了

150
00:14:16,100 --> 00:14:18,300
具体呢能达到 92TFLOPS

151
00:14:18,400 --> 00:14:20,500
比英伟达的 K80 的推理芯片呢

152
00:14:20,500 --> 00:14:22,300
要高出非常多倍

153
00:14:59,700 --> 00:15:00,666
谷歌的 TPU 呢

154
00:15:17,166 --> 00:15:19,266
谷歌 TPU 还把片上缓存呢

155
00:15:34,500 --> 00:15:36,500
是节省我们 PCIe 传输慢

156
00:15:36,500 --> 00:15:37,900
一个效率的问题

157
00:15:51,100 --> 00:15:53,566
使用 8 比特进行一个推理

158
00:16:27,500 --> 00:16:29,700
当时候 TPU 的架构呢

159
00:16:33,100 --> 00:16:35,600
讲到这其实我们接近尾声了

160
00:16:55,000 --> 00:16:56,466
而当时候黄仁勋呢

161
00:16:58,700 --> 00:17:00,866
打算搞手机里面的 GPU

162
00:17:37,966 --> 00:17:38,966
或者回顾一下

163
00:17:41,166 --> 00:17:41,466
第六个呢

164
00:17:44,666 --> 00:17:45,266
这些

165
00:17:49,366 --> 00:17:50,466
那今天的内容呢

166
00:17:50,466 --> 00:17:51,933
就到这里为止了

167
00:17:52,966 --> 00:17:53,933
拜了个拜

168
00:00:05,700 --> 00:00:06,866
哈喽大家好

169
00:00:13,400 --> 00:00:15,500
特别是第一代的详细解读

170
00:00:15,700 --> 00:00:16,200
我们知道呢

171
00:00:16,266 --> 00:00:17,933
在整个课程里面

172
00:00:20,566 --> 00:00:22,333
国外 AI 芯片里面

173
00:00:28,566 --> 00:00:29,966
而说到芯片架构呢

174
00:00:47,200 --> 00:00:48,100
我们一开始呢

175
00:00:48,100 --> 00:00:51,866
看看谷歌 TPU 第一代的整体的电路板

176
00:00:51,966 --> 00:00:54,466
那这个呢就是它的整体的产品形态

177
00:00:54,566 --> 00:00:57,366
中间最核心的这一块灰色的呢

178
00:00:57,400 --> 00:00:59,466
就是它的 TPU 的地带

179
00:00:59,500 --> 00:01:01,866
用来做推理应用

180
00:01:02,266 --> 00:01:03,866
ZOMI 觉得谷歌 TPU 里面呢

181
00:01:03,900 --> 00:01:05,500
最著名的一个应用呢

182
00:01:05,500 --> 00:01:08,166
就是 2015 年到 2017 年

183
00:01:14,200 --> 00:01:17,100
把两个世界围棋的高手呢都打败了

184
00:01:17,100 --> 00:01:18,300
这是谷歌 TPU 里面

185
00:01:28,866 --> 00:01:32,166
特别是里面特别重要遇到的脉动阵列

186
00:01:32,200 --> 00:01:33,200
那我们现在呢

187
00:01:38,566 --> 00:01:41,166
会详细的对这个芯片的架构图呢

188
00:01:43,500 --> 00:01:46,566
首先呢第一个内容就是 Weight FIFO

189
00:01:46,566 --> 00:01:48,766
就这里面的一个内容

190
00:01:53,066 --> 00:01:54,466
DDR 是内存

191
00:01:54,500 --> 00:01:56,200
这里面呢把内存的内容呢

192
00:01:56,300 --> 00:01:57,400
从 DRAM 上面呢

193
00:01:57,466 --> 00:01:59,566
读取到芯片上面

194
00:01:59,800 --> 00:02:01,100
进行一个计算

195
00:02:01,100 --> 00:02:02,000
那具体计算呢

196
00:02:02,066 --> 00:02:04,366
就会给这个 MXU 的正念

197
00:02:04,500 --> 00:02:06,300
进行一个核心的计算

198
00:02:06,366 --> 00:02:08,666
那提到了刚才的 MXU

199
00:02:08,666 --> 00:02:09,466
这个阵列呢

200
00:02:18,100 --> 00:02:22,066
提供 256*256*8 的乘加的计算

201
00:02:29,100 --> 00:02:31,900
那这个点呢大家需要注意的是 FP16

202
00:02:32,466 --> 00:02:34,366
整个 MXU 矩阵单元里面呢

203
00:02:34,366 --> 00:02:36,866
还包含 64KB 的 Weight tile

204
00:02:42,366 --> 00:02:43,966
我们后面会展开

205
00:02:43,966 --> 00:02:46,466
它更多的是用来做一个缓存

206
00:03:01,800 --> 00:03:03,700
把结果存下来

207
00:03:08,566 --> 00:03:12,066
整个 MXU 呢会取 256 个元素

208
00:03:19,100 --> 00:03:21,066
存放在 accumulators

209
00:03:24,300 --> 00:03:26,100
也就是给我们下面的这个模块

210
00:03:33,700 --> 00:03:37,566
所以 accumulators 的大小呢刚好是 4MB

211
00:03:38,700 --> 00:03:40,100
对于控制指令来说

212
00:03:40,100 --> 00:03:41,466
 Control 啊

213
00:03:41,500 --> 00:03:42,700
因为每块芯片呢

214
00:03:45,866 --> 00:03:46,933
整个控制单元呢

215
00:03:57,866 --> 00:03:59,566
去通过 4 级的流水

216
00:04:04,400 --> 00:04:05,900
而指令的来源呢

217
00:04:05,900 --> 00:04:08,566
是 CPU 向 TPU 去发射指令

218
00:04:08,566 --> 00:04:09,666
让它去执行

219
00:04:09,966 --> 00:04:12,266
整个的芯片的指令的架构呢

220
00:04:12,266 --> 00:04:13,766
采用的是 CSIC 卡指令

221
00:04:16,466 --> 00:04:18,366
里面呢一共有 12 条

222
00:04:21,366 --> 00:04:23,866
为什么 TPU 提供十几条指令

223
00:04:27,400 --> 00:04:29,500
RISC 不是更简单吗

224
00:04:30,500 --> 00:04:33,166
小新提出这个疑问非常有意思

225
00:04:33,400 --> 00:04:35,866
因为谷歌定义的每一条指令呢

226
00:04:35,866 --> 00:04:37,466
平均的时钟周期呢

227
00:04:37,500 --> 00:04:39,366
需要 10 到 20 个时钟周期

228
00:04:39,366 --> 00:04:41,466
才能够执行完一条指令

229
00:04:57,166 --> 00:04:58,666
而设计的高级指令

230
00:05:01,700 --> 00:05:02,300
那这里面呢

231
00:05:05,266 --> 00:05:06,733
一个是 Read_Host_Memory

232
00:05:07,466 --> 00:05:08,766
还有 matmul 的计算

233
00:05:08,766 --> 00:05:12,066
卷积计算 activate 和 Write_Host_Memory

234
00:05:12,066 --> 00:05:14,366
那从这几个顺序可以看出来

235
00:05:16,366 --> 00:05:19,166
写数据计算算激活

236
00:05:19,300 --> 00:05:21,266
然后写回数据

237
00:05:21,366 --> 00:05:22,766
完成我们神经网络

238
00:05:22,766 --> 00:05:24,966
对每一层的具体的计算

239
00:05:27,500 --> 00:05:28,300
是最核心

240
00:05:28,366 --> 00:05:29,866
也单独的拿出来

241
00:05:29,900 --> 00:05:31,466
当然他还有其他指令啊

242
00:05:31,600 --> 00:05:32,700
如果开发者和你们呢

243
00:05:32,700 --> 00:05:35,366
非常关心谷歌 TPU 里面的指令呢

244
00:05:40,766 --> 00:05:42,766
具体是怎么定义

245
00:05:43,666 --> 00:05:45,133
了解完谷歌 TPU 的架构图呢

246
00:05:47,800 --> 00:05:49,166
那我们从这个图里面呢

247
00:05:50,500 --> 00:05:52,566
属于一个专用的电路里面

248
00:05:52,566 --> 00:05:53,366
最大的面积呢

249
00:05:58,966 --> 00:06:02,133
一个是具体的计算的阵列

250
00:06:02,500 --> 00:06:03,766
对于刚才讲到的指令

251
00:06:03,800 --> 00:06:05,000
 control 呢

252
00:06:21,266 --> 00:06:22,333
我还在录课

253
00:06:22,466 --> 00:06:24,466
现在我们才来到了脉动阵列

254
00:06:30,000 --> 00:06:30,900
整体的脉动呢

255
00:06:30,900 --> 00:06:33,566
就像心跳一样啊

256
00:06:33,800 --> 00:06:35,566
在正式进入到脉动阵列之前呢

257
00:06:35,566 --> 00:06:38,133
ZOMI 想跟大家一起去回顾一下

258
00:06:42,100 --> 00:06:43,400
关于推理的场景

259
00:06:43,466 --> 00:06:45,733
或者在 2015 年 2017 年的时候呢

260
00:06:46,300 --> 00:06:49,500
用到的最多的是卷积神经网络

261
00:06:49,566 --> 00:06:51,266
那整个卷积神经网络里面呢

262
00:06:51,300 --> 00:06:51,700
实际上

263
00:06:51,700 --> 00:06:54,500
我们不会去真正的对图片呢

264
00:07:04,766 --> 00:07:06,566
给大家讲过上面的这个呢

265
00:07:06,566 --> 00:07:08,566
就是真正的原始的卷积

266
00:07:10,166 --> 00:07:12,266
转换成为 img2col 的方式呢

267
00:07:12,300 --> 00:07:14,966
变成一个具体的矩阵乘的操作

268
00:07:15,166 --> 00:07:16,566
去得到我们跟卷积

269
00:07:16,566 --> 00:07:19,166
数学上原理一致的矩阵乘法

270
00:07:19,200 --> 00:07:21,900
最后再把结果 col2img 反复回来

271
00:07:21,900 --> 00:07:23,066
变成 feature map

272
00:07:23,066 --> 00:07:24,566
整体流程呢是这样

273
00:07:25,100 --> 00:07:25,700
而在这里面呢

274
00:07:25,700 --> 00:07:27,700
我们刚才提到一个很核心的功能

275
00:07:27,700 --> 00:07:30,800
就是变成了一个矩阵乘的计算

276
00:07:32,766 --> 00:07:34,066
得到一个元素

277
00:07:36,100 --> 00:07:38,300
这个图我觉得是非常非常核心

278
00:07:43,900 --> 00:07:44,900
进行计算

279
00:07:46,866 --> 00:07:47,733
相乘相加

280
00:07:48,766 --> 00:07:50,866
最后得到我们橙色

281
00:07:50,866 --> 00:07:52,466
这一个模块的计算

282
00:07:52,500 --> 00:07:55,300
有没有有点类似于脉动阵列

283
00:07:55,300 --> 00:07:56,900
或者脉冲呢

284
00:08:17,700 --> 00:08:19,766
也就是左边蓝色这个模块

285
00:08:19,900 --> 00:08:22,400
流到 MXU 里面进行计算

286
00:08:22,466 --> 00:08:22,966
最终呢

287
00:08:22,966 --> 00:08:25,333
流出到下面的这个计算器里面

288
00:08:26,200 --> 00:08:27,100
然后输出

289
00:08:27,200 --> 00:08:28,266
整体的计算方式呢

290
00:08:32,300 --> 00:08:32,900
非常庞

291
00:08:35,400 --> 00:08:37,300
又一拨一拨的流出

292
00:08:40,600 --> 00:08:42,100
所以在一个时钟周期内呢

293
00:08:42,100 --> 00:08:45,500
可以处理 665536 次矩阵的运算

294
00:08:50,300 --> 00:08:52,066
再进行下一次的累积

295
00:08:59,000 --> 00:09:01,266
我们正式的来到了脉动阵列的原理

296
00:09:02,666 --> 00:09:05,166
不过呢并不影响理解

297
00:09:07,666 --> 00:09:08,966
就是 m 跟 pe

298
00:09:10,766 --> 00:09:12,533
pe 呢就是 process Unit

299
00:09:14,166 --> 00:09:16,066
里面的具体的计算的执行单元

300
00:09:16,166 --> 00:09:17,466
传统的冯洛伊曼呢

301
00:09:17,466 --> 00:09:18,733
就是我算一次乘一次

302
00:09:21,466 --> 00:09:23,566
在计算不断的往回迭代

303
00:09:23,600 --> 00:09:25,566
这种呢就是我们传统的计算方式

304
00:09:30,066 --> 00:09:30,966
然后进行计算

305
00:09:31,300 --> 00:09:33,100
于是计算呢就变成一个串形的方式

306
00:09:36,900 --> 00:09:38,100
再给下次 PE

307
00:09:42,900 --> 00:09:44,866
提供一个脉动的阵列

308
00:09:45,500 --> 00:09:47,000
只是一个串形的方式

309
00:09:52,366 --> 00:09:53,066
那现在呢

310
00:09:57,666 --> 00:09:58,666
那脉动的阵列呢

311
00:09:58,700 --> 00:09:59,766
这里面的数据

312
00:09:59,766 --> 00:10:01,733
排布比较有意思

313
00:10:04,566 --> 00:10:07,533
也就是意味着我们假设 a 矩阵哦

314
00:10:22,566 --> 00:10:25,766
是错开一位进去输进去去计算

315
00:10:30,166 --> 00:10:32,466
跟 B00 进行相乘

316
00:10:33,300 --> 00:10:35,866
就变成我们现在所是的样子

317
00:10:35,866 --> 00:10:36,666
算完这个之后呢

318
00:10:44,600 --> 00:10:47,766
我们会把 A02 跟 B20 进行相乘

319
00:10:47,766 --> 00:10:49,366
累加到这里面

320
00:11:01,100 --> 00:11:01,700
最后呢

321
00:11:05,266 --> 00:11:07,066
完成三个矩阵的运算之后呢

322
00:11:07,066 --> 00:11:10,133
我们就会把整个矩阵的运算的结果呢

323
00:11:14,466 --> 00:11:16,466
就把整个 3*3 的 a 矩阵呢

324
00:11:16,500 --> 00:11:18,500
乘以 3*3 的 b 矩阵呢

325
00:11:18,666 --> 00:11:21,133
完全的计算出了那个结果

326
00:11:26,166 --> 00:11:28,166
刚才的计算其实是把数据呢

327
00:11:28,166 --> 00:11:29,766
一波一波的数据去

328
00:11:32,900 --> 00:11:33,600
那这个时候呢

329
00:11:56,866 --> 00:11:57,866
直到计算完之后

330
00:11:57,866 --> 00:12:00,466
我们才能够把所有的数据取出来

331
00:12:01,366 --> 00:12:03,366
就会导致我们有计算的延迟

332
00:12:03,800 --> 00:12:04,100
于是呢

333
00:12:07,066 --> 00:12:08,566
用其他指令的执行呢

334
00:12:08,566 --> 00:12:11,166
与刚才 MXU 的指令重新堆叠

335
00:12:11,166 --> 00:12:13,466
从而隐藏计算的延迟

336
00:12:13,500 --> 00:12:15,866
也就是我们在下一次取指的时候

337
00:12:17,200 --> 00:12:18,666
它已经在计算了

338
00:12:18,900 --> 00:12:20,500
通过指令流水的重叠呢

339
00:12:20,500 --> 00:12:22,566
从而隐藏时延

340
00:12:22,600 --> 00:12:26,500
这也是 SIMD 里面一个非常重要的功能

341
00:12:27,900 --> 00:12:30,100
从数学和数值上面来看呢

342
00:12:30,166 --> 00:12:31,466
TPU 整个脉动阵列呢

343
00:12:31,500 --> 00:12:33,700
就提供了 256 个乘积的计算

344
00:12:33,700 --> 00:12:36,866
以对角波的形式呢通过阵列

345
00:12:37,666 --> 00:12:38,366
刚才讲到

346
00:12:38,400 --> 00:12:41,300
是一个比较简单的脉动阵列的原理

347
00:12:41,300 --> 00:12:41,966
但实际上呢

348
00:12:41,966 --> 00:12:44,133
我们在卷积神经网络里面呢

349
00:12:44,300 --> 00:12:45,400
权重的数据呢

350
00:12:45,466 --> 00:12:47,966
会预先的放在脉动阵列里面

351
00:12:48,000 --> 00:12:49,266
就先放进去

352
00:12:49,300 --> 00:12:49,800
接着呢

353
00:12:49,800 --> 00:12:53,666
会把数据或者中间产生的计算结果呢

354
00:12:57,500 --> 00:13:00,100
所以看到了我们会先置一个 0

355
00:13:00,200 --> 00:13:01,900
然后呢以对角波的方式

356
00:13:10,266 --> 00:13:10,666
哎

357
00:13:10,666 --> 00:13:14,066
现在我们来到了接近最后一个小内容

358
00:13:14,100 --> 00:13:15,500
里面就是竞品的对比

359
00:13:15,500 --> 00:13:16,666
那讲到竞品呢

360
00:13:18,266 --> 00:13:18,966
那这里面呢

361
00:13:18,966 --> 00:13:22,366
谷歌地带的 TPU 呢就用 CPU GPU 跟 TPU 进

362
00:13:22,400 --> 00:13:24,900
行对比在硬件并行形态里面呢

363
00:13:24,900 --> 00:13:27,966
谷歌 TPU1 呢采用的是 SIMD 的模式

364
00:13:27,966 --> 00:13:30,966
而 GPU 呢采用的是 SIMT 的模式

365
00:13:31,066 --> 00:13:33,066
虽然 TPU 采用的是 SIMD

366
00:13:33,100 --> 00:13:35,200
但是经过我们刚才讲到

367
00:13:35,466 --> 00:13:38,166
通过多级流水呢去掩盖时延

368
00:13:38,266 --> 00:13:40,466
使得 TPU 确定性执行的方式呢

369
00:13:40,466 --> 00:13:42,166
会比 CPU 或者 GPU 呢

370
00:13:45,066 --> 00:13:46,566
有助于提高 TPU 的吞吐

371
00:13:46,566 --> 00:13:48,166
而不是降低它的延迟

372
00:13:48,366 --> 00:13:49,466
那这里面这个概念呢

373
00:13:49,466 --> 00:13:51,366
宗敏觉得非常有意思

374
00:13:51,500 --> 00:13:52,500
TPU 的目的呢

375
00:13:52,566 --> 00:13:54,066
是提高我们神经网络

376
00:13:54,100 --> 00:13:55,800
我们 AI 的计算的吞吐

377
00:13:55,900 --> 00:13:59,066
而我们在之前的 GPU 的核心内容

378
00:13:59,066 --> 00:14:01,166
或者 GPU 的技术分享里面呢

379
00:14:01,166 --> 00:14:02,466
去讲到 GPU 呢

380
00:14:02,466 --> 00:14:03,733
是通过多级的缓存

381
00:14:05,166 --> 00:14:07,466
去降低数据和计算的延迟

382
00:14:07,500 --> 00:14:09,766
所以他们两个的目的是不一样

383
00:14:11,066 --> 00:14:12,866
我们可以看到谷歌的 TPU 呢

384
00:14:12,900 --> 00:14:16,100
提供一个非常庞大非常澎湃的算力

385
00:14:23,566 --> 00:14:25,066
GPU 呢主要是解决延迟

386
00:14:25,100 --> 00:14:26,700
那就离不开我们看到的 GPU

387
00:14:26,700 --> 00:14:28,766
主要是线程分层的去执行

388
00:14:28,866 --> 00:14:31,866
通过网格到分开的网格的线程块

389
00:14:31,900 --> 00:14:34,100
到最终的线程去执行

390
00:14:34,100 --> 00:14:34,700
而这里面呢

391
00:14:34,700 --> 00:14:37,266
英伟达就提出了多级的缓存

392
00:14:37,500 --> 00:14:38,500
从 HBM 的缓存呢

393
00:14:38,566 --> 00:14:39,766
到 L2Cache 的缓存呢

394
00:14:39,766 --> 00:14:40,966
到 L1Cache 的缓存

395
00:14:41,100 --> 00:14:44,500
再到里面 SM 的 register file 寄存器

396
00:14:44,566 --> 00:14:45,866
通过多级的缓存呢

397
00:14:45,866 --> 00:14:47,866
去提升整体数据的吞吐

398
00:14:47,900 --> 00:14:50,100
减少数据搬运的延迟

399
00:14:50,166 --> 00:14:52,666
这个是 GPU 主要的核心的机制

400
00:14:52,966 --> 00:14:54,866
跟谷歌 TPU 推出出来目

401
00:14:54,900 --> 00:14:56,666
主要是提升计算的核心

402
00:14:56,666 --> 00:14:58,933
计算的吞吐不是一个概念

403
00:15:00,666 --> 00:15:02,933
它的计算性能就非常的夸张

404
00:15:06,100 --> 00:15:08,000
也就是 92T PROS

405
00:15:08,500 --> 00:15:10,600
那具体的计算方式呢就在这里面

406
00:15:10,600 --> 00:15:12,200
大家可以慢慢的去看

407
00:15:12,200 --> 00:15:15,100
ZOMI 就不一一的去给大家练起来了

408
00:15:16,366 --> 00:15:17,166
另外一方面呢

409
00:15:19,300 --> 00:15:20,900
做的非常非常的大

410
00:15:20,900 --> 00:15:23,666
从而去节省片外访存的消耗

411
00:15:23,700 --> 00:15:26,200
因为在 2015 年刚发出来的时候

412
00:15:26,200 --> 00:15:27,900
或者谷歌在做项目预研的时候

413
00:15:27,900 --> 00:15:29,200
其实在 2013 年

414
00:15:29,300 --> 00:15:31,100
当时还没有用到 HBM

415
00:15:31,100 --> 00:15:32,066
而是使用 DDR

416
00:15:32,166 --> 00:15:34,266
所以他把片上的缓存做大

417
00:15:39,000 --> 00:15:39,466
另外呢

418
00:15:39,466 --> 00:15:42,066
不得不提谷歌里面的技术前瞻性呢

419
00:15:42,066 --> 00:15:44,466
是非常非常的 outstanding 

420
00:15:44,666 --> 00:15:46,766
里面就提出了一个量化的概念

421
00:15:46,800 --> 00:15:47,600
谷歌 TPU 呢

422
00:15:47,600 --> 00:15:51,100
主要是针对推理场景的首个芯片

423
00:15:53,600 --> 00:15:55,400
谷歌非常具有前瞻性

424
00:15:55,400 --> 00:15:57,000
与之对应的就是英伟达

425
00:15:57,000 --> 00:15:59,300
当时候的推理卡 K80 呢

426
00:15:59,300 --> 00:16:01,366
还是使用 FP32 的精度

427
00:16:01,400 --> 00:16:05,000
所以说谷歌呢属于引领的阶段

428
00:16:05,200 --> 00:16:07,366
那通过下面这个图我们可以看到啊

429
00:16:07,366 --> 00:16:08,266
batch size 呢

430
00:16:08,266 --> 00:16:11,266
谷歌 TPU 呢是能够放的非常非常的大

431
00:16:11,466 --> 00:16:13,666
所以可以放到非常大的 Batch size

432
00:16:13,666 --> 00:16:14,466
那这个时候呢

433
00:16:14,500 --> 00:16:17,300
它的吞吐也就是对应的当时候图片呢

434
00:16:17,300 --> 00:16:19,600
它的单位呢是以 IPS 来说

435
00:16:19,666 --> 00:16:21,166
IPS 是非常的高

436
00:16:21,266 --> 00:16:23,866
比 GPU 最好的一款推理卡呢

437
00:16:23,900 --> 00:16:25,400
高了将近 10 倍

438
00:16:25,400 --> 00:16:27,500
所以说当时候的 TPU 的提出

439
00:16:29,800 --> 00:16:31,400
是非常有意义

440
00:16:35,600 --> 00:16:37,000
最后引起一些思考

441
00:16:37,000 --> 00:16:37,666
那谷歌呢

442
00:16:37,666 --> 00:16:38,866
在 2015 年的时候呢

443
00:16:38,866 --> 00:16:40,666
就已经部署了 TPU

444
00:16:40,700 --> 00:16:43,166
第一代 ASIC 的张量处理器

445
00:16:43,200 --> 00:16:45,966
那这意味着从芯片到逆向往前推呀

446
00:16:46,100 --> 00:16:49,566
其实谷歌在 2013 年的时候已经去立项了

447
00:16:49,566 --> 00:16:49,766
所以

448
00:16:49,766 --> 00:16:52,566
谷歌 AI 系统的思想还是非常超前

449
00:16:52,666 --> 00:16:54,966
当时英伟达呢还没有出现 Tensor Core

450
00:16:56,466 --> 00:16:58,666
也是来到中国跟小米去合作

451
00:17:19,966 --> 00:17:21,066
当时候的英伟达呀

452
00:17:21,100 --> 00:17:22,100
想都没想过

453
00:17:22,166 --> 00:17:25,133
居然能够在 AI 领域发光发热

454
00:17:25,966 --> 00:17:26,566
因此 ZOMI 呢

455
00:17:26,566 --> 00:17:29,266
就提出了两个简单的小问题

456
00:17:29,266 --> 00:17:30,766
谷歌 TPU 做对了什么

457
00:17:30,766 --> 00:17:32,866
有哪些超越时代的事迹

458
00:17:32,900 --> 00:17:33,800
那刚才这个呢

459
00:17:33,800 --> 00:17:35,500
ZOMI 已经给大家去汇报过了

460
00:17:35,500 --> 00:17:37,900
大家可以简单的去思考一下

461
00:17:38,966 --> 00:17:40,966
今天所给大家汇报的内容

462
00:17:41,500 --> 00:17:43,500
就是针对目前的 AI 的发展

463
00:17:43,500 --> 00:17:44,600
特别是大模型啊

464
00:17:45,266 --> 00:17:48,966
谷歌有哪些在 2015 年的时候没有做到

465
00:17:51,966 --> 00:17:52,866
谢谢各位

466
00:00:01,900 --> 00:00:04,600
字幕生成：mkwei  字幕校准：mkwei

467
00:00:08,500 --> 00:00:11,200
不知啥时候能下班的 ZOMI

468
00:05:06,700 --> 00:05:07,400
Read_Weights

469
00:05:55,500 --> 00:05:58,966
还有 MXU 那一个是缓存

470
00:06:45,700 --> 00:06:46,300
神经网络

471
00:07:47,700 --> 00:07:48,666
相乘相加

472
00:09:12,500 --> 00:09:13,900
或者 process execution

473
00:11:10,100 --> 00:11:12,600
同时间的去取出来

474
00:11:21,100 --> 00:11:22,766
然后输出出来

475
00:14:03,700 --> 00:14:05,100
多级的计算核心呢

476
00:14:58,900 --> 00:14:59,666
那这里面呢

477
00:00:11,266 --> 00:00:13,366
今天呢我们来到了谷歌 TPU 系列

478
00:00:25,266 --> 00:00:28,566
最核心的是谷歌的芯片的架构

479
00:00:45,766 --> 00:00:47,166
跟其他视频一样啊

480
00:01:26,800 --> 00:01:28,866
提供澎湃的散力

481
00:01:37,900 --> 00:01:38,566
我们后面呢

482
00:01:41,200 --> 00:01:42,700
进行打开

483
00:01:49,500 --> 00:01:53,066
Weight FO 呢主要是负责把 8GB 的 off chip 的 DDR

484
00:02:15,800 --> 00:02:18,100
里面呢就以脉动阵列的方式呢

485
00:02:39,500 --> 00:02:42,300
那至于 Weight tile 呢和双缓存的单元呢

486
00:02:53,366 --> 00:02:54,733
这个 accumulators 呢

487
00:02:58,666 --> 00:03:01,766
乘法产生的 16 比特的结果

488
00:03:06,866 --> 00:03:08,466
就是每个时钟周期呢

489
00:03:13,500 --> 00:03:16,500
256*256 乘以 8 比特的乘加

490
00:03:16,500 --> 00:03:19,066
计算产生 16 比特的结果

491
00:03:31,166 --> 00:03:33,533
b 就等于 4MB

492
00:03:44,666 --> 00:03:45,733
控制的单元

493
00:03:51,100 --> 00:03:52,666
需要获取具体的指令

494
00:03:55,500 --> 00:03:56,566
通过 host 主机

495
00:03:56,600 --> 00:03:57,766
就是 CPU 呢

496
00:04:01,766 --> 00:04:04,366
特别是 TPU 里面去进行控制

497
00:04:25,966 --> 00:04:27,266
而不用 RISC

498
00:04:46,700 --> 00:04:48,266
而不是用 RISC 的指令

499
00:04:59,600 --> 00:05:01,700
所以呢它只能用 RISC 了

500
00:05:02,300 --> 00:05:05,266
有 5 个比较核心的指令

501
00:05:49,166 --> 00:05:50,466
可以看到整个 TPU 呢

502
00:06:08,166 --> 00:06:10,766
给整个片上的存储和计算的单元呢

503
00:06:17,466 --> 00:06:19,533
哎讲到口水都干了

504
00:07:00,466 --> 00:07:02,533
换成矩阵乘的方式

505
00:07:03,300 --> 00:07:04,766
在之前推理系统里面呢

506
00:07:59,000 --> 00:08:01,200
谷歌的天才工程师就想到了哎

507
00:08:10,500 --> 00:08:11,666
去对矩阵

508
00:08:32,966 --> 00:08:35,366
大的计算数据一拨一拨的流入

509
00:08:37,866 --> 00:08:40,566
因为整个脉动阵列呢是 256*256

510
00:08:52,066 --> 00:08:54,666
这个呢就是脉动阵列最原始的来源

511
00:08:58,666 --> 00:08:58,966
接下来

512
00:09:27,300 --> 00:09:27,800
就觉得哎

513
00:09:33,100 --> 00:09:34,366
把 PE 计算结果呢

514
00:09:39,500 --> 00:09:41,900
再存到 memory 里面

515
00:09:48,400 --> 00:09:51,000
其实非常符合我们刚才讲到

516
00:10:08,300 --> 00:10:10,000
右边的这个是 b 矩阵

517
00:10:17,000 --> 00:10:22,566
例如这里面的 A00 A01 A02 到 A01 A11 A12 呢

518
00:10:43,100 --> 00:10:44,600
接着在第二个寄存器里面

519
00:11:36,866 --> 00:11:37,966
对吞吐来说呢

520
00:11:45,300 --> 00:11:47,300
所以呢我们性能去计算的时候

521
00:11:54,800 --> 00:11:56,866
会一步一步的去累加

522
00:15:02,966 --> 00:15:06,066
每秒能达到 92 万亿次计算

