1
00:00:00,012 --> 00:00:04,824
（把 88~拔~把把 88 拔~）

2
00:00:04,878 --> 00:00:08,617
大家好，是那个月入两千八,

3
00:00:08,617 --> 00:00:10,200
拿命往里搭的 ZOMI

4
00:00:10,200 --> 00:00:13,157
今天我们来到 AI 编译后端优化的

5
00:00:13,157 --> 00:00:15,000
AutoTuning 这个内容

6
00:00:15,000 --> 00:00:17,638
其实呢，上一节里面我们讲了

7
00:00:17,638 --> 00:00:19,900
算子调度有哪些优化的方法

8
00:00:19,900 --> 00:00:22,074
但是这些优化方法要落到实处的

9
00:00:22,074 --> 00:00:24,200
靠的是我们的 AutoTuning 的方法

10
00:00:24,200 --> 00:00:26,996
今天我给大家汇报一下 AutoTuning 这里面

11
00:00:26,996 --> 00:00:29,500
到底有哪些内容具体是怎么 Tune 的

12
00:00:30,300 --> 00:00:33,500
回到我们 AI 编译器的全栈的一个架构图里面

13
00:00:33,500 --> 00:00:37,900
我们现在还是在 OPS Optimizer 这个模块里面

14
00:00:37,900 --> 00:00:39,669
其实 AutoTuning 这个概念

15
00:00:39,669 --> 00:00:41,800
不是因为 AI 编译器而衍生的

16
00:00:41,800 --> 00:00:43,200
在传统的编译器里面

17
00:00:43,200 --> 00:00:45,500
我们已经早就有了这个概念

18
00:00:45,500 --> 00:00:48,000
我们现在看看传统编译器是怎么做的

19
00:00:48,000 --> 00:00:50,900
首先传统编译器它对 AutoTuning 的一个定义

20
00:00:50,900 --> 00:00:53,700
就是对于给定的程序和目标的一个架构

21
00:00:53,700 --> 00:00:56,400
找到一个最优的编译优化的方法

22
00:00:56,400 --> 00:01:00,000
所以这种是一个普适性通用性的一种方法

23
00:01:00,000 --> 00:01:03,200
具体可能会有几个问题我们一起去思考的

24
00:01:03,200 --> 00:01:06,300
就是我们需要使用哪些优化的方法

25
00:01:06,300 --> 00:01:09,100
选择什么样的一个优化的参数集

26
00:01:09,100 --> 00:01:10,777
接着用什么样的顺序

27
00:01:10,777 --> 00:01:13,200
去应用找到的一些优化的方法

28
00:01:13,200 --> 00:01:16,100
然后让我们的整个程序的性能最佳

29
00:01:16,100 --> 00:01:18,500
而传统编译器里面的性能最佳

30
00:01:18,500 --> 00:01:20,800
主要是指我们程序的性能最佳

31
00:01:22,000 --> 00:01:24,600
因为传统编译器也围绕着大量的计算

32
00:01:24,700 --> 00:01:26,700
其实我们有非常多的文章

33
00:01:26,700 --> 00:01:28,000
去对传统的编译器

34
00:01:28,000 --> 00:01:31,800
例如一些 BLas 库去做一些性能的优化

35
00:01:31,800 --> 00:01:34,700
下面这个图的文章出自于这一篇文章

36
00:01:34,700 --> 00:01:38,600
这里面主要是对 GEMM 矩阵层进行优化

37
00:01:38,600 --> 00:01:41,100
而这里面充分的去利用了整个

38
00:01:41,100 --> 00:01:42,900
现代处理器的多级缓存

39
00:01:42,900 --> 00:01:45,400
这里面的分为 1 2 3 4

40
00:01:45,400 --> 00:01:47,400
现代处理器里面的四级缓存

41
00:01:47,400 --> 00:01:48,400
那我们可以看一下

42
00:01:48,400 --> 00:01:51,700
其实主要是利用了 L3 L2 L1

43
00:01:51,700 --> 00:01:54,600
还有 Register 一些 ALU 旁边的一些

44
00:01:54,600 --> 00:01:56,800
寄存器进行一个加速的

45
00:01:56,800 --> 00:01:59,100
这种就是传统编译器对于矩阵层

46
00:01:59,100 --> 00:02:01,200
对于我们的一些大量的计算

47
00:02:01,200 --> 00:02:02,900
进行一个优化的

48
00:02:02,900 --> 00:02:03,900
传统的编译器

49
00:02:03,900 --> 00:02:05,700
其实发展了一段的时间之后

50
00:02:05,700 --> 00:02:08,300
主要分为两个主要的优化的方法

51
00:02:08,300 --> 00:02:10,200
第一个就是优化的选择

52
00:02:10,200 --> 00:02:13,100
选择使用哪些优化的算法

53
00:02:13,100 --> 00:02:15,100
第二个就是优化的顺序

54
00:02:15,100 --> 00:02:16,500
不同的优化的方法

55
00:02:16,500 --> 00:02:18,600
它们之间的顺序如何组成

56
00:02:18,600 --> 00:02:19,800
不同的优化的顺序

57
00:02:19,800 --> 00:02:21,700
可能会导致不同的优化的结果

58
00:02:23,300 --> 00:02:25,500
现在我们回到 AI 编译器里面的

59
00:02:25,500 --> 00:02:27,700
Auto-Tuning in AI

60
00:02:27,700 --> 00:02:30,000
其实 AI 编译器跟传统编译器之间

61
00:02:30,000 --> 00:02:31,600
一个 Auto-Tuning 的最大的区别

62
00:02:31,600 --> 00:02:32,200
有两个

63
00:02:32,200 --> 00:02:34,400
第一个就是更低的实验开销

64
00:02:34,400 --> 00:02:37,800
第二个就是针对特定领域的一个结构

65
00:02:37,800 --> 00:02:39,200
我们来看一下第一个

66
00:02:39,200 --> 00:02:42,000
更低的实验开销指的是啥呢

67
00:02:42,000 --> 00:02:43,600
更低的实验开销主要是指

68
00:02:43,600 --> 00:02:45,700
我们现在更多的是聚焦于

69
00:02:45,700 --> 00:02:48,800
某个算子或者某个 Kernel 级别的一个优化

70
00:02:48,800 --> 00:02:50,300
而不是整个程序

71
00:02:50,300 --> 00:02:51,900
而传统的编译器主要是指

72
00:02:51,900 --> 00:02:54,100
整个程序的一个性能的优化

73
00:02:55,300 --> 00:02:56,900
第二个点就是 Cost Model

74
00:02:56,900 --> 00:02:58,000
就是我们的成本函数

75
00:02:58,000 --> 00:02:59,100
我们后面会讲的

76
00:02:59,100 --> 00:03:00,600
在 CPU 上面去模拟

77
00:03:00,600 --> 00:03:02,500
我们在 NPU 在我们具体的

78
00:03:02,500 --> 00:03:05,000
AI 加速芯片上面去执行

79
00:03:05,000 --> 00:03:07,400
对于训练和推理的模拟的速度

80
00:03:07,400 --> 00:03:08,700
要求足够的快

81
00:03:08,700 --> 00:03:11,500
那这个就是我们更低的实验开销

82
00:03:11,500 --> 00:03:14,300
第二个就是特定的领域结构

83
00:03:14,300 --> 00:03:16,500
提到 Domain Specific 就特定了

84
00:03:16,500 --> 00:03:18,400
我们肯定知道主要是针对

85
00:03:18,400 --> 00:03:19,900
神经网络的算子

86
00:03:19,900 --> 00:03:22,400
或者神经网络的 Kernel 级别的优化

87
00:03:22,400 --> 00:03:25,600
那其实跟刚才上面那条是有点类似的

88
00:03:25,600 --> 00:03:27,300
但是这里面更多的是指

89
00:03:27,300 --> 00:03:29,300
我们真正的特定的内容

90
00:03:29,300 --> 00:03:31,800
就是我们的神经网络或者 Kernel 的算子

91
00:03:31,800 --> 00:03:34,000
大部分都是高度的循环化

92
00:03:34,000 --> 00:03:35,400
就是很多的 Loop

93
00:03:35,400 --> 00:03:37,500
第二个就是高度的张量化

94
00:03:37,500 --> 00:03:40,600
我们的数据的结构的维度是非常的高的

95
00:03:40,600 --> 00:03:43,200
另外一个有这么高的维度和循环化

96
00:03:43,200 --> 00:03:45,400
我们需要大量的并行的特性

97
00:03:45,400 --> 00:03:46,700
进行一个优化的

98
00:03:46,700 --> 00:03:47,300
那第二个呢

99
00:03:47,300 --> 00:03:50,600
就是大量相类似的算子的计算模式

100
00:03:50,600 --> 00:03:53,000
大量相类似的算子的计算模式呢

101
00:03:53,000 --> 00:03:54,500
我们可以回顾一下

102
00:03:54,500 --> 00:03:55,900
在神经网络里面呢

103
00:03:55,900 --> 00:03:59,400
我们有非常多的 Pooling 的算子

104
00:03:59,400 --> 00:04:01,100
Max Pooling, Average Pooling

105
00:04:01,100 --> 00:04:03,700
这些都是相类似的一些计算的模式

106
00:04:03,700 --> 00:04:06,100
还有我们的 Normalization

107
00:04:06,100 --> 00:04:07,100
Group Normalization

108
00:04:07,100 --> 00:04:08,000
Layer Normalization

109
00:04:08,000 --> 00:04:09,700
不同的 Normalization 的方式

110
00:04:09,700 --> 00:04:12,100
其实它有大量的相似的模式

111
00:04:12,100 --> 00:04:13,800
那这些部分相似的模式

112
00:04:13,800 --> 00:04:15,800
我们都可以抽象出来

113
00:04:15,800 --> 00:04:18,600
变成一个固定的优化的 Pattern 或者方式

114
00:04:18,600 --> 00:04:19,100
那这种呢

115
00:04:19,100 --> 00:04:21,400
就是特定领域的结构了

116
00:04:21,400 --> 00:04:23,500
在 AI 编译器里面的 Auto-Tuning

117
00:04:23,500 --> 00:04:24,900
就是因为这两点

118
00:04:24,900 --> 00:04:27,700
而跟传统的 Auto-Tuning 所区分出来的

119
00:04:30,100 --> 00:04:30,600
下面呢

120
00:04:30,600 --> 00:04:32,100
我们一起来思考两个问题

121
00:04:32,100 --> 00:04:32,700
这个问题呢

122
00:04:32,700 --> 00:04:36,200
也是陈天奇去在一篇博客里面提到的一个问题

123
00:04:36,200 --> 00:04:37,200
我们来看一下

124
00:04:37,200 --> 00:04:39,300
其实我们 AI 编译器里面呢

125
00:04:39,300 --> 00:04:40,400
做一个 Auto-Tuning

126
00:04:40,400 --> 00:04:42,400
最大的工作或者最大的目的呢

127
00:04:42,400 --> 00:04:45,600
就是使得如何让机器生成的 Kernel

128
00:04:45,600 --> 00:04:50,000
跟我们人工手写优化后的 Kernel 的性能相匹配

129
00:04:50,000 --> 00:04:50,700
而这里面呢

130
00:04:50,700 --> 00:04:53,500
它给出一个比较简单粗暴的回答

131
00:04:53,500 --> 00:04:53,900
第一个呢

132
00:04:53,900 --> 00:04:56,700
就是建立一个足够大的搜索空间

133
00:04:56,700 --> 00:04:58,200
那这个搜索空间的保证

134
00:04:58,200 --> 00:05:01,700
我们可以把人工手写优化的全部的内容

135
00:05:01,700 --> 00:05:04,500
都包含在这个搜索空间里面

136
00:05:04,500 --> 00:05:05,000
接着呢

137
00:05:05,000 --> 00:05:07,700
对这个搜索空间快速的去搜索

138
00:05:07,700 --> 00:05:11,200
找到最好的一个优化的实现的方式

139
00:05:11,200 --> 00:05:14,300
但实际上面对第一个和第二个问题的步骤

140
00:05:14,300 --> 00:05:16,400
有非常多大的挑战

141
00:05:16,400 --> 00:05:18,400
第一个就是这个搜索空间这么大

142
00:05:18,400 --> 00:05:21,700
保证所有的人工优化的方法都包含在里面

143
00:05:21,700 --> 00:05:24,700
本来就不一定能够完完全全的实现

144
00:05:24,700 --> 00:05:27,600
第二个如何快速的搜索这个空间

145
00:05:27,600 --> 00:05:30,100
也是一个很大的一个挑战

146
00:05:30,100 --> 00:05:31,200
面对这个问题呢

147
00:05:31,200 --> 00:05:33,200
越来越多的计算机科学家

148
00:05:33,200 --> 00:05:34,500
或者 AI 科学家呢

149
00:05:34,500 --> 00:05:37,100
或者工程师投身到这个领域里面

150
00:05:37,100 --> 00:05:39,200
然后把我们的一些大量的循环

151
00:05:39,200 --> 00:05:41,100
或者大量的一些 Kernel 的算子

152
00:05:41,100 --> 00:05:43,300
进行一些优化

153
00:05:43,300 --> 00:05:45,300
去探索 auto-tuning 的方式

154
00:05:45,300 --> 00:05:46,000
那现在呢

155
00:05:46,000 --> 00:05:46,800
一般来说呢

156
00:05:46,800 --> 00:05:48,400
总结成为三个步骤

157
00:05:48,400 --> 00:05:50,800
那第一个步骤就是参数化

158
00:05:50,800 --> 00:05:53,100
Parameterization 就是参数化

159
00:05:53,100 --> 00:05:53,600
第二个呢

160
00:05:53,600 --> 00:05:55,300
就是成本函数

161
00:05:55,300 --> 00:05:56,500
建立一个 cost model 呢

162
00:05:56,500 --> 00:05:59,600
去评价我们的参数化到底合不合理

163
00:05:59,600 --> 00:06:01,300
有了参数化成本函数之后呢

164
00:06:01,300 --> 00:06:03,700
就需要建立一个搜索算法

165
00:06:03,700 --> 00:06:05,000
Search Algorithm

166
00:06:05,000 --> 00:06:06,500
在搜索空间里面呢

167
00:06:06,500 --> 00:06:08,600
去找到一个比较好的算法

168
00:06:08,600 --> 00:06:10,300
比较好的参数化的方式

169
00:06:10,300 --> 00:06:12,700
让我们的成本函数最小化

170
00:06:12,700 --> 00:06:15,800
那接下来我们逐个的打开每一个步骤

171
00:06:17,000 --> 00:06:17,800
第一个步骤呢

172
00:06:17,800 --> 00:06:20,400
就是参数化的内容

173
00:06:20,400 --> 00:06:21,500
参数化这个概念呢

174
00:06:21,500 --> 00:06:22,700
听上去比较实在

175
00:06:22,700 --> 00:06:23,400
但实际上呢

176
00:06:23,400 --> 00:06:25,500
我们主要是对调度优化的问题呢

177
00:06:25,500 --> 00:06:26,800
进行建模

178
00:06:26,800 --> 00:06:27,900
参数化优化的空间

179
00:06:27,900 --> 00:06:29,400
或者我们的搜索空间呢

180
00:06:29,400 --> 00:06:31,000
主要是由我们在上一节里面

181
00:06:31,000 --> 00:06:33,800
给大家去介绍的一些 loop 的一些方法

182
00:06:33,800 --> 00:06:35,600
不管是循环优化

183
00:06:35,600 --> 00:06:36,200
指令优化

184
00:06:36,200 --> 00:06:37,500
还是内存优化

185
00:06:37,500 --> 00:06:39,100
它们都可以组成一些

186
00:06:39,100 --> 00:06:40,700
相对固定的组合结构

187
00:06:41,800 --> 00:06:44,500
变成一些实际上可以调度的一些原语

188
00:06:44,500 --> 00:06:45,600
有了这些原语呢

189
00:06:45,600 --> 00:06:48,500
我们就可以对它进行参数化的表示

190
00:06:48,500 --> 00:06:49,200
那 Halide 呢

191
00:06:49,200 --> 00:06:51,100
就将算法和调度解耦出来

192
00:06:51,100 --> 00:06:52,500
变成一个调度树

193
00:06:52,500 --> 00:06:53,000
TVM 呢

194
00:06:53,000 --> 00:06:54,600
就提供了调度模板

195
00:06:54,600 --> 00:06:55,700
通过调度模板呢

196
00:06:55,700 --> 00:06:58,100
对我们的调度树进行一个表达

197
00:06:58,100 --> 00:07:00,600
我们来看一下下面这条公式

198
00:07:00,600 --> 00:07:01,100
我们呢

199
00:07:01,100 --> 00:07:02,800
有 bx, tx 里面呢

200
00:07:02,800 --> 00:07:05,900
就声明我的 s[C] 这一个数据的内容

201
00:07:05,900 --> 00:07:08,000
进行 split 循环切分

202
00:07:08,000 --> 00:07:09,500
因子呢是 64

203
00:07:09,500 --> 00:07:10,800
而 factor 等于 64 呢

204
00:07:10,800 --> 00:07:13,800
就是我们可能的一个参数取值的空间

205
00:07:13,800 --> 00:07:14,700
或者范围

206
00:07:14,700 --> 00:07:17,000
我们的 factor 可以等于各种内容

207
00:07:17,000 --> 00:07:17,800
或者各种参数

208
00:07:17,800 --> 00:07:20,000
让我们整个空间进行一个搜索的

209
00:07:20,000 --> 00:07:22,200
这个就是参数化的作用

210
00:07:22,200 --> 00:07:22,700
接着呢

211
00:07:22,700 --> 00:07:23,700
有了参数化之后

212
00:07:23,700 --> 00:07:25,900
我们需要建立一个成本函数

213
00:07:25,900 --> 00:07:27,000
就是 cost model

214
00:07:27,000 --> 00:07:28,100
这个 cost model 呢

215
00:07:28,100 --> 00:07:29,500
主要是用来评价

216
00:07:29,500 --> 00:07:32,000
在某一个参数化的调度性能下面呢

217
00:07:32,000 --> 00:07:34,300
去评估这个调度策略

218
00:07:34,300 --> 00:07:35,800
到底好还是不好

219
00:07:35,800 --> 00:07:36,800
那怎么评价呢

220
00:07:36,900 --> 00:07:38,200
从哪几个维度呢

221
00:07:38,200 --> 00:07:41,000
更多的我们有几种评价的手段

222
00:07:41,000 --> 00:07:41,400
第一种呢

223
00:07:41,400 --> 00:07:42,700
就是运行的时间

224
00:07:42,700 --> 00:07:44,000
真正的运行时间

225
00:07:44,000 --> 00:07:44,400
第二个呢

226
00:07:44,400 --> 00:07:47,200
就是内存的占用的大小

227
00:07:47,200 --> 00:07:47,800
第三个呢

228
00:07:47,800 --> 00:07:50,300
就是编译后指令的数量来去评价的

229
00:07:50,300 --> 00:07:51,900
但是现在来说呀

230
00:07:51,900 --> 00:07:54,300
为了让我们的成本函数计算的越快

231
00:07:54,300 --> 00:07:57,400
我们一般的更关注的是运行的时间

232
00:07:57,400 --> 00:08:00,200
运行的越快肯定是越好的

233
00:08:00,200 --> 00:08:01,000
实现方式呢

234
00:08:01,000 --> 00:08:02,000
主要分为三种

235
00:08:02,000 --> 00:08:02,400
第一种呢

236
00:08:02,400 --> 00:08:04,700
是大家能够想到的最简单的

237
00:08:04,700 --> 00:08:07,800
就是基于我们的 NPU 的硬件的一个黑盒模式

238
00:08:07,800 --> 00:08:09,700
就真正的我们的成本函数

239
00:08:09,700 --> 00:08:11,800
跑在我们的 AI 加速芯片上面

240
00:08:11,800 --> 00:08:12,500
那但是呢

241
00:08:12,500 --> 00:08:14,000
这种方式是最耗时

242
00:08:14,000 --> 00:08:15,800
也是最慢的

243
00:08:15,800 --> 00:08:18,100
好处就是非常的准确

244
00:08:18,100 --> 00:08:18,600
第二种呢

245
00:08:18,600 --> 00:08:20,100
就是模拟的预定义模型

246
00:08:20,100 --> 00:08:22,500
就我们现在所谓的模拟

247
00:08:22,500 --> 00:08:23,200
模拟什么呢

248
00:08:23,200 --> 00:08:24,800
模拟我们的 NPU

249
00:08:24,800 --> 00:08:25,700
那这种方式呢

250
00:08:25,700 --> 00:08:26,600
也是非常耗时

251
00:08:26,600 --> 00:08:29,600
而且建模的手段非常的复杂

252
00:08:29,600 --> 00:08:30,200
第三种呢

253
00:08:30,200 --> 00:08:33,700
就是基于机器学习的 ML-based 的一种模型

254
00:08:33,800 --> 00:08:37,300
通过机器学习来对调度性能进行预测

255
00:08:37,300 --> 00:08:38,400
那这第三种呢

256
00:08:38,400 --> 00:08:39,800
就是 TVM 使用的方式

257
00:08:39,800 --> 00:08:43,400
也是现在来说相对成熟的一种方案

258
00:08:43,400 --> 00:08:44,900
有了成本函数之后呢

259
00:08:44,900 --> 00:08:47,400
第三步就是进行一个搜索

260
00:08:47,400 --> 00:08:49,400
对我们的搜索空间进行搜索

261
00:08:49,400 --> 00:08:50,800
对我们的参数化的内容

262
00:08:50,800 --> 00:08:53,800
对我们参数化的调度方式进行搜索

263
00:08:53,800 --> 00:08:57,100
以找到一个性能最好的参数的配置的方案

264
00:08:57,100 --> 00:09:00,300
那现在的搜索方法或者搜索算法有非常多

265
00:09:00,300 --> 00:09:01,200
有遗传算法

266
00:09:01,200 --> 00:09:02,100
模拟退火算法

267
00:09:02,100 --> 00:09:03,100
还有强化学习

268
00:09:03,100 --> 00:09:04,300
非常多的算法

269
00:09:04,300 --> 00:09:06,552
我们现在来看看 TVM 的一个栈

270
00:09:06,552 --> 00:09:08,000
里面是怎么去实现的

271
00:09:08,000 --> 00:09:08,600
首先呢

272
00:09:08,600 --> 00:09:09,400
TVM 里面呢

273
00:09:09,400 --> 00:09:10,900
分为非常多的栈

274
00:09:10,900 --> 00:09:11,300
但是呢

275
00:09:11,300 --> 00:09:13,200
我们基本上从这个框开始

276
00:09:13,200 --> 00:09:15,700
就是我们 AutoTuning 的内容

277
00:09:15,700 --> 00:09:18,406
AutoTuning 的第一个内容就对应于 Section 4

278
00:09:18,406 --> 00:09:19,600
我们的参数化

279
00:09:19,600 --> 00:09:21,100
我们把 Tensor Expression

280
00:09:21,100 --> 00:09:23,200
就是把我们 Tensor 的表达方式

281
00:09:23,200 --> 00:09:24,000
抽离出来

282
00:09:24,000 --> 00:09:25,200
类似于 Halide 的

283
00:09:25,200 --> 00:09:28,700
把我们调度和计算的逻辑分离出来

284
00:09:28,700 --> 00:09:30,200
右边这个灰色的框呢

285
00:09:30,200 --> 00:09:32,200
就是硬件感知的优化的一些

286
00:09:32,200 --> 00:09:33,600
原语 primitives

287
00:09:33,600 --> 00:09:34,800
有了参数化之后呢

288
00:09:34,800 --> 00:09:36,700
我们就可以很好的对我们的计算

289
00:09:36,700 --> 00:09:37,900
还有对我们的硬件呢

290
00:09:37,900 --> 00:09:39,400
进行一个表示

291
00:09:39,400 --> 00:09:40,500
有了这种表示之后呢

292
00:09:40,500 --> 00:09:43,200
就建立了我们的成本的模型

293
00:09:43,200 --> 00:09:44,100
就 Cost Model

294
00:09:44,100 --> 00:09:45,100
这个成本模型呢

295
00:09:45,100 --> 00:09:47,800
就是使用了机器学习的一个算法

296
00:09:47,800 --> 00:09:49,300
进行去评价的

297
00:09:49,300 --> 00:09:50,100
至于搜索呢

298
00:09:50,100 --> 00:09:51,400
我们就在这里面呢

299
00:09:51,400 --> 00:09:54,300
对 Loop Programming 进行一个不断的迭代搜索

300
00:09:54,300 --> 00:09:55,700
搜索我们的整个空间

301
00:09:55,700 --> 00:09:58,000
使得我们的成本函数最优

302
00:09:58,000 --> 00:09:59,300
找到对应的策略之后呢

303
00:09:59,300 --> 00:10:00,900
就利用 LLVM

304
00:10:00,900 --> 00:10:01,900
或者其他编程语言

305
00:10:01,900 --> 00:10:03,300
或者底层的编译器呢

306
00:10:03,300 --> 00:10:07,100
去实现或者生成对应的指令代码

307
00:10:07,100 --> 00:10:09,900
然后去给我们真正的硬件去部署使用

308
00:10:09,900 --> 00:10:10,500
那这种呢

309
00:10:10,500 --> 00:10:12,600
就是 TVM 的方式

310
00:10:12,600 --> 00:10:15,400
当然了 TVM 也迭代了好几个版本

311
00:10:15,400 --> 00:10:16,200
像 Ansor 呢

312
00:10:16,200 --> 00:10:18,400
就是 TVM 最新的一个版本

313
00:10:18,400 --> 00:10:20,900
里面呢 就避免了使用的模板的方式

314
00:10:20,900 --> 00:10:21,600
那这里面呢

315
00:10:21,600 --> 00:10:22,900
主要是有几个步骤

316
00:10:22,900 --> 00:10:24,900
那第一个步骤就是我们深度学习

317
00:10:24,900 --> 00:10:26,600
用 AI 框架进行表达之后呢

318
00:10:26,600 --> 00:10:28,400
我们就拿到它的一个子图

319
00:10:28,400 --> 00:10:29,000
那接着呢

320
00:10:29,000 --> 00:10:30,400
我们会对子图呢

321
00:10:30,400 --> 00:10:32,000
去做一个切分

322
00:10:32,000 --> 00:10:34,300
获取一些比较重要的一些子图

323
00:10:34,300 --> 00:10:36,100
拿到重要的一个子图之后呢

324
00:10:36,100 --> 00:10:38,100
我们就需要走到 Section 4

325
00:10:38,100 --> 00:10:39,100
或者 Section 5

326
00:10:39,100 --> 00:10:40,300
这个步骤里面的

327
00:10:40,300 --> 00:10:40,900
这里面呢

328
00:10:40,900 --> 00:10:41,900
主要有两个

329
00:10:41,900 --> 00:10:43,700
第一个呢 Sketch Generation

330
00:10:43,700 --> 00:10:45,900
确定优化的主要的结构

331
00:10:45,900 --> 00:10:46,600
那第二个呢

332
00:10:46,600 --> 00:10:48,700
就是 Random Annotation

333
00:10:48,700 --> 00:10:49,700
随机注释呢

334
00:10:49,700 --> 00:10:51,100
主要是对应于右边

335
00:10:51,100 --> 00:10:52,300
我们看到这里面呢

336
00:10:52,300 --> 00:10:54,600
产生非常多的不同的循环

337
00:10:54,600 --> 00:10:56,400
或者不同的方式

338
00:10:56,400 --> 00:10:58,600
这里面会进行一些随机的初始化

339
00:10:58,600 --> 00:10:59,600
随机的 Tuning

340
00:10:59,600 --> 00:11:01,300
或者随机的 Unrolling

341
00:11:01,300 --> 00:11:04,200
优化的方式随机的进行 Annotation

342
00:11:04,200 --> 00:11:06,900
产生了非常多的搜索的配置的方式

343
00:11:06,900 --> 00:11:07,700
接着下一步呢

344
00:11:07,700 --> 00:11:09,700
就是 Performance tuning

345
00:11:09,700 --> 00:11:10,200
这里面呢

346
00:11:10,200 --> 00:11:11,900
就使用了启发式的搜索方法

347
00:11:11,900 --> 00:11:13,800
还有我们的 Cost Model 去配合

348
00:11:13,800 --> 00:11:16,100
Cost Model 也是一个可学习的 Cost Model

349
00:11:16,100 --> 00:11:17,500
互相配合学习迭代

350
00:11:17,500 --> 00:11:20,400
最后学习到了一个比较好的策略之后

351
00:11:20,400 --> 00:11:23,400
就利用我们右边的 Sampled Program 3

352
00:11:23,400 --> 00:11:25,400
学习到一个比较明确的策略之后

353
00:11:25,400 --> 00:11:26,400
或者比较好的

354
00:11:26,400 --> 00:11:27,900
Kernels 的优化方法之后呢

355
00:11:27,900 --> 00:11:31,800
就变成我们实际在硬件上面可以执行的

356
00:11:31,800 --> 00:11:32,500
一些指令

357
00:11:32,500 --> 00:11:34,000
然后进行一个黑盒测试

358
00:11:34,000 --> 00:11:35,200
黑盒测试完之后呢

359
00:11:35,200 --> 00:11:37,700
再迭代回来不断的进行优化

360
00:11:37,700 --> 00:11:38,300
那这种呢

361
00:11:38,300 --> 00:11:40,100
就是 TVM 最新的一种方法

362
00:11:41,700 --> 00:11:42,100
好了

363
00:11:42,100 --> 00:11:43,000
今天的内容呢

364
00:11:43,000 --> 00:11:44,000
就到这里为止

365
00:11:44,000 --> 00:11:44,800
谢谢各位

366
00:11:44,800 --> 00:11:45,700
(拜了个拜)
摆了个掰

367
00:11:45,700 --> 00:11:46,600
卷的不行了

368
00:11:46,600 --> 00:11:47,500
卷的不行了

369
00:11:47,500 --> 00:11:49,300
记得一键三连加关注哦

370
00:11:49,300 --> 00:11:50,900
所有的内容都会开源在

371
00:11:50,900 --> 00:11:52,900
下面这条链接里面

372
00:11:52,900 --> 00:11:53,600
(拜了个拜)
摆了个掰