1
00:00:00,000 --> 00:00:05,900
字幕生成：mkwei 字幕校对：qiaokai

2
00:00:06,150 --> 00:00:07,920
Hello 大家好 我是 ZOMI 

3
00:00:07,920 --> 00:00:10,800
今天我来给大家分享一个新的内容

4
00:00:10,800 --> 00:00:13,680
就是 ChatGPT 狂飙原理剖析

5
00:00:13,680 --> 00:00:17,440
深入的去看看 ChatGPT 里面的核心的原理

6
00:00:17,440 --> 00:00:20,360
其实 ChatGPT 就不用我说它是什么

7
00:00:20,360 --> 00:00:21,520
最近发生了什么

8
00:00:21,520 --> 00:00:23,960
确实已经火遍大江南北了

9
00:00:23,960 --> 00:00:26,120
然后我今天要分享的一个内容

10
00:00:26,320 --> 00:00:29,920
主要是看一下 ChatGPT 里面的一些核心的

11
00:00:29,920 --> 00:00:30,560
技术点

12
00:00:30,560 --> 00:00:34,080
那首先会分开 4 个内容去给大家介绍的

13
00:00:34,080 --> 00:00:36,720
主要可能我后面会分开三个视频

14
00:00:36,720 --> 00:00:38,320
那第一个视频去看一看

15
00:00:38,320 --> 00:00:38,920
bert 模型

16
00:00:38,920 --> 00:00:41,560
还有 GPT 这个模型的系列

17
00:00:41,560 --> 00:00:43,600
这个系列到底发生了什么

18
00:00:43,600 --> 00:00:46,120
接着在后面第 2 个内容里面

19
00:00:46,120 --> 00:00:48,360
去看看强化学习里面

20
00:00:48,360 --> 00:00:50,400
怎么去引入了人类的反馈

21
00:00:50,400 --> 00:00:51,360
就 Reinforcement

22
00:00:51,360 --> 00:00:53,615
Human Feedback 这种模式

23
00:00:53,615 --> 00:00:53,640
那这种模式又加入了 PPO 的算法

24
00:00:53,640 --> 00:00:56,775
那这种模式又加入了 PPO 的算法

25
00:00:57,920 --> 00:00:59,320
在第 3 个内容里面

26
00:00:59,560 --> 00:01:02,200
就会去深入的来到 InstructGPT 的

27
00:01:02,200 --> 00:01:04,680
一个原理的深度剖析

28
00:01:04,680 --> 00:01:08,080
其实 InstructGPT 就是 ChatGPT 的一个原生

29
00:01:08,080 --> 00:01:10,640
当然了 ChatGPT 现在的论文没有公布

30
00:01:10,640 --> 00:01:13,000
但是 InstructGPT 是现在为止

31
00:01:13,000 --> 00:01:16,600
跟 ChatGPT 的算法原理是最接近的

32
00:01:17,240 --> 00:01:19,240
下面来到第 1 个内容

33
00:01:19,240 --> 00:01:24,400
从 GPT12 到 GPT-3 里面的一个技术的过渡

34
00:01:24,400 --> 00:01:25,680
或者技术的变化

35
00:01:25,680 --> 00:01:28,440
那 ChatGPT12 大部分都是用微调的

36
00:01:28,440 --> 00:01:29,240
这种方式

37
00:01:29,400 --> 00:01:33,680
到 GPT-3 就引入了一个 prompt learning 的方式

38
00:01:33,760 --> 00:01:36,400
下面来看看具体的内容

39
00:01:36,400 --> 00:01:38,280
像在 GPT 系列里面

40
00:01:38,480 --> 00:01:40,360
已经经历了三代

41
00:01:40,360 --> 00:01:43,040
GPT1 了 2 了 3 了三代

42
00:01:43,040 --> 00:01:46,040
其实三代都是以 Transformer 为核心

43
00:01:46,040 --> 00:01:47,840
去构造网络模型

44
00:01:47,840 --> 00:01:50,440
不同的就在于他们的一个模型层

45
00:01:50,440 --> 00:01:52,040
还有词向量的长度 

46
00:01:52,280 --> 00:01:54,640
最大的不同就是学习的方式

47
00:01:54,640 --> 00:01:57,000
learning 的方式不太一样

48
00:01:57,000 --> 00:02:00,360
下面就是我汇总的一个简单的小表格

49
00:02:00,360 --> 00:02:02,200
可以看到 OpenAI

50
00:02:02,200 --> 00:02:05,600
主导了 GPT123 整个系列

51
00:02:05,600 --> 00:02:08,000
从 18 年 19 年 20 年

52
00:02:08,000 --> 00:02:09,600
到现在的 23 年

53
00:02:09,600 --> 00:02:10,800
ChatGPT 出来

54
00:02:10,800 --> 00:02:13,360
确实网络模型的参数量

55
00:02:13,360 --> 00:02:14,800
进一步的提升

56
00:02:14,800 --> 00:02:16,280
而且网络模型的参数量

57
00:02:16,680 --> 00:02:19,360
到了现在的 1000 多亿的规模

58
00:02:19,360 --> 00:02:22,080
基本上在生态里面要训起来

59
00:02:22,080 --> 00:02:24,840
一个 ChatGPT 或者一个 GPT-3

60
00:02:24,880 --> 00:02:28,080
可能要消耗好多的集群的资源

61
00:02:28,640 --> 00:02:31,320
下面来看一下 GPT-1

62
00:02:31,320 --> 00:02:35,080
GPT-1 它主要是基于 Transformer 的 decoder

63
00:02:35,080 --> 00:02:37,520
再加上微调的这种方式

64
00:02:37,720 --> 00:02:39,760
既然是 decoder 跟微调

65
00:02:39,760 --> 00:02:42,400
它就分开了两个阶段

66
00:02:42,600 --> 00:02:45,760
第一个阶段就是预训练的阶段

67
00:02:45,760 --> 00:02:48,240
利用语言模型进行一个预训练

68
00:02:48,240 --> 00:02:49,320
预学习

69
00:02:49,320 --> 00:02:52,640
接着到第二阶段就需要进行微调

70
00:02:52,680 --> 00:02:55,000
微调就是针对下游任务

71
00:02:55,000 --> 00:02:56,800
或者下游的一些数据

72
00:02:56,800 --> 00:02:58,200
进行 fine tuning

73
00:02:58,200 --> 00:02:59,120
fine tuning 的工作

74
00:02:59,240 --> 00:03:01,440
就会可能加几层 layer 层

75
00:03:01,440 --> 00:03:03,360
或者加几层其他 head

76
00:03:03,360 --> 00:03:04,880
然后进行一个微调的

77
00:03:04,880 --> 00:03:05,640
微调的工作

78
00:03:05,840 --> 00:03:08,120
确实会带来一些新的入参

79
00:03:08,120 --> 00:03:09,600
新的模型层

80
00:03:10,150 --> 00:03:11,040
哎

81
00:03:11,040 --> 00:03:12,360
ZOMI 老师你好

82
00:03:12,360 --> 00:03:14,880
像这种预训练加微调的方式

83
00:03:14,880 --> 00:03:16,840
跟 Bert 非常类似

84
00:03:17,135 --> 00:03:20,080
那它跟 Bert 有什么区别吗 

85
00:03:20,080 --> 00:03:23,240
小新的问题问得非常及时

86
00:03:23,240 --> 00:03:25,440
先来看一看这个图

87
00:03:25,440 --> 00:03:27,600
这个图就是 GPT-1

88
00:03:27,600 --> 00:03:29,680
就 GPT-1 里面的一个图

89
00:03:29,680 --> 00:03:32,760
左边的就是 Transformer 的一个结构

90
00:03:32,760 --> 00:03:34,240
用了 Transformer 的结构之后

91
00:03:34,400 --> 00:03:36,360
针对不同的下游任务

92
00:03:36,520 --> 00:03:38,520
右边就列了 4 个下游任务

93
00:03:38,520 --> 00:03:39,120
4 个下游任务

94
00:03:39,120 --> 00:03:40,560
输了不同的数据之后

95
00:03:40,760 --> 00:03:42,360
通过不同的堆叠方式

96
00:03:42,360 --> 00:03:45,800
然后去组成新的下游任务处理方式

97
00:03:46,400 --> 00:03:47,920
GPT-1 的这种方式

98
00:03:48,080 --> 00:03:50,280
确实跟 Bert 非常类似

99
00:03:50,280 --> 00:03:51,960
但是看下面两个图

100
00:03:51,960 --> 00:03:53,280
有个最大的区别

101
00:03:53,280 --> 00:03:54,920
可以看到左边的图

102
00:03:55,240 --> 00:03:56,560
就是谷歌的 Bert

103
00:03:56,560 --> 00:03:58,200
它一个架构的图

104
00:03:58,200 --> 00:04:01,480
右边就是 OpenAI 的一个 GPT 的图

105
00:04:01,480 --> 00:04:03,080
从下面来看

106
00:04:03,080 --> 00:04:04,920
基本上的层数都是一样的

107
00:04:04,920 --> 00:04:06,600
通过磁相量

108
00:04:06,880 --> 00:04:08,440
传进去变成 embedding

109
00:04:08,440 --> 00:04:10,600
然后给 Transformer 的层

110
00:04:10,600 --> 00:04:11,280
Transformer 层

111
00:04:11,400 --> 00:04:13,320
最后输出的是一些 token

112
00:04:13,880 --> 00:04:15,600
看到最大的区别

113
00:04:16,120 --> 00:04:17,000
像这里面

114
00:04:17,000 --> 00:04:19,680
这个 E2 就是第二个 embedding 层

115
00:04:19,680 --> 00:04:21,880
它会向左边有一个箭头

116
00:04:21,880 --> 00:04:23,440
像 embedding 最后一层

117
00:04:23,440 --> 00:04:26,120
向左边也有一个箭头

118
00:04:26,160 --> 00:04:29,200
但是反观 OpenAI 的 GPT

119
00:04:29,440 --> 00:04:32,160
它基本上只会向右边的箭头

120
00:04:32,160 --> 00:04:34,160
它没有左边的箭头

121
00:04:34,640 --> 00:04:36,480
两者之间最大的区别

122
00:04:36,480 --> 00:04:39,080
就是对任务的处理不太一样

123
00:04:39,080 --> 00:04:41,760
看看左边的一个输入的例子

124
00:04:41,840 --> 00:04:44,080
假设现在有一个

125
00:04:44,080 --> 00:04:45,200
或者有一句话

126
00:04:45,200 --> 00:04:47,160
ZOMI 经常更新什么

127
00:04:47,160 --> 00:04:48,080
在大晚上

128
00:04:48,240 --> 00:04:50,880
这个时候我要预测中间的一个词

129
00:04:50,880 --> 00:04:52,760
可能这里面做了个 musk

130
00:04:52,760 --> 00:04:54,280
ZOMI 经常更新视频

131
00:04:54,280 --> 00:04:55,080
在大晚上

132
00:04:55,080 --> 00:04:56,400
中间这个 musk 的词

133
00:04:56,680 --> 00:04:58,760
是嵌在我左边跟右边

134
00:04:59,000 --> 00:05:01,200
假设我要预测中间的词

135
00:05:01,200 --> 00:05:02,720
我可能会根据左边

136
00:05:02,720 --> 00:05:03,760
根据我前面

137
00:05:03,760 --> 00:05:04,720
根据我后面

138
00:05:04,720 --> 00:05:05,960
根据我上下文

139
00:05:05,960 --> 00:05:07,480
去做一个预测的

140
00:05:07,520 --> 00:05:10,280
但是像 GPT 这种方式

141
00:05:10,560 --> 00:05:12,360
就是 ZOMI 经常在大晚上

142
00:05:12,360 --> 00:05:13,280
更新什么

143
00:05:13,720 --> 00:05:14,360
什么

144
00:05:14,560 --> 00:05:15,200
这时候

145
00:05:15,470 --> 00:05:18,320
基本上我只会根据前文的信息

146
00:05:18,320 --> 00:05:20,920
去预测下一个单词是什么

147
00:05:20,920 --> 00:05:23,160
这种只是做一个简单的

148
00:05:23,160 --> 00:05:24,320
后向的预测

149
00:05:24,320 --> 00:05:25,000
或者预测

150
00:05:25,000 --> 00:05:26,840
我接下来要发生什么

151
00:05:26,960 --> 00:05:28,680
所以他们最大的区别

152
00:05:28,680 --> 00:05:31,080
就是语言任务上的区别

153
00:05:31,080 --> 00:05:32,760
那可以看一下

154
00:05:32,760 --> 00:05:34,240
一个简单的总结

155
00:05:34,240 --> 00:05:35,320
像 GPT-1

156
00:05:35,600 --> 00:05:38,400
它虽然也是以语言模型作为目标

157
00:05:38,400 --> 00:05:40,880
但采用的是单向的语言模型

158
00:05:40,880 --> 00:05:42,160
注意是单向

159
00:05:42,160 --> 00:05:43,040
像 BERT 那种

160
00:05:43,200 --> 00:05:46,360
确实是双向的语言模型

161
00:05:46,720 --> 00:05:47,920
在网络结构方面

162
00:05:48,080 --> 00:05:49,000
BERT 网络模型

163
00:05:49,160 --> 00:05:50,600
更多的是采用了

164
00:05:50,600 --> 00:05:53,160
像 Transformer encoder 的部分

165
00:05:53,160 --> 00:05:55,600
而 GPT 更多的是采用了 

166
00:05:55,600 --> 00:05:59,840
类似于 Transformer decoder 的部分

167
00:06:00,000 --> 00:06:01,360
可以看到下面这个图

168
00:06:01,600 --> 00:06:04,520
就是 GDP-1 里面的一个简单的图

169
00:06:04,520 --> 00:06:05,920
里面主要是用了

170
00:06:05,920 --> 00:06:08,160
Transformer decoder 的模块

171
00:06:08,360 --> 00:06:08,920
简单的

172
00:06:08,920 --> 00:06:11,760
从单个网络模型的结构来看

173
00:06:11,760 --> 00:06:13,440
最大最明显的区别

174
00:06:13,440 --> 00:06:14,520
就是像 BERT

175
00:06:14,840 --> 00:06:16,680
会采用 multi-head attention

176
00:06:16,840 --> 00:06:20,320
像 GDP 会采用 max multi-head attention

177
00:06:20,320 --> 00:06:21,280
这种方式

178
00:06:21,640 --> 00:06:22,520
总结来说

179
00:06:22,600 --> 00:06:23,880
从三个维度

180
00:06:23,880 --> 00:06:25,160
去看他们的区别

181
00:06:25,160 --> 00:06:26,520
第一个是语言模型

182
00:06:26,520 --> 00:06:27,360
到底是单向的

183
00:06:27,360 --> 00:06:28,400
还是双向的

184
00:06:28,480 --> 00:06:30,480
第二个采用了 Transformer

185
00:06:30,480 --> 00:06:31,520
哪个部分

186
00:06:31,520 --> 00:06:32,760
然后去组成的

187
00:06:32,760 --> 00:06:36,040
第三个在某一个具体的结构上面

188
00:06:36,040 --> 00:06:37,640
它到底是 multi-head attention

189
00:06:37,640 --> 00:06:40,160
还是 max multi-head attention

190
00:06:40,160 --> 00:06:41,000
三种

191
00:06:42,240 --> 00:06:43,440
下面来看看

192
00:06:43,440 --> 00:06:45,720
什么是 max multi-head attention

193
00:06:45,960 --> 00:06:46,640
multi-head attention

194
00:06:46,760 --> 00:06:48,520
我就不再多说了

195
00:06:48,520 --> 00:06:49,400
因为在 Transformer

196
00:06:49,400 --> 00:06:50,920
还有 BERT 这些网络模型里面的

197
00:06:50,920 --> 00:06:51,400
大量的

198
00:06:51,400 --> 00:06:53,360
已经做了非常多的例子

199
00:06:53,360 --> 00:06:54,920
网上你搜也很多

200
00:06:54,920 --> 00:06:56,720
来看看 GPT-1 里面的

201
00:06:56,720 --> 00:06:58,040
max multi-head attention

202
00:06:58,040 --> 00:06:59,320
最主要的通俗理解

203
00:06:59,320 --> 00:07:00,960
就是在处理当前词的时候

204
00:07:00,960 --> 00:07:02,360
看不到后面的词

205
00:07:02,360 --> 00:07:03,760
假设我在处理一的时候

206
00:07:03,920 --> 00:07:05,520
我是看不到后面的词

207
00:07:05,520 --> 00:07:07,560
但是我会看到前面的单词

208
00:07:07,560 --> 00:07:08,320
a 和 o

209
00:07:08,320 --> 00:07:09,520
这种就是单向

210
00:07:09,520 --> 00:07:11,280
我去看 a 和 o 的时候

211
00:07:11,400 --> 00:07:13,320
就去预测跟 it 之间的

212
00:07:13,320 --> 00:07:14,240
一个 attention

213
00:07:14,240 --> 00:07:15,320
还有它的分数

214
00:07:15,320 --> 00:07:17,520
还有它之间的关联关系

215
00:07:17,520 --> 00:07:19,480
然后去算 qkb 的词

216
00:07:19,480 --> 00:07:21,840
这个就是 max multi-head attention

217
00:07:21,840 --> 00:07:23,560
一个简单的例子

218
00:07:24,080 --> 00:07:27,480
现在来到 GDP-2

219
00:07:27,480 --> 00:07:29,040
这个模型里面

220
00:07:29,360 --> 00:07:31,240
GDP-2 跟 GDP-1

221
00:07:31,240 --> 00:07:32,120
最大的区别

222
00:07:32,520 --> 00:07:34,360
就是标题

223
00:07:34,360 --> 00:07:36,200
没有了微调的任务

224
00:07:36,200 --> 00:07:37,440
直接使用了

225
00:07:37,440 --> 00:07:39,240
zero shot learning

226
00:07:39,640 --> 00:07:41,040
谈到 zero shot learning

227
00:07:41,360 --> 00:07:43,000
就是小样本学习

228
00:07:43,360 --> 00:07:45,200
来看看小样本学习

229
00:07:45,200 --> 00:07:46,920
具体分为哪几种

230
00:07:47,120 --> 00:07:48,240
现在小样本学习

231
00:07:48,440 --> 00:07:51,480
其实最主要的是下面三种模式

232
00:07:51,480 --> 00:07:53,080
第一种就是 zero shot

233
00:07:53,080 --> 00:07:54,800
就是零样本的学习

234
00:07:54,800 --> 00:07:56,200
针对具体的课程任务

235
00:07:56,360 --> 00:07:58,760
就不需要进行一个微调了

236
00:07:58,760 --> 00:07:59,880
像 one shot learning

237
00:08:00,000 --> 00:08:01,360
就是单样本学习

238
00:08:01,360 --> 00:08:03,160
可能我有小量的样本

239
00:08:03,160 --> 00:08:05,240
进行一个简单的微调

240
00:08:05,240 --> 00:08:07,560
然后去预测更多的任务

241
00:08:07,880 --> 00:08:08,840
那 few shot learning

242
00:08:09,080 --> 00:08:10,840
更多的就是少量的样本

243
00:08:10,840 --> 00:08:12,000
进行学习

244
00:08:12,000 --> 00:08:13,560
进行一个简单的微调

245
00:08:13,560 --> 00:08:15,280
完成特殊的任务

246
00:08:15,440 --> 00:08:16,800
最主要的 ChatGPT2

247
00:08:17,000 --> 00:08:19,200
就是使用了 Few shot learning

248
00:08:19,200 --> 00:08:21,440
这种学习的方式

249
00:08:22,080 --> 00:08:24,400
除了刚才提到的 GPT-2

250
00:08:24,640 --> 00:08:26,680
采用了一种 Zero shot learning 的方式

251
00:08:26,680 --> 00:08:27,920
其实最大的区别

252
00:08:27,920 --> 00:08:30,240
就是因为我引用了 Zero shot learning

253
00:08:30,240 --> 00:08:31,800
我需要更多的参数

254
00:08:31,800 --> 00:08:33,200
更大的网络模型

255
00:08:33,240 --> 00:08:35,240
记录数据的特征

256
00:08:35,440 --> 00:08:36,840
这个时候最有效的办法

257
00:08:36,840 --> 00:08:39,200
就是增大网络模型

258
00:08:39,200 --> 00:08:41,080
还用更大的数据集

259
00:08:41,120 --> 00:08:42,560
从下面这个图可以看到

260
00:08:42,560 --> 00:08:43,920
其实 GPT-2

261
00:08:44,680 --> 00:08:45,320
网络模型

262
00:08:45,320 --> 00:08:49,240
提供了 4 种不同的模型的结构

263
00:08:49,400 --> 00:08:50,160
不同模型结构

264
00:08:50,280 --> 00:08:51,600
更多的是通过 decoder

265
00:08:51,600 --> 00:08:53,480
来不断的去堆叠

266
00:08:53,480 --> 00:08:55,440
所以简单的对比 GPT-2

267
00:08:55,440 --> 00:08:56,360
还有 GPT-1

268
00:08:56,360 --> 00:08:57,600
可以看到 GPT-2

269
00:08:58,040 --> 00:08:59,560
拾弃了微调的方式

270
00:08:59,560 --> 00:09:01,160
使用了 Zero shot learning

271
00:09:01,160 --> 00:09:03,880
因此引入了更大的网络模型

272
00:09:03,880 --> 00:09:05,920
还有更大的数据集

273
00:09:07,040 --> 00:09:09,840
下面来到第三个内容

274
00:09:09,840 --> 00:09:12,440
就是 GPT-3 了

275
00:09:12,560 --> 00:09:13,360
GPT-3

276
00:09:13,600 --> 00:09:16,400
作为一个非常划时代的一个模型

277
00:09:16,400 --> 00:09:18,040
看一看 GDP-3

278
00:09:18,040 --> 00:09:20,880
主要是开启了 NLP 的新方式

279
00:09:20,880 --> 00:09:21,600
Prompt learning

280
00:09:21,600 --> 00:09:23,800
实现了小样本的学习

281
00:09:23,800 --> 00:09:25,080
把 zero shot

282
00:09:25,080 --> 00:09:27,440
精度进一步的提升

283
00:09:27,640 --> 00:09:29,040
其实在一开始

284
00:09:29,160 --> 00:09:30,640
像 Prompt tuning 这种动机

285
00:09:30,800 --> 00:09:31,520
主要是解决

286
00:09:31,520 --> 00:09:33,800
目前 fine tuning 的两个痛点

287
00:09:34,000 --> 00:09:36,960
第一个就是降低语义的差别

288
00:09:36,960 --> 00:09:37,800
降低语义的差别

289
00:09:37,800 --> 00:09:40,440
其实下面这句话有点冗余

290
00:09:40,440 --> 00:09:41,240
很好理解

291
00:09:41,240 --> 00:09:42,440
预训练模型

292
00:09:42,440 --> 00:09:43,440
跟下游任务

293
00:09:43,440 --> 00:09:45,680
其实有一个比较大的区别的

294
00:09:45,680 --> 00:09:46,920
需要引入新的参数

295
00:09:46,920 --> 00:09:48,560
或者新的网络的模型层

296
00:09:48,560 --> 00:09:49,680
针对不同的下游任务

297
00:09:49,840 --> 00:09:51,240
可能需要重新调参

298
00:09:51,240 --> 00:09:52,480
进行一个训练

299
00:09:52,680 --> 00:09:53,320
这个时候

300
00:09:53,640 --> 00:09:55,200
预训练模型跟下游任务

301
00:09:55,200 --> 00:09:56,960
其实脱节还是比较严重的

302
00:09:56,960 --> 00:09:59,160
就像现在的 CV 一样

303
00:09:59,160 --> 00:10:00,920
为什么 CV 没有那么多大模型

304
00:10:00,920 --> 00:10:02,040
就是因为他们现在

305
00:10:02,080 --> 00:10:03,400
还没有一个很好的

306
00:10:03,400 --> 00:10:04,640
统一的范式

307
00:10:04,640 --> 00:10:05,040
当然了

308
00:10:05,040 --> 00:10:06,200
现在慢慢的出现了

309
00:10:06,200 --> 00:10:07,840
不能说完全没有

310
00:10:07,840 --> 00:10:09,440
那回到正题

311
00:10:09,440 --> 00:10:13,200
第二个就是避免过拟和 over fitting

312
00:10:13,200 --> 00:10:14,080
这个工作

313
00:10:14,080 --> 00:10:15,960
因为在 fine tuning 的阶段

314
00:10:15,960 --> 00:10:17,800
会引入新的一些参数

315
00:10:17,800 --> 00:10:18,680
会重新训练

316
00:10:18,680 --> 00:10:20,320
这个时候对预训的模型

317
00:10:20,440 --> 00:10:23,160
就有可能会造成过拟和的问题了

318
00:10:24,400 --> 00:10:26,120
GPT-3 针对这两个问题

319
00:10:26,640 --> 00:10:29,280
提出了新的 Prompt 一种范式

320
00:10:29,280 --> 00:10:31,320
那 Prompt 它一开始其实不叫 Prompt

321
00:10:31,320 --> 00:10:32,520
后来大家总结总结的

322
00:10:32,520 --> 00:10:33,640
或者弄着弄着

323
00:10:33,640 --> 00:10:36,160
就把它重新的命名为 Prompt 了

324
00:10:36,160 --> 00:10:37,600
看看 PPT

325
00:10:37,760 --> 00:10:40,520
这个是一个新的模型里面的一张图

326
00:10:40,520 --> 00:10:41,640
可以看到最上面

327
00:10:41,760 --> 00:10:44,880
就是原始的一种预训练的方式

328
00:10:44,880 --> 00:10:45,640
通过 Prompt tuning

329
00:10:45,800 --> 00:10:47,280
就是我有一句话

330
00:10:47,280 --> 00:10:49,160
去指引我下面的那句话

331
00:10:49,160 --> 00:10:50,320
到底预测是什么

332
00:10:50,320 --> 00:10:51,680
我每一次训练的时候

333
00:10:51,840 --> 00:10:53,360
都会塞一句 Prompt tuning

334
00:10:53,360 --> 00:10:54,280
就是塞一句

335
00:10:54,280 --> 00:10:55,800
指示性的一个语言

336
00:10:55,800 --> 00:10:57,400
或者指示性的一个句子进去

337
00:10:57,400 --> 00:10:59,240
更好的对后面的数据

338
00:10:59,240 --> 00:11:00,400
进行一个预测

339
00:11:00,400 --> 00:11:01,440
或者训练

340
00:11:01,440 --> 00:11:02,520
那后面训练的时候

341
00:11:02,760 --> 00:11:04,760
就可能会把自己里面做一个 mask

342
00:11:04,760 --> 00:11:05,800
比起一个 mask

343
00:11:06,000 --> 00:11:07,240
预训练可能简单的

344
00:11:07,240 --> 00:11:08,080
只有后面

345
00:11:08,080 --> 00:11:10,840
那现在加了一句更好的句子

346
00:11:10,840 --> 00:11:12,600
或者更好的上下文

347
00:11:12,600 --> 00:11:14,720
对它进行一个指导

348
00:11:14,920 --> 00:11:16,560
那下面看看

349
00:11:16,560 --> 00:11:20,320
GPT-3 的一个具体的 GIF 的图

350
00:11:20,320 --> 00:11:21,360
可以看到下面

351
00:11:21,560 --> 00:11:23,280
最上面就是一些

352
00:11:23,280 --> 00:11:24,280
具体的数据

353
00:11:24,720 --> 00:11:26,000
这个就是 Prompt

354
00:11:26,000 --> 00:11:27,680
Prompt 的一些语料

355
00:11:28,160 --> 00:11:29,720
可以看到最上面第一句话

356
00:11:29,840 --> 00:11:30,960
就是 Prompt 语调

357
00:11:30,960 --> 00:11:32,160
Prompt 语调输进来

358
00:11:32,160 --> 00:11:33,360
给 GDP-3 之后

359
00:11:33,680 --> 00:11:35,640
就去对下面的句子

360
00:11:35,640 --> 00:11:37,240
进行一个预测

361
00:11:37,400 --> 00:11:39,720
可能每一次都是预测最后一句话

362
00:11:39,960 --> 00:11:41,440
或者最后一个单词

363
00:11:41,440 --> 00:11:44,120
然后下次再预测下一个单词

364
00:11:44,320 --> 00:11:46,800
这种方式就结合了 GPT-2

365
00:11:46,800 --> 00:11:48,920
然后引用了 Prompt learning

366
00:11:48,920 --> 00:11:49,800
这种方式

367
00:11:49,800 --> 00:11:52,960
完成了 GPT-1 到 GPT-3

368
00:11:52,960 --> 00:11:55,400
的一个整体的升级

369
00:11:55,680 --> 00:11:57,640
现在来总结一下

370
00:11:57,640 --> 00:11:58,960
整个 GPT 系列

371
00:11:59,160 --> 00:12:00,720
从一开始的 fine tuning

372
00:12:00,720 --> 00:12:03,680
再到 Prompt tuning 这种工作

373
00:12:04,720 --> 00:12:07,480
简单的用一个图来概括一下

374
00:12:07,480 --> 00:12:08,840
可以看到下面这个图

375
00:12:09,000 --> 00:12:10,120
从 GPT-1

376
00:12:10,120 --> 00:12:10,720
GPT-2

377
00:12:10,720 --> 00:12:11,440
GPT-3

378
00:12:11,440 --> 00:12:12,920
到 InstructGPT

379
00:12:12,920 --> 00:12:14,680
到最后的 ChatGPT

380
00:12:14,680 --> 00:12:16,000
它们最大的不同

381
00:12:16,000 --> 00:12:18,400
就是一开始从预训练到微调

382
00:12:18,400 --> 00:12:20,840
引入了预训练加 Zero Shot

383
00:12:20,840 --> 00:12:22,520
最后到 GPT-3 的时候

384
00:12:22,680 --> 00:12:25,320
就引入了预训练加 Prompt tuning

385
00:12:25,320 --> 00:12:27,200
而在 InstructGPT 的时候

386
00:12:27,360 --> 00:12:30,000
引入了一个新的学习的方式

387
00:12:30,000 --> 00:12:31,600
instruct learning

388
00:12:31,760 --> 00:12:32,880
下面来看看

389
00:12:32,880 --> 00:12:35,680
什么叫做 instruct learning

390
00:12:35,680 --> 00:12:36,600
所以大家不要觉得

391
00:12:36,600 --> 00:12:38,400
我在重复讲 GPT-3

392
00:12:38,400 --> 00:12:39,240
没有什么意义

393
00:12:39,240 --> 00:12:41,480
慢慢的引导下来

394
00:12:41,480 --> 00:12:42,840
像这种 Prompt tuning

395
00:12:43,120 --> 00:12:44,000
或者 Prompt learning

396
00:12:44,160 --> 00:12:46,160
叫做提示的学习

397
00:12:46,160 --> 00:12:47,640
而 instruct learning

398
00:12:47,960 --> 00:12:50,160
叫做指示性的学习

399
00:12:50,280 --> 00:12:52,400
它们都有相同点和区别

400
00:12:52,600 --> 00:12:54,760
相同点都是想去挖掘

401
00:12:54,760 --> 00:12:55,960
语言模型

402
00:12:56,000 --> 00:12:58,240
本身所具备的知识

403
00:12:58,440 --> 00:13:00,280
这句话其实是废话

404
00:13:01,600 --> 00:13:02,920
目标都是相同的

405
00:13:02,920 --> 00:13:04,640
都是搞 LLM 大模型

406
00:13:04,640 --> 00:13:07,320
来看看区别点在哪

407
00:13:07,400 --> 00:13:08,600
像 Prompt 这种方式

408
00:13:08,760 --> 00:13:12,000
更多的是做一种补全的功能

409
00:13:12,040 --> 00:13:13,640
确实类似于补全

410
00:13:13,640 --> 00:13:14,880
根据上半句

411
00:13:15,000 --> 00:13:17,240
上半句就是 Prompt 的一种提示

412
00:13:17,240 --> 00:13:20,400
生成或者完成下半句的任务

413
00:13:20,400 --> 00:13:22,200
这种更像完形填空

414
00:13:22,200 --> 00:13:24,640
补全整体的语言语调

415
00:13:24,640 --> 00:13:25,920
的一种模型能力

416
00:13:26,360 --> 00:13:27,360
instruct 的方式

417
00:13:27,560 --> 00:13:31,760
更多的是希望激发语言模型的理解能力

418
00:13:31,760 --> 00:13:34,160
我一开始给出一个明确的指示

419
00:13:34,160 --> 00:13:35,720
让模型去做一个

420
00:13:35,720 --> 00:13:37,680
对这个指示的一个认知

421
00:13:37,680 --> 00:13:38,680
或者理解

422
00:13:39,080 --> 00:13:41,000
说起来可能比较难理解

423
00:13:41,000 --> 00:13:42,960
还是来看看一个具体的

424
00:13:42,960 --> 00:13:44,200
或者可爱的例子

425
00:13:44,520 --> 00:13:47,120
第一个就是提示学习

426
00:13:47,120 --> 00:13:48,160
Prompt tunning

427
00:13:48,200 --> 00:13:50,080
假设现在有一句话

428
00:13:50,080 --> 00:13:51,880
他很喜欢这个项链

429
00:13:51,880 --> 00:13:53,200
太什么太好了

430
00:13:53,200 --> 00:13:54,160
太好看了

431
00:13:54,200 --> 00:13:55,880
这种就是对语料

432
00:13:55,880 --> 00:13:57,000
进行一个完形填空

433
00:13:57,000 --> 00:13:58,080
或者补齐

434
00:13:58,440 --> 00:14:00,640
而此次学习更多的

435
00:14:00,640 --> 00:14:03,400
可以判断这句话的情感

436
00:14:03,400 --> 00:14:05,000
我给他买了个项链

437
00:14:05,000 --> 00:14:06,160
他很喜欢

438
00:14:06,160 --> 00:14:08,120
他到底是好还是不好

439
00:14:08,480 --> 00:14:10,560
需要去理解这句话

440
00:14:10,720 --> 00:14:13,240
这也是 GPT 这种单一训练模型

441
00:14:13,440 --> 00:14:14,960
跟 ChatGPT 这种

442
00:14:14,960 --> 00:14:16,400
可学习可反馈的模型

443
00:14:16,400 --> 00:14:17,880
最大的区别

444
00:14:17,880 --> 00:14:20,800
看看右边的几个图

445
00:14:21,040 --> 00:14:22,680
确实还是在这里面

446
00:14:23,080 --> 00:14:24,600
只是通过不同的维度

447
00:14:24,600 --> 00:14:26,160
给大家去分享

448
00:14:26,400 --> 00:14:27,320
像模型微调

449
00:14:27,560 --> 00:14:29,120
这个任务确实比较简单

450
00:14:29,120 --> 00:14:30,000
从预训练

451
00:14:30,000 --> 00:14:31,240
training model

452
00:14:31,240 --> 00:14:32,520
然后再到 fine tuning

453
00:14:32,520 --> 00:14:34,480
接着进行一个预测

454
00:14:34,520 --> 00:14:35,320
而提示学习

455
00:14:35,480 --> 00:14:37,120
只要通过一个简单的预训练

456
00:14:37,120 --> 00:14:38,680
就可以在不同的任务上面

457
00:14:38,680 --> 00:14:39,840
做一个预测

458
00:14:39,880 --> 00:14:40,960
像提示学习

459
00:14:41,160 --> 00:14:42,280
我可能首先

460
00:14:42,400 --> 00:14:43,640
大家都要有一个预训练

461
00:14:44,040 --> 00:14:45,480
然后我会在 BCD

462
00:14:45,480 --> 00:14:45,960
任务里面

463
00:14:46,080 --> 00:14:47,400
做一个仔指示性的学习

464
00:14:47,400 --> 00:14:50,280
接着我回到 A 任务上面做预测

465
00:14:50,560 --> 00:14:52,040
就是希望让模型

466
00:14:52,280 --> 00:14:54,760
更加具备理解能力

467
00:14:54,800 --> 00:14:55,640
提示学习

468
00:14:55,840 --> 00:14:57,080
更加聚焦于

469
00:14:57,080 --> 00:15:00,640
对语料的一种预测生成

470
00:15:01,960 --> 00:15:02,440
好了

471
00:15:02,440 --> 00:15:04,240
这一节的内容就到这里为止

472
00:15:04,240 --> 00:15:04,960
下一节

473
00:15:04,960 --> 00:15:06,240
更加深入的去看看

474
00:15:06,240 --> 00:15:07,360
ChatGPT 里面的

475
00:15:07,360 --> 00:15:08,440
强化学习部分

