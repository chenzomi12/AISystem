<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 深度学习推理系统 (Deep Learning Inference Systems)

# 简介

推理系统（Inference System）是一种用于部署人工智能模型，执行推理任务的人工智能系统，类似传统 Web 服务或移动端应用系统。通过推理系统，可以将深度学习模型部署到云（Cloud）端或者边缘（Edge）端，并服务用户的请求。模型训练过程好比是传统软件工程中的代码开发的过程，而开发完的代码势必要打包，部署给用户使用，那么推理系统就负责应对模型部署的生命周期中遇到的挑战和问题。

当推理系统将完成训练的模型进行部署，并在服务时还需要考虑设计和提供负载均衡，请求调度，加速优化，多副本和生命周期管理等支持。相比深度学习框架等为训练而设计的系统，推理系统不仅关注低延迟，高吞吐，可靠性等设计目标，同时受到资源，服务等级协议（Service-Level Agreement），功耗等约束。本章将围绕深度学习推理系统的设计，实现与优化内容展开，同时还会在最后介绍部署和 MLOps 等内容。

# 内容概览

本章包含以下内容：

- [8.1 推理系统简介](8.1-推理系统简介)
- [8.2 推理系统的低延迟优化](8.2-推理系统的低延迟优化.md)
- [8.3 推理系统的高吞吐优化](8.3-推理系统的高吞吐优化.md)
- [8.4 部署](8.4-部署.md)
- [8.5 MLOps](8.5-MLOps.md)
