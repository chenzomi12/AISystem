1
00:00:00,000 --> 00:00:04,175
字幕生成：qiaokai  字幕校对：A是传奇

2
00:00:05,050 --> 00:00:06,240
Hello大家好

3
00:00:06,240 --> 00:00:09,720
下面的知识估计关注的人或者看的人就确实不多

4
00:00:09,720 --> 00:00:13,680
如果你正在看证明你可能是非常关注这个领域

5
00:00:13,680 --> 00:00:15,520
或者正在做这个领域的

6
00:00:15,520 --> 00:00:18,080
下面我们来到模型转换

7
00:00:18,080 --> 00:00:21,480
里面最重要的一个模块就是模型的优化

8
00:00:21,480 --> 00:00:24,760
这里面最重要的就是对计算图进行优化

9
00:00:24,760 --> 00:00:28,480
我们在上一节里面讲了一下怎么去制定一个计算图

10
00:00:28,480 --> 00:00:30,240
然后还有计算图的基本流程

11
00:00:30,240 --> 00:00:35,080
下面我们来看一下计算图的优化具体的细节内容

12
00:00:35,080 --> 00:00:38,520
就是这里面的details计算图优化的详解

13
00:00:38,520 --> 00:00:42,440
在计算图优化其实我们在上一节里面去给大家普及过

14
00:00:42,440 --> 00:00:47,080
分为basic, extend,还有layout和memory三种的优化方式

15
00:00:47,080 --> 00:00:51,280
三种优化方式对应到整个推理引擎的计算流

16
00:00:51,280 --> 00:00:55,960
在预优化的阶段会进行很多代数相关的一些优化和简化

17
00:00:56,000 --> 00:01:01,240
接着在真正优化阶段会更多的结合我们神经网络的一些知识进行优化

18
00:01:01,240 --> 00:01:04,320
在后优化阶段更多的是对一些数据的格式

19
00:01:04,320 --> 00:01:05,880
内存的布局的重排

20
00:01:05,880 --> 00:01:09,880
还有一些很重要核心的重复的算子的kernel进行合并

21
00:01:10,880 --> 00:01:13,640
在正式进入后面硬核的内容当中

22
00:01:13,640 --> 00:01:17,440
我想希望大家去看一看我之前发的一系列的视频

23
00:01:17,440 --> 00:01:19,480
就是AI编译器的前端优化

24
00:01:19,480 --> 00:01:23,840
因为后面的很多图优化的一些原理都会在这里面

25
00:01:23,840 --> 00:01:26,200
后面基本上就不会讲原理了

26
00:01:26,200 --> 00:01:29,080
这里面更多的都是一些原理性的知识

27
00:01:29,080 --> 00:01:31,760
后面都是一些非常硬核的具体的内容

28
00:01:32,760 --> 00:01:36,680
注意了不是所有图优化都是基于模板去写的

29
00:01:36,680 --> 00:01:40,440
而是只有推理引擎或者大部分推理引擎都会基于模板来写

30
00:01:40,440 --> 00:01:42,640
在AI框架当中它是不一样的

31
00:01:42,640 --> 00:01:44,440
像AI框架我们回顾一下

32
00:01:44,440 --> 00:01:47,200
主要是我们在TVM，我们假设以TVM为例子

33
00:01:47,200 --> 00:01:50,320
它的一个算子融合或者它的一个图优化的方式

34
00:01:50,400 --> 00:01:55,640
是创建了通过AST把python的代码转换成为TVM里面的Relay IR

35
00:01:55,640 --> 00:02:01,160
然后便利这个Relay IR或者relate数去创建整个DAG图

36
00:02:01,160 --> 00:02:05,080
通过DAG图用于后面的支配树的分析

37
00:02:05,080 --> 00:02:09,640
有了支配树之后就会应用真正的算子融合的一些算法

38
00:02:09,640 --> 00:02:11,800
去实现计算图的优化

39
00:02:11,800 --> 00:02:16,960
可以看到像TVM这种更多的是去发现一些常用的规则

40
00:02:16,960 --> 00:02:19,720
去对计算图进行一个融合优化

41
00:02:21,280 --> 00:02:22,720
ZOMI老师你好

42
00:02:22,720 --> 00:02:24,240
我有个问题

43
00:02:24,240 --> 00:02:29,320
像你刚才提到像AI框架或者AI编译器它的图优化

44
00:02:29,320 --> 00:02:34,920
采用基于规则树或者特殊的树的IR的方式进行融合优化吗

45
00:02:34,920 --> 00:02:39,160
那为什么推理引擎里面的图优化采用hard code

46
00:02:39,160 --> 00:02:42,600
硬编码或者模型匹配的方式呢

47
00:02:43,200 --> 00:02:46,080
你问的这个问题非常好

48
00:02:46,080 --> 00:02:48,000
我简单复述理解一下

49
00:02:48,000 --> 00:02:51,360
其实像我们之前讲到的AI编译器或者AI框架

50
00:02:51,360 --> 00:02:55,720
更多的做一些计算图的优化是通过编译方式去做的

51
00:02:55,720 --> 00:02:57,800
而推理引擎的图优化

52
00:02:57,800 --> 00:03:01,640
更多的是基于模板匹配或者一些hard code的方式去写的

53
00:03:01,640 --> 00:03:04,880
大家都知道通过hard code或者模板匹配的方式

54
00:03:04,880 --> 00:03:07,200
确实不能覆盖非常多的场景

55
00:03:07,200 --> 00:03:09,600
只能覆盖一些有用常用的场景

56
00:03:09,600 --> 00:03:11,200
而通过编译的方式

57
00:03:11,320 --> 00:03:14,080
确实我们可以覆盖很多常规的一些应用

58
00:03:14,080 --> 00:03:15,000
正因为这个原因

59
00:03:15,120 --> 00:03:20,240
我们在推理引擎大部分都是针对于我们一些常用的一些模型进行部署

60
00:03:20,240 --> 00:03:22,840
而AI框架因为大家用来做创新的

61
00:03:22,840 --> 00:03:25,160
所以更多的去考虑到长尾的问题

62
00:03:25,160 --> 00:03:28,560
而AI框架大部分都是在计算服务中心

63
00:03:28,560 --> 00:03:31,240
或者有很强的算力平台上面去执行的

64
00:03:31,240 --> 00:03:33,960
所以说时间对它来说不是说非常重要

65
00:03:33,960 --> 00:03:37,960
我们可以采取很多GIT的编译方式来去提升一些性能

66
00:03:37,960 --> 00:03:41,240
而推理引擎图优化大部分都是离线的

67
00:03:41,240 --> 00:03:43,360
或者我们叫做AOT的方式

68
00:03:43,400 --> 00:03:45,120
进行一些模板匹配或hard code

69
00:03:45,120 --> 00:03:48,280
更好的去覆盖我们主要的场景就可以了

70
00:03:48,280 --> 00:03:49,680
这也是它们最大的区别

71
00:03:51,080 --> 00:03:53,880
下面我们来看看计算图优化的详解

72
00:03:53,880 --> 00:03:57,080
计算图优化详解里面的内容特别的多

73
00:03:57,880 --> 00:04:01,600
像是基础的图优化的内容就特别的多了

74
00:04:01,600 --> 00:04:02,800
包括常量的折叠

75
00:04:02,920 --> 00:04:04,080
冗余节点的消除

76
00:04:04,080 --> 00:04:04,840
算子的融合

77
00:04:04,840 --> 00:04:05,520
算子替换

78
00:04:05,520 --> 00:04:06,800
用算子的前移

79
00:04:06,800 --> 00:04:08,320
我们会讲非常多的内容

80
00:04:08,320 --> 00:04:10,600
可能里面会分开好几个内容来去讲

81
00:04:10,600 --> 00:04:13,000
更多的是去讲真正的融合规则

82
00:04:13,120 --> 00:04:15,160
而不是去讲为什么要这么融合了

83
00:04:15,160 --> 00:04:17,120
所以说下面的内容会越来越难

84
00:04:17,120 --> 00:04:18,800
或者也越来越难懂

85
00:04:18,800 --> 00:04:20,320
大家去记住就好了

86
00:04:20,320 --> 00:04:21,920
现在我们看第一个内容

87
00:04:21,920 --> 00:04:24,520
就是我们的O1 Constant Folding

88
00:04:24,520 --> 00:04:25,760
常量折叠

89
00:04:25,760 --> 00:04:28,560
那常量折叠它其实是编译优化的一个技术

90
00:04:28,840 --> 00:04:31,640
对我们编译时的常量或者常量的表达式

91
00:04:31,640 --> 00:04:34,280
进行计算来去简化代码的

92
00:04:34,800 --> 00:04:37,160
在计算图里面就可以预先的去确定

93
00:04:37,160 --> 00:04:39,920
输出节点的值替换成常量

94
00:04:39,920 --> 00:04:42,200
就把常量这个折叠隐掉了

95
00:04:42,320 --> 00:04:45,400
然后对计算图进行一些结构简化的操作

96
00:04:45,400 --> 00:04:47,840
我们下面看一些具体的例子

97
00:04:47,840 --> 00:04:49,680
好像现在有的一些constant fold

98
00:04:49,760 --> 00:04:51,200
就是常量的折叠

99
00:04:51,200 --> 00:04:52,560
还有binary的折叠

100
00:04:52,560 --> 00:04:53,680
我们看一下具体的图

101
00:04:53,680 --> 00:04:55,760
这里面后面的字我就不简单的读了

102
00:04:56,080 --> 00:04:58,480
像这种我有两个常量输进去

103
00:04:58,480 --> 00:05:00,360
然后有一个Op1和Op2

104
00:05:00,800 --> 00:05:02,080
但是可以看到Op1

105
00:05:02,200 --> 00:05:04,600
它是接收两个常量作为输入

106
00:05:04,600 --> 00:05:06,560
这些常量在我们离线的时候

107
00:05:06,840 --> 00:05:08,480
其实可以把它算出来

108
00:05:08,480 --> 00:05:09,360
把它算完之后

109
00:05:09,520 --> 00:05:10,880
作为一个新的常量

110
00:05:10,920 --> 00:05:11,920
输给我们的Op2

111
00:05:11,920 --> 00:05:14,040
这种就是最常见的常量折叠

112
00:05:14,920 --> 00:05:16,160
推理引擎里面最重要的

113
00:05:16,160 --> 00:05:17,600
或者ZOMI之前写过的

114
00:05:17,600 --> 00:05:19,120
这种一个常量折叠的公式

115
00:05:19,640 --> 00:05:21,360
其实是BN折叠

116
00:05:21,360 --> 00:05:23,280
BN折叠也就是利用了这个原理

117
00:05:23,280 --> 00:05:25,080
所以大家简单的理解一下就好了

118
00:05:25,720 --> 00:05:26,880
下面我们看一下

119
00:05:26,880 --> 00:05:29,120
ExpandDims的一种折叠方式

120
00:05:30,280 --> 00:05:32,320
当ExpandDims的第二个维度

121
00:05:32,320 --> 00:05:34,440
就指定维度的输入的时候是常量

122
00:05:34,440 --> 00:05:37,120
那我们就可以把它直叠进去参数的方式

123
00:05:37,120 --> 00:05:39,320
放在ExpandDims这个算子里面

124
00:05:39,440 --> 00:05:41,240
然后就少了一个算子

125
00:05:41,240 --> 00:05:43,200
因为constant它有可能是一个算子

126
00:05:43,200 --> 00:05:46,200
或者有可能是占用内存的一块空间

127
00:05:46,520 --> 00:05:48,440
下面还有binary折叠

128
00:05:48,560 --> 00:05:49,320
binary折叠

129
00:05:49,440 --> 00:05:51,360
其实跟刚才的ExpandDims的

130
00:05:51,360 --> 00:05:52,240
折叠差不多

131
00:05:52,240 --> 00:05:54,160
它里面的输入可能是个标量

132
00:05:54,280 --> 00:05:57,600
这时候就把标量变成binary的一个参数

133
00:05:57,600 --> 00:05:59,440
然后进行一个计算的

134
00:05:59,600 --> 00:06:00,520
这个时候可以看到

135
00:06:00,520 --> 00:06:01,840
少了一个计算的节点

136
00:06:01,840 --> 00:06:02,920
对我们的计算来说

137
00:06:03,040 --> 00:06:04,560
确实有很多的好处

138
00:06:05,560 --> 00:06:07,080
接着我们看一下第二个内容

139
00:06:07,080 --> 00:06:08,200
就是计算图的优化

140
00:06:08,200 --> 00:06:10,320
冗余节点的消除

141
00:06:10,320 --> 00:06:11,160
冗余节点的消除

142
00:06:11,160 --> 00:06:12,760
里面这些内容特别的多

143
00:06:12,880 --> 00:06:15,400
就没有用的节点进行消除

144
00:06:15,600 --> 00:06:17,000
这里面我们分开好几个分类

145
00:06:17,000 --> 00:06:19,000
第一个就是op本身没有意义

146
00:06:19,000 --> 00:06:21,600
就这个算子本身是没有意义的

147
00:06:21,600 --> 00:06:23,440
所以我们就会把它去掉

148
00:06:23,440 --> 00:06:25,720
例如cast转换之前的前后类型

149
00:06:25,720 --> 00:06:26,600
都是相同的

150
00:06:26,600 --> 00:06:29,160
concat只有一个输入的tensor

151
00:06:29,160 --> 00:06:30,520
还有NoOp Print

152
00:06:30,520 --> 00:06:31,760
Assert StopGradient

153
00:06:31,760 --> 00:06:32,360
Split

154
00:06:32,440 --> 00:06:35,040
这些算子其实都可以干掉

155
00:06:35,240 --> 00:06:37,200
这个时候我们就会有一系列的规则

156
00:06:37,200 --> 00:06:38,600
去写一系列的模板

157
00:06:38,600 --> 00:06:40,600
去删掉这些没有用的算子

158
00:06:40,600 --> 00:06:41,680
包括dropout这种

159
00:06:41,680 --> 00:06:42,800
在训练的过程当中

160
00:06:42,920 --> 00:06:44,000
去有用的算子

161
00:06:44,000 --> 00:06:44,920
我们在退集的时候

162
00:06:45,120 --> 00:06:46,480
或者在推理引擎转换的时候

163
00:06:46,920 --> 00:06:48,440
都会把它干掉

164
00:06:48,440 --> 00:06:51,080
那么简单的看几个图

165
00:06:51,080 --> 00:06:53,760
像这里面有一个冗余的算子

166
00:06:53,760 --> 00:06:54,720
输入进来的时候

167
00:06:54,800 --> 00:06:56,640
我们就会把这个算子干掉

168
00:06:56,840 --> 00:06:59,400
这种算子的输入跟输出

169
00:06:59,400 --> 00:07:01,080
会把上一个算子的输入跟输出

170
00:07:01,080 --> 00:07:02,320
把它连回来

171
00:07:02,520 --> 00:07:05,080
但是有种就是这个算子的输出

172
00:07:05,240 --> 00:07:07,080
对下一个算子是没有意义的

173
00:07:07,280 --> 00:07:10,360
这个时候我们就会把它切成两个子图

174
00:07:10,360 --> 00:07:12,080
一个子图就是op1 input

175
00:07:12,080 --> 00:07:15,080
一个子图就是op2进行一个具体的计算

176
00:07:16,200 --> 00:07:19,600
第三种情况就是像冗余的算子

177
00:07:19,600 --> 00:07:21,880
它的输入对它来说是没用的

178
00:07:22,160 --> 00:07:24,400
既然这个输入对它来说是没用的

179
00:07:24,400 --> 00:07:26,880
那前面的计算是不是也是没用的

180
00:07:27,360 --> 00:07:28,080
既然是这样

181
00:07:28,080 --> 00:07:30,640
那就会迭代式的去网上

182
00:07:30,640 --> 00:07:33,160
轮循删除往上的节点

183
00:07:33,160 --> 00:07:34,840
只要这个节点的输入没有意义

184
00:07:35,000 --> 00:07:37,280
证明这个节点它是没有意义的

185
00:07:37,280 --> 00:07:39,000
因为它的输出没有人接

186
00:07:39,160 --> 00:07:41,160
这个时候就可以把这个算子干掉

187
00:07:41,160 --> 00:07:42,840
它的算子如果也是这种情况

188
00:07:43,000 --> 00:07:44,560
也会一直轮循

189
00:07:44,560 --> 00:07:46,640
把它往上的算子也干掉

190
00:07:46,640 --> 00:07:49,760
这种就是删除op就是这个算子

191
00:07:49,760 --> 00:07:50,920
本身没有意义的算子

192
00:07:50,920 --> 00:07:52,040
它就有三种方式

193
00:07:52,800 --> 00:07:56,040
接下来我们看一下op的参数没有意义

194
00:07:56,040 --> 00:07:58,600
也就是说这个算子其实是有意义的

195
00:07:58,600 --> 00:08:02,400
但是当你设定为具体某些参数的时候

196
00:08:02,400 --> 00:08:03,520
或者某些区别的时候

197
00:08:03,760 --> 00:08:04,720
它就没有意义

198
00:08:04,800 --> 00:08:06,920
我们举个最典型的例子

199
00:08:06,920 --> 00:08:08,000
就tensor的cast

200
00:08:08,360 --> 00:08:11,000
cast的算子主要是对数据的排布

201
00:08:11,000 --> 00:08:12,560
进行一个转换的

202
00:08:12,560 --> 00:08:14,400
假设我的输入的参数

203
00:08:14,400 --> 00:08:16,280
等于输出的参数的时候

204
00:08:16,880 --> 00:08:18,040
假设我现在有个数据

205
00:08:18,160 --> 00:08:19,080
NCHW

206
00:08:19,080 --> 00:08:21,640
我把它cast成NCHW

207
00:08:22,080 --> 00:08:23,760
我cast前后都是相同的

208
00:08:23,880 --> 00:08:25,000
我干嘛要这个算子

209
00:08:25,320 --> 00:08:27,800
所以就可以把这个算子干掉

210
00:08:28,840 --> 00:08:30,320
当然了我们还有很多种情况

211
00:08:30,440 --> 00:08:32,280
像是list的index大等于0

212
00:08:32,280 --> 00:08:34,920
index_end等于channel-1的时候

213
00:08:35,080 --> 00:08:36,160
这个算子是没有意义的

214
00:08:36,160 --> 00:08:37,280
像expand的时候

215
00:08:37,440 --> 00:08:40,320
当输出的shape等于输入的shape的时候

216
00:08:40,320 --> 00:08:41,520
也是没有意义的

217
00:08:41,760 --> 00:08:43,800
当pooling的参数或者滑窗的等于1乘1

218
00:08:43,800 --> 00:08:45,200
它也是没有用的

219
00:08:45,200 --> 00:08:47,200
所以我们看一下下面的图

220
00:08:48,120 --> 00:08:49,480
假设cast的算子

221
00:08:49,480 --> 00:08:51,440
它的source等于destination的时候

222
00:08:51,640 --> 00:08:52,600
这个算子就没有意义

223
00:08:52,600 --> 00:08:54,640
我们把cast的算子干掉

224
00:08:54,920 --> 00:08:56,320
像这种ExpandDims的时候

225
00:08:56,320 --> 00:08:58,720
假设这个shape跟输入的shape是一样的

226
00:08:58,920 --> 00:09:01,520
我们就把这个算子干掉

227
00:09:01,840 --> 00:09:02,800
像pooling的时候

228
00:09:02,880 --> 00:09:04,480
我们等于1乘1也是没有用的

229
00:09:04,480 --> 00:09:06,880
像start等于某些特殊特性的时候

230
00:09:06,880 --> 00:09:07,720
也是没有用的

231
00:09:07,720 --> 00:09:09,760
也把这个算子干掉

232
00:09:10,040 --> 00:09:12,240
像这种确实在我们的神经网络里面

233
00:09:12,360 --> 00:09:14,040
会出现大量的冗余节点

234
00:09:14,040 --> 00:09:15,960
而总比在具体的实践当中

235
00:09:16,160 --> 00:09:17,800
就我之前在项目交付的时候

236
00:09:17,960 --> 00:09:19,600
会做过相关的工作

237
00:09:19,600 --> 00:09:21,640
确实也会把这些算子干掉之后

238
00:09:21,800 --> 00:09:23,240
性能提升了非常的多

239
00:09:23,240 --> 00:09:26,200
而且网络模型确实简化了非常的多

240
00:09:26,200 --> 00:09:29,560
另外还有一些OP的位置没有意义的

241
00:09:30,080 --> 00:09:31,440
这里面有非常的多

242
00:09:31,600 --> 00:09:34,400
所以大家如果真想了解里面的细节

243
00:09:34,600 --> 00:09:36,720
可以翻看我Github上面里面的

244
00:09:36,720 --> 00:09:39,000
关于推理引擎的很多的内容

245
00:09:39,000 --> 00:09:40,640
很多的slide就PPT

246
00:09:40,640 --> 00:09:42,440
我都已经开源开放了

247
00:09:42,440 --> 00:09:43,720
还有一些对应的video

248
00:09:43,720 --> 00:09:45,000
也会放在这里面

249
00:09:45,920 --> 00:09:47,280
回到我们的话题

250
00:09:47,280 --> 00:09:49,320
我们这里面就不一一去过了

251
00:09:49,320 --> 00:09:51,760
我们看一个具体的一些例子

252
00:09:52,000 --> 00:09:53,240
确实非常的多

253
00:09:53,400 --> 00:09:54,680
这里面除了Flatten的消除

254
00:09:54,680 --> 00:09:55,680
重复的消除

255
00:09:55,680 --> 00:09:56,760
还有很多

256
00:09:56,960 --> 00:09:59,280
我们看一些具体的一些图

257
00:09:59,400 --> 00:10:02,360
看图说话确实非常之乐观

258
00:10:02,360 --> 00:10:04,200
像这种Cast的算子是没有意义的

259
00:10:04,200 --> 00:10:05,560
我们就会把它删掉

260
00:10:05,560 --> 00:10:07,320
像UnSqueeze的算子时候没有意义

261
00:10:07,320 --> 00:10:08,600
也把它删掉

262
00:10:08,840 --> 00:10:11,040
有时候我的input给OP1的时候

263
00:10:11,200 --> 00:10:12,480
OP1的输出

264
00:10:12,480 --> 00:10:13,440
它是没有输出的

265
00:10:13,440 --> 00:10:14,240
没有算子接的

266
00:10:14,360 --> 00:10:15,200
我算来干嘛了

267
00:10:15,640 --> 00:10:16,440
它没有人要

268
00:10:16,560 --> 00:10:20,080
所以这个时候就可以把这条分支给干掉

269
00:10:20,920 --> 00:10:22,560
最后还有就是Global pooling

270
00:10:22,560 --> 00:10:23,920
它后面接一些Reshape

271
00:10:23,920 --> 00:10:24,800
或者Flatten的时候

272
00:10:24,880 --> 00:10:26,040
其实也是没有意义的

273
00:10:26,040 --> 00:10:28,680
我们也其实可以把这些算子干掉

274
00:10:28,800 --> 00:10:30,440
所以说干掉的这些算子

275
00:10:30,440 --> 00:10:32,040
可以还有非常多

276
00:10:32,720 --> 00:10:33,840
像冗余节点消除

277
00:10:33,840 --> 00:10:35,880
确实里面写了非常多的past

278
00:10:36,320 --> 00:10:38,240
后面这两个图其实比较相似

279
00:10:38,240 --> 00:10:39,600
假设我有两个Reshape

280
00:10:39,600 --> 00:10:41,440
两个Reshape都是相反的时候

281
00:10:41,440 --> 00:10:43,640
这个时候我们就可以把它都干掉

282
00:10:43,640 --> 00:10:45,040
Cast A到B

283
00:10:45,040 --> 00:10:46,600
然后Cast B到A

284
00:10:46,800 --> 00:10:49,240
我还不如把这两个算子直接干掉就好了

285
00:10:49,240 --> 00:10:50,440
它的语义相反

286
00:10:50,840 --> 00:10:54,840
这里面就讲到了OP前后两个的语义相反

287
00:10:55,080 --> 00:10:57,960
这个时候两个OP都可以把它干掉

288
00:10:58,560 --> 00:11:00,880
这里面也有非常多

289
00:11:00,880 --> 00:11:02,520
例如Squeeze跟Expand

290
00:11:02,600 --> 00:11:04,040
它确实语义相反的

291
00:11:04,040 --> 00:11:05,240
还有两个Cast

292
00:11:05,480 --> 00:11:07,160
它可能也是语义相反的

293
00:11:07,160 --> 00:11:09,360
像我们量化和反量化

294
00:11:09,360 --> 00:11:10,680
假设它连在一起

295
00:11:10,680 --> 00:11:11,840
也是没有意义的

296
00:11:11,840 --> 00:11:12,920
可以把它删掉

297
00:11:12,920 --> 00:11:15,680
Concat跟Slice也是可以删掉

298
00:11:15,680 --> 00:11:17,400
语义相反的我们都可以干掉

299
00:11:17,400 --> 00:11:19,400
我们可以看一下下面的具体的图

300
00:11:19,400 --> 00:11:21,800
像ExpandDims就是扩充的维度

301
00:11:21,800 --> 00:11:23,880
Squeeze就把不同的维度合在一起

302
00:11:23,880 --> 00:11:25,640
这种确实也可以干掉

303
00:11:25,640 --> 00:11:27,040
我一个扩充一个合并

304
00:11:27,160 --> 00:11:28,960
我就干脆啥都不做就行了

305
00:11:28,960 --> 00:11:31,240
当然它里面的参数或者里面的轴

306
00:11:31,680 --> 00:11:32,680
大家要注意一下

307
00:11:32,680 --> 00:11:35,080
就得相对应才行

308
00:11:35,440 --> 00:11:37,040
后面像这种Concat跟Slice

309
00:11:37,640 --> 00:11:39,440
还不如我直接Slice就完了

310
00:11:39,440 --> 00:11:42,560
所以说它基本上很多很相似的地方

311
00:11:42,560 --> 00:11:44,160
下面我们看一下

312
00:11:44,640 --> 00:11:46,640
冗余节点消除的第4个内容

313
00:11:46,760 --> 00:11:48,680
冗余节点消除特别的多

314
00:11:48,680 --> 00:11:51,720
第4个内容就是公共子图的消除

315
00:11:51,720 --> 00:11:54,800
公共子图就是把一些有大模块的

316
00:11:54,800 --> 00:11:56,320
两个完全相同的子图

317
00:11:56,480 --> 00:11:57,280
把它干掉

318
00:11:57,360 --> 00:11:58,240
我们看一下这个图

319
00:11:58,240 --> 00:12:00,160
我们假设有三个输入

320
00:12:00,160 --> 00:12:03,600
三个输入对应的是给OP1 OP2 OP3去执行

321
00:12:03,600 --> 00:12:05,280
假设我们右边又有一个分支

322
00:12:05,280 --> 00:12:07,840
同样给OP1 OP2 OP3去执行的时候

323
00:12:07,840 --> 00:12:09,720
这个时候我们可以把红色

324
00:12:09,720 --> 00:12:11,200
把黄色的这块干掉

325
00:12:11,200 --> 00:12:12,600
黄色我没有色盲

326
00:12:13,240 --> 00:12:14,240
把黄色的这块干掉

327
00:12:14,240 --> 00:12:16,080
就剩下左边的子图了

328
00:12:16,080 --> 00:12:18,160
这种就是公共子图的消除

329
00:12:19,520 --> 00:12:21,920
其实刚才讲了很多算法

330
00:12:21,920 --> 00:12:23,120
怎么去能寻的图

331
00:12:23,120 --> 00:12:25,000
其实大部分都是一些经典的

332
00:12:25,040 --> 00:12:26,680
leetcode算法的题目

333
00:12:26,680 --> 00:12:27,880
例如公共子图消除

334
00:12:28,000 --> 00:12:29,760
就是寻找公共子树

335
00:12:29,760 --> 00:12:31,000
所以有兴趣的同学

336
00:12:31,160 --> 00:12:33,240
或者你不明白为什么要刷leetcode

337
00:12:33,240 --> 00:12:35,360
就你发现为什么我写的代码

338
00:12:35,360 --> 00:12:36,480
基本上都跟你关系相关的

339
00:12:36,480 --> 00:12:38,520
为什么都是做树的检索

340
00:12:38,520 --> 00:12:41,960
所以其实这个就是刷leetcode的好处

341
00:12:41,960 --> 00:12:44,640
或者为什么我们很多大厂做面试的时候

342
00:12:44,800 --> 00:12:47,400
也是需要进行一个leetcode的面试

343
00:12:47,400 --> 00:12:49,800
这真的是有它的好处和理由的

344
00:12:49,800 --> 00:12:52,000
可能你平时在做一些简单的应用

345
00:12:52,000 --> 00:12:52,640
API的时候

346
00:12:52,640 --> 00:12:54,160
只是调用人家一些库

347
00:12:54,200 --> 00:12:56,080
你觉得你根本没有必要去写

348
00:12:56,080 --> 00:12:58,200
但是你去写一些内核的代码

349
00:12:58,200 --> 00:13:00,560
或者真正做一些开创性的内容的时候

350
00:13:00,560 --> 00:13:01,360
你就会发现

351
00:13:01,840 --> 00:13:02,720
里面很多知识

352
00:13:02,720 --> 00:13:04,560
真的是可以借鉴过来的

353
00:13:04,920 --> 00:13:06,640
好了今天的内容就这么多

354
00:13:06,640 --> 00:13:08,240
我们将会在下一节当中

355
00:13:08,240 --> 00:13:12,560
分享更多的图优化的一些算法和内容

356
00:13:12,560 --> 00:13:13,520
谢谢各位

357
00:13:13,520 --> 00:13:14,960
拜了个拜

358
00:13:15,920 --> 00:13:17,600
卷的不行了

359
00:13:17,600 --> 00:13:19,040
记得一键三连加关注

360
00:13:19,440 --> 00:13:20,800
所有的内容都会开源

361
00:13:20,800 --> 00:13:22,600
在下面这条链接里面

362
00:13:22,600 --> 00:13:24,000
拜了个拜

