1
00:00:00,000 --> 00:00:04,325
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:05,050 --> 00:00:08,442
Hello大家好,我是一直坚持分享AI系统

3
00:00:08,442 --> 00:00:11,020
但是关注的人并不是很多的ZOMI

4
00:00:11,200 --> 00:00:16,000
今天我们来到推理系统里面的模型小型化

5
00:00:16,000 --> 00:00:17,925
今天我要给大家去分享的一个内容

6
00:00:17,925 --> 00:00:21,115
就是CNN网络模型的小型化

7
00:00:21,400 --> 00:00:25,400
为啥会单独分开CNN跟Transformer模型的小型化呢?

8
00:00:25,400 --> 00:00:26,970
就有两个内容了

9
00:00:26,970 --> 00:00:30,800
是因为其实我们现在Transformer非常非常的火

10
00:00:30,800 --> 00:00:31,829
火到一塌糊涂

11
00:00:31,829 --> 00:00:33,829
现在你不发Transformer的模型呢

12
00:00:33,829 --> 00:00:35,379
说实话它的精度很难上去

13
00:00:35,400 --> 00:00:37,479
而且Transformer确实很outstanding

14
00:00:37,479 --> 00:00:40,600
可以发很多新的一些paper

15
00:00:40,600 --> 00:00:41,728
但是呢

16
00:00:41,728 --> 00:00:44,800
Transformer根据我们在对客户的一些交付

17
00:00:44,800 --> 00:00:47,150
它的落地并不是说非常的理想

18
00:00:47,150 --> 00:00:48,355
而现在用的更多 

19
00:00:48,355 --> 00:00:49,659
或者在落地场景里面

20
00:00:49,659 --> 00:00:52,400
做模型的小型化或者结构的小型化

21
00:00:52,400 --> 00:00:54,600
更多的是以CNN为主

22
00:00:58,200 --> 00:01:02,600
那可以看到其实我们的网络模型的精度越高

23
00:01:02,600 --> 00:01:05,200
确实我们的模型的参数量越大

24
00:01:05,200 --> 00:01:10,000
好像DenseNet, SENet,还有MobileNet都是一些相同精度

25
00:01:10,000 --> 00:01:14,000
但是模型量确实小了非常多的模型推出来了

26
00:01:15,800 --> 00:01:18,371
今天我主要是聚焦这种CNN模型的

27
00:01:18,371 --> 00:01:20,893
小型化的一些结构给大家去分享的

28
00:01:21,200 --> 00:01:22,225
在分享的过程当中

29
00:01:22,225 --> 00:01:24,600
我们以一些系列作为例子

30
00:01:24,600 --> 00:01:24,621
那我们看一看简单的去了解一下

31
00:01:24,621 --> 00:01:27,009
那我们看一看简单的去了解一下

32
00:01:27,009 --> 00:01:29,000
今天主要分享的内容有哪些

33
00:01:29,000 --> 00:01:29,009
首先呢

34
00:01:29,009 --> 00:01:29,974
首先呢

35
00:01:29,974 --> 00:01:33,400
我们会去按时间的序号去排一排

36
00:01:33,400 --> 00:01:37,200
我们会了解一下SqueezeNet,ShuffleNet,MobileNet

37
00:01:37,200 --> 00:01:39,200
了解完这一套系列之后呢

38
00:01:39,200 --> 00:01:44,200
我们再去简单的去看一看ESPNet,FBNet,EfficientNet

39
00:01:44,200 --> 00:01:47,200
最后还有华为诺亚的GhostNet

40
00:01:47,200 --> 00:01:49,600
那了解完这些内容之后呢

41
00:01:49,600 --> 00:01:52,600
我们再会去做一个具体的总结

42
00:01:52,600 --> 00:01:56,400
现在我们一起去看看具体的算法

43
00:01:56,400 --> 00:02:01,000
去了解一下轻量级网络模型具体是怎么设计的

44
00:02:01,000 --> 00:02:03,150
它跟传统的CNN卷积一层一层的

45
00:02:03,150 --> 00:02:05,728
堆下来这种RestNet有哪些区别

46
00:02:08,400 --> 00:02:10,600
在轻量化主干网络模型里面呢

47
00:02:10,600 --> 00:02:13,600
第一个比较著名的是SqueezeNet

48
00:02:13,600 --> 00:02:16,600
跟AlexNet相比有了50倍的模型的压缩

49
00:02:16,600 --> 00:02:18,263
那下面呢我们看看SqueezeNet

50
00:02:18,263 --> 00:02:22,000
具体有哪些不一样的网络模型结构的设计

51
00:02:22,000 --> 00:02:22,800
看到这个图呢

52
00:02:22,800 --> 00:02:25,200
其实SqueezeNet提出一个最重要的model

53
00:02:25,200 --> 00:02:27,900
就是它的一个FireModel

54
00:02:27,900 --> 00:02:30,300
这个FireModel有两个模块去组成的

55
00:02:30,300 --> 00:02:31,800
一个是Squeeze模块

56
00:02:31,800 --> 00:02:34,000
一个是Expand模块

57
00:02:34,000 --> 00:02:35,300
Squeeze这个模块呢

58
00:02:35,300 --> 00:02:39,200
主要是由一系列连续的1x1 的卷积进行组成

59
00:02:39,200 --> 00:02:43,200
它最重要的主要是对一些FeatureMap的通道数进行改变

60
00:02:43,200 --> 00:02:45,900
减少了整个通道数

61
00:02:45,900 --> 00:02:47,000
Expand 的模块呢

62
00:02:47,000 --> 00:02:48,700
主要是由1x1的卷积核心

63
00:02:48,700 --> 00:02:51,200
还有 3x3 的卷积核进程组成的

64
00:02:51,200 --> 00:02:51,900
它们之间呢

65
00:02:51,900 --> 00:02:54,600
通过Concat进行汇聚起来

66
00:02:54,600 --> 00:02:55,300
下面呢

67
00:02:55,300 --> 00:03:00,500
我们往下浏览去看看SqueezeNet的一个网络模型的结构

68
00:03:00,500 --> 00:03:02,900
可以看到SqueezeNet的网络模型啊

69
00:03:02,900 --> 00:03:05,400
会比AlexNet的网络模型更加深

70
00:03:05,400 --> 00:03:08,200
而且会有很多个FireModel进行组成

71
00:03:08,200 --> 00:03:09,600
不同的组织模块呢

72
00:03:09,600 --> 00:03:12,300
或者不同的网络模型的组成了又有不同的精度

73
00:03:13,700 --> 00:03:15,000
那对应来说啊

74
00:03:15,000 --> 00:03:17,200
SqueezeNet V1这篇文章呢

75
00:03:17,200 --> 00:03:19,400
是发表在ICLR 2017

76
00:03:19,400 --> 00:03:22,700
而它真正的是在2016的时候发表出来了

77
00:03:22,700 --> 00:03:23,600
我们现在呢

78
00:03:23,600 --> 00:03:26,000
看看SqueezeNet V2这个版本

79
00:03:28,600 --> 00:03:32,500
下面我们看一下SqueezeNet系列里面的第二篇SqueezeNet

80
00:03:32,500 --> 00:03:34,600
我们也叫做SqueezeNet V2

81
00:03:34,600 --> 00:03:35,900
那这篇文章呢

82
00:03:35,900 --> 00:03:37,600
对比那个ResNet50呢

83
00:03:37,600 --> 00:03:39,000
它有了相同的精度

84
00:03:39,000 --> 00:03:41,500
但是提升了100倍的模型压缩

85
00:03:41,500 --> 00:03:42,200
左边这个呢

86
00:03:42,200 --> 00:03:44,600
就是ResNet最经典的一个Block

87
00:03:44,600 --> 00:03:45,500
像SqueezeNet呢

88
00:03:45,500 --> 00:03:48,500
就提出了右边的两个结构

89
00:03:48,500 --> 00:03:51,600
中间这个就是SqueezeNet V1的一个Block

90
00:03:51,600 --> 00:03:54,000
由Squeeze跟Expand两层进行组成

91
00:03:54,000 --> 00:03:54,500
Squeeze呢

92
00:03:54,500 --> 00:03:56,500
就是1x1的卷积合

93
00:03:56,500 --> 00:03:57,300
Expand层呢

94
00:03:57,300 --> 00:04:00,700
就是1x1 Concat 3x3这种方式

95
00:04:00,700 --> 00:04:01,700
那在右边呢

96
00:04:01,700 --> 00:04:03,600
就是SqueezeNet的一个Block

97
00:04:03,600 --> 00:04:07,400
就是SqueezeNet V2的一种Block的一个网络模型结构

98
00:04:07,400 --> 00:04:08,400
那么可以看到啊

99
00:04:08,400 --> 00:04:09,300
其实这里面呢

100
00:04:09,300 --> 00:04:10,500
有一种比较有意思

101
00:04:10,500 --> 00:04:12,100
就是它主要呢

102
00:04:12,100 --> 00:04:15,300
是采用了ResNet的这种残差的结构方式

103
00:04:15,300 --> 00:04:16,000
但是呢

104
00:04:16,000 --> 00:04:19,100
它做了一个就是把3x3的卷积呢

105
00:04:19,100 --> 00:04:20,600
拆分成3x1

106
00:04:20,600 --> 00:04:23,300
然后1x3两层的卷积

107
00:04:23,300 --> 00:04:24,200
这样的方式呢

108
00:04:24,200 --> 00:04:25,800
有点类似于D字分解

109
00:04:25,800 --> 00:04:28,400
最后再接1x1的卷积

110
00:04:28,400 --> 00:04:29,800
网络模型的参数量呢

111
00:04:29,800 --> 00:04:32,200
从k的平方变成2k

112
00:04:32,200 --> 00:04:33,100
而另外一点呢

113
00:04:33,100 --> 00:04:35,200
需要注意的就是网络模型的数呢

114
00:04:35,200 --> 00:04:37,000
是有128的通道

115
00:04:37,000 --> 00:04:38,100
那通过这里面呢

116
00:04:38,100 --> 00:04:40,400
64 32两个1x1的卷积呢

117
00:04:40,500 --> 00:04:41,500
把通道数呢

118
00:04:41,500 --> 00:04:44,100
从128减到64再减到32

119
00:04:44,100 --> 00:04:44,600
然后呢

120
00:04:44,600 --> 00:04:46,100
通过刚才的D字分解呢

121
00:04:46,100 --> 00:04:48,400
慢慢的把通道数恢复上来

122
00:04:48,400 --> 00:04:51,700
最后再通过一个1x1的卷积恢复到128

123
00:04:51,700 --> 00:04:53,500
然后再往下传递

124
00:04:53,500 --> 00:04:54,700
那通过这种方式呢

125
00:04:54,700 --> 00:04:57,300
其实代替掉了原来的SqueezeNet

126
00:04:57,300 --> 00:04:58,200
那原来的数呢

127
00:04:58,200 --> 00:04:59,000
是128

128
00:04:59,000 --> 00:05:00,900
中间是两层64

129
00:05:00,900 --> 00:05:03,600
最后输出的是128

130
00:05:03,600 --> 00:05:04,700
那这种方式呢

131
00:05:04,700 --> 00:05:06,300
更多的是采用了

132
00:05:06,300 --> 00:05:10,100
或者借鉴了ResNet50的这种残差的结构的方式

133
00:05:10,100 --> 00:05:12,700
对原来的SqueezeNet V1进行改进

134
00:05:14,300 --> 00:05:17,400
下面我们看看第二个模型系列

135
00:05:17,400 --> 00:05:18,900
ShuffleNet

136
00:05:18,900 --> 00:05:19,500
在这里面呢

137
00:05:19,500 --> 00:05:22,100
我更多的是把他们之间最重要的特点

138
00:05:22,100 --> 00:05:23,900
给大家简单的过一遍

139
00:05:23,900 --> 00:05:25,700
我希望其实更多的开发者

140
00:05:25,700 --> 00:05:28,000
或者更多的有兴趣的读者呢

141
00:05:28,000 --> 00:05:30,800
能够深入的去自己去看看相关的论文

142
00:05:32,900 --> 00:05:34,100
ShuffleNet这篇文章呢

143
00:05:34,100 --> 00:05:36,300
最早是发布于2017年的

144
00:05:36,300 --> 00:05:38,500
我们现在看看它具体的一个

145
00:05:38,500 --> 00:05:41,100
网络模型的结构

146
00:05:41,100 --> 00:05:43,700
如果对算法比较了解的同学可以知道啊

147
00:05:43,700 --> 00:05:44,500
其实卷积呢

148
00:05:44,500 --> 00:05:46,000
有很多种有Depth-wise卷积

149
00:05:46,000 --> 00:05:46,700
有Group卷积

150
00:05:46,700 --> 00:05:48,000
还有普通的卷积

151
00:05:48,000 --> 00:05:48,500
当然了

152
00:05:48,500 --> 00:05:51,500
1x1跟3x3跟7x7的卷积的具体实现方式

153
00:05:51,500 --> 00:05:52,700
也是不一样的

154
00:05:52,700 --> 00:05:55,500
而且他们的运算速度也是不一样

155
00:05:55,500 --> 00:05:56,700
这里面作者就提到

156
00:05:56,700 --> 00:05:57,700
1x1的卷积呢

157
00:05:57,700 --> 00:05:59,700
会消耗大量的计算资源

158
00:05:59,700 --> 00:06:01,100
而像Group卷积这种

159
00:06:01,100 --> 00:06:03,000
可以降低我们计算量的卷积呢

160
00:06:03,000 --> 00:06:04,700
其实很难对不同Group

161
00:06:04,700 --> 00:06:06,600
之间的一些信息进行共享

162
00:06:06,600 --> 00:06:07,200
所以呢

163
00:06:07,200 --> 00:06:09,700
他就提出了一个使用Group卷积

164
00:06:09,700 --> 00:06:10,400
但是呢

165
00:06:10,400 --> 00:06:13,400
后面去采用一个Channel Shuffle的操作

166
00:06:13,400 --> 00:06:15,700
然后去代替传统的Group卷积

167
00:06:15,700 --> 00:06:18,200
直接得到一个Feature Map的这种方式

168
00:06:18,200 --> 00:06:22,000
那我们可以看一下具体的网络模型结构

169
00:06:22,000 --> 00:06:23,100
那左边的这个呢

170
00:06:23,100 --> 00:06:25,200
就是最原始的这种

171
00:06:25,200 --> 00:06:27,300
ResNet50的网络模型结构

172
00:06:27,300 --> 00:06:27,900
这里面呢

173
00:06:27,900 --> 00:06:30,200
DW就是Depth-wise卷积

174
00:06:30,200 --> 00:06:31,600
而右边的这两个图呢

175
00:06:31,600 --> 00:06:33,800
就是刚才提出的ShuffleNet的一个思想

176
00:06:33,800 --> 00:06:34,300
里面呢

177
00:06:34,300 --> 00:06:36,800
就使用了一个Channel Shuffle

178
00:06:36,800 --> 00:06:37,500
而上面呢

179
00:06:37,500 --> 00:06:39,800
把普通卷积变成一个Group卷积

180
00:06:39,800 --> 00:06:40,300
接着呢

181
00:06:40,300 --> 00:06:42,000
去接一个Depth-wise的卷积

182
00:06:42,000 --> 00:06:43,600
最后再接一个Group卷积

183
00:06:43,600 --> 00:06:46,900
就是魔改了ResNet50的一种结构的方式

184
00:06:46,900 --> 00:06:49,200
好处就是能够减少我们的网络模型的大小

185
00:06:49,200 --> 00:06:50,300
保持相同的精度

186
00:06:50,300 --> 00:06:51,600
模型更小了

187
00:06:51,600 --> 00:06:53,300
但是有一个比较大的缺点

188
00:06:53,300 --> 00:06:54,500
就是Channel Shuffle呢

189
00:06:54,500 --> 00:06:56,000
是人工去定义的

190
00:06:56,000 --> 00:06:57,000
那这里面的规则呢

191
00:06:57,000 --> 00:06:58,000
就比较复杂

192
00:07:00,100 --> 00:07:00,600
下面呢

193
00:07:00,600 --> 00:07:03,300
我们来看看ShuffleNet V2的一个网络模型结构

194
00:07:03,300 --> 00:07:04,000
那A呢

195
00:07:04,000 --> 00:07:06,200
就是ShuffleNet V1的网络模型结构

196
00:07:06,200 --> 00:07:06,900
那C呢

197
00:07:06,900 --> 00:07:10,100
就是对应的ShuffleNet V2的网络模型结构

198
00:07:10,100 --> 00:07:10,700
可以看到啊

199
00:07:10,700 --> 00:07:11,900
在模型输入的时候呢

200
00:07:11,900 --> 00:07:13,700
加了一个Channel Split的工作

201
00:07:13,700 --> 00:07:15,300
把我们的网络模型的Channel呢

202
00:07:15,300 --> 00:07:17,800
直接分开成为两半

203
00:07:17,800 --> 00:07:18,400
在这里面呢

204
00:07:18,400 --> 00:07:22,000
作者就表明在网络模型的输入输出大小相同的时候呢

205
00:07:22,000 --> 00:07:22,600
这种方式呢

206
00:07:22,600 --> 00:07:25,000
能够有效的提升网络模型的

207
00:07:25,000 --> 00:07:27,200
或者我们计算的MAC

208
00:07:27,200 --> 00:07:28,500
第二个优化的工作呢

209
00:07:28,500 --> 00:07:31,200
就取消了1x1的Group卷积

210
00:07:31,200 --> 00:07:33,500
就直接使用了1x1的卷积去代替

211
00:07:34,500 --> 00:07:36,500
作者在经过大量的实验里面呢

212
00:07:36,500 --> 00:07:38,900
去表示其实过多的Group卷积呢

213
00:07:38,900 --> 00:07:41,300
会提升我们整个计算的MAC

214
00:07:42,000 --> 00:07:43,000
那MAC是什么

215
00:07:43,000 --> 00:07:45,600
我们其实在上一节课里面给大家去普及过

216
00:07:45,600 --> 00:07:47,800
相关的一些参数的概念

217
00:07:48,700 --> 00:07:49,800
第三点优化呢

218
00:07:49,800 --> 00:07:51,200
就是Channel Shuffle

219
00:07:51,200 --> 00:07:53,000
可以看到蓝色的这个模块呢

220
00:07:53,000 --> 00:07:54,900
其实挪到最后面

221
00:07:54,900 --> 00:07:56,200
就是Concate之后

222
00:07:56,200 --> 00:07:57,300
因为我们可以看到

223
00:07:57,300 --> 00:07:58,900
我们改变了这个Group卷积

224
00:07:58,900 --> 00:08:01,100
后面再加个Channel Shuffle是没有意义的

225
00:08:01,500 --> 00:08:04,100
我们把Channel Shuffle往后面放

226
00:08:04,100 --> 00:08:07,100
把不同通道之间的一些信息的传递

227
00:08:07,100 --> 00:08:08,100
放到最后面

228
00:08:08,100 --> 00:08:11,500
有效的减少我们网络模型的碎片化的程度

229
00:08:11,500 --> 00:08:13,000
那第四个改变呢

230
00:08:13,000 --> 00:08:15,400
就是Concate这个操作

231
00:08:15,400 --> 00:08:17,100
可以看到在ShuffleNet V1里面呢

232
00:08:17,100 --> 00:08:19,000
用的是一个Add的操作

233
00:08:19,000 --> 00:08:21,000
而在ShuffleNet V1里面呢

234
00:08:21,000 --> 00:08:23,600
用的是一个Concate的这种方式

235
00:08:23,600 --> 00:08:24,200
其实呢

236
00:08:24,200 --> 00:08:26,200
Add它是一个ElementWise的一种操作

237
00:08:26,200 --> 00:08:27,200
而Concate呢

238
00:08:27,200 --> 00:08:30,200
其实更加有效的提升我们计算的FLOPs

239
00:08:32,100 --> 00:08:32,800
接下来呢

240
00:08:32,800 --> 00:08:34,800
我们看一个最重要的系列

241
00:08:34,800 --> 00:08:36,400
就是MobileNet系列

242
00:08:36,400 --> 00:08:39,500
那MobileNet系列是由谷歌去提出来的

243
00:08:39,500 --> 00:08:40,000
里面呢

244
00:08:40,000 --> 00:08:41,600
就推出了V1 V2 V3

245
00:08:41,600 --> 00:08:42,900
三个不同的系列

246
00:08:45,900 --> 00:08:48,200
首先我们看看MobileNet V1

247
00:08:48,200 --> 00:08:51,400
这个系列到底有什么最大的特点呢

248
00:08:51,400 --> 00:08:52,300
MobileNet V1呢

249
00:08:52,300 --> 00:08:55,500
最重要的就提出了一个新的计算的方式

250
00:08:55,500 --> 00:08:56,900
或者新的计算的结构

251
00:08:56,900 --> 00:08:58,300
那上面这个图A呢

252
00:08:58,300 --> 00:09:01,000
就是Standard卷积的一种滤波器啊

253
00:09:01,000 --> 00:09:03,700
可以看到大部分的我们假设两个DK呢

254
00:09:03,700 --> 00:09:05,700
就是我们Kernels的大小

255
00:09:05,700 --> 00:09:06,500
那一般来说呢

256
00:09:06,500 --> 00:09:09,400
我们会使用3x3的这种卷积的方式

257
00:09:09,400 --> 00:09:10,500
那结果呢

258
00:09:10,500 --> 00:09:13,000
作者就把这种3x3的卷积方式呢

259
00:09:13,000 --> 00:09:14,800
就替换成为了两个结构

260
00:09:14,800 --> 00:09:16,500
一个呢就是Depth-wise Convolution

261
00:09:16,500 --> 00:09:18,400
另外一个是Pointwise Convolution

262
00:09:18,400 --> 00:09:20,100
可以看到Depth-wise Convolution呢

263
00:09:20,100 --> 00:09:24,100
里面就使用了跟Standard卷积是一样的DK

264
00:09:24,100 --> 00:09:24,700
但是呢

265
00:09:24,700 --> 00:09:25,700
在Pointwise里面呢

266
00:09:25,700 --> 00:09:29,900
就使用了1乘以1卷积的方式去替换掉

267
00:09:29,900 --> 00:09:31,100
那举一个具体的例子

268
00:09:31,100 --> 00:09:33,600
假设原始的网络模型的卷积呢

269
00:09:33,600 --> 00:09:35,400
使用了一个3x3的卷积

270
00:09:35,400 --> 00:09:36,100
那这里面呢

271
00:09:36,100 --> 00:09:38,100
先使用一个Depth-wise3x3

272
00:09:38,100 --> 00:09:40,600
再执行一个1x1的Pointwise的卷积

273
00:09:40,600 --> 00:09:42,100
那通过这种方式呢

274
00:09:42,100 --> 00:09:44,800
去代替掉原来的卷积的方式

275
00:09:44,800 --> 00:09:49,600
有了一个比较大的网络模型的参数量的降低

276
00:09:49,600 --> 00:09:50,300
这种方式呢

277
00:09:50,300 --> 00:09:54,100
有效的增加了我们整个网络模型执行的FLOPs

278
00:09:54,100 --> 00:09:55,400
模型的参数量呢

279
00:09:55,400 --> 00:09:58,200
也是急剧的减少了非常非常的多

280
00:10:00,600 --> 00:10:04,600
接下来我们看看MobileNet V2这个网络模型

281
00:10:04,600 --> 00:10:05,900
那这个网络模型呢

282
00:10:05,900 --> 00:10:08,500
确实是非常非常的经典了

283
00:10:08,500 --> 00:10:09,000
这里面呢

284
00:10:09,000 --> 00:10:10,700
就提出了两个概念

285
00:10:10,700 --> 00:10:11,100
第一个呢

286
00:10:11,100 --> 00:10:13,000
就是Inverted Residuals

287
00:10:13,000 --> 00:10:15,400
第二个就是Linear Bottlenecks

288
00:10:15,400 --> 00:10:17,700
那我们看看两个具体的概念

289
00:10:17,700 --> 00:10:22,200
对我们整个模型压缩和轻量化有什么不一样的工作

290
00:10:22,200 --> 00:10:23,000
第一个概念呢

291
00:10:23,000 --> 00:10:24,600
就是Invert Residual这个Block

292
00:10:24,600 --> 00:10:25,400
可以看到啊

293
00:10:25,400 --> 00:10:27,600
左边这个就是传统的Residual的Block

294
00:10:27,800 --> 00:10:28,900
传统Residual的Block呢

295
00:10:28,900 --> 00:10:31,400
就是输入的一个Invert Channel之后呢

296
00:10:31,400 --> 00:10:33,100
我进行一个1x1的卷积

297
00:10:33,100 --> 00:10:34,700
再进行3x3的卷积

298
00:10:34,700 --> 00:10:36,600
最后再进行1x1的卷积

299
00:10:36,600 --> 00:10:38,300
然后把残差中间那条线呢

300
00:10:38,300 --> 00:10:40,400
就是残差的结构输出

301
00:10:40,400 --> 00:10:41,900
那可以看到基本上输入呢

302
00:10:41,900 --> 00:10:45,200
是一个128或者256的一个Channel的大小

303
00:10:45,200 --> 00:10:45,700
接着呢

304
00:10:45,700 --> 00:10:47,400
Channel的大小会减少

305
00:10:47,400 --> 00:10:48,800
最后再恢复过来

306
00:10:48,800 --> 00:10:49,600
那作者呢

307
00:10:49,600 --> 00:10:50,800
就觉得像这种方式呢

308
00:10:50,800 --> 00:10:53,700
其实是破坏了Feature Map的完整性

309
00:10:53,700 --> 00:10:57,000
然后没办法去获取很多有效的Feature Map

310
00:10:57,600 --> 00:10:58,000
于是呢

311
00:10:58,000 --> 00:11:01,500
右边的这边的作者就加入了一个Invert Residual Block

312
00:11:01,500 --> 00:11:03,300
可以看到输入的时候呢

313
00:11:03,300 --> 00:11:04,800
假设是128

314
00:11:04,800 --> 00:11:05,400
但是呢

315
00:11:05,400 --> 00:11:07,200
里面的Channel的大小呢

316
00:11:07,200 --> 00:11:09,000
就变得非常的多了

317
00:11:09,000 --> 00:11:10,700
就是他先做一个扩展

318
00:11:10,700 --> 00:11:13,200
然后相关的卷积也是1x1 3x3

319
00:11:13,200 --> 00:11:14,900
再做1x1的输出

320
00:11:14,900 --> 00:11:15,900
可以看到这里面呢

321
00:11:15,900 --> 00:11:18,600
就是跟Residual的一个Channel的数呢

322
00:11:18,600 --> 00:11:20,000
是刚好是相反的

323
00:11:20,000 --> 00:11:20,800
一个是压缩

324
00:11:20,800 --> 00:11:21,700
一个是增大

325
00:11:21,700 --> 00:11:22,600
那这种方式呢

326
00:11:22,600 --> 00:11:26,200
有效的去提升了我们网络模型的精度

327
00:11:26,200 --> 00:11:28,400
沿用了MobileNet V1的前提下呢

328
00:11:28,400 --> 00:11:30,600
有效的提升了我们网络模型的性能

329
00:11:33,500 --> 00:11:37,800
而第二个比较重要的工作就是Linear Bottleneck

330
00:11:37,800 --> 00:11:38,500
那这个呢

331
00:11:38,500 --> 00:11:42,500
就使用了一个ReLU6代替了传统的ReLU

332
00:11:42,500 --> 00:11:44,000
而且在Depth-wise之前呢

333
00:11:44,000 --> 00:11:46,400
增加了一个1x1的卷积

334
00:11:46,400 --> 00:11:47,900
那通过这两种方式呢

335
00:11:47,900 --> 00:11:51,000
有效的去提升了模型的精度

336
00:11:51,000 --> 00:11:54,800
而且在保持相同的一个网络模型的压缩比的前提下呢

337
00:11:54,800 --> 00:11:57,200
确实MobileNet V2这个模型呢

338
00:11:57,200 --> 00:11:58,900
就已经非常的SOTA

339
00:11:58,900 --> 00:11:59,600
现在呢

340
00:11:59,600 --> 00:12:01,500
很多Transformer的网络模型结构啊

341
00:12:01,500 --> 00:12:03,200
也是去引用了

342
00:12:03,200 --> 00:12:04,700
或者加入了MobileNet V2

343
00:12:04,700 --> 00:12:05,800
或者把MobileNet V2呢

344
00:12:05,800 --> 00:12:07,300
当成它的一个Benchmark

345
00:12:08,700 --> 00:12:09,400
最后呢

346
00:12:09,400 --> 00:12:11,800
我们看一下MobileNet V3这个系列

347
00:12:11,800 --> 00:12:13,100
其实MobileNet V3这个系呢

348
00:12:13,100 --> 00:12:15,600
我觉得可能谈的内容不太多

349
00:12:15,600 --> 00:12:16,700
因为MobileNet V3呢

350
00:12:16,700 --> 00:12:17,200
主要呢

351
00:12:17,200 --> 00:12:18,900
是针对谷歌的TPU进行优化的

352
00:12:18,900 --> 00:12:21,400
其实我们在实践的过程当中发现

353
00:12:21,400 --> 00:12:22,200
MobileNet V3呢

354
00:12:22,300 --> 00:12:26,500
并不是对很多大部分的设备能够有一个很好的性能的优化

355
00:12:26,500 --> 00:12:28,200
或者模型的进一步的降低

356
00:12:28,200 --> 00:12:30,600
但是保持相同的一个性能

357
00:12:30,600 --> 00:12:32,400
那在这篇文章里面呢

358
00:12:32,400 --> 00:12:34,400
更多的是使用了Searching

359
00:12:34,400 --> 00:12:36,400
就是一些NAS的搜索方法

360
00:12:36,400 --> 00:12:39,400
然后找到了一个比较好的网络模型的结构

361
00:12:39,400 --> 00:12:40,300
那这篇文章呢

362
00:12:40,300 --> 00:12:41,700
是在19年发布的

363
00:12:41,700 --> 00:12:42,200
这边呢

364
00:12:42,200 --> 00:12:45,200
我就不再对MobileNet V3进行详细的展开

365
00:12:45,200 --> 00:12:46,700
更多的大家MobileNet系列呢

366
00:12:46,700 --> 00:12:49,100
可以聚焦于MobileNet V1和V2

367
00:12:49,100 --> 00:12:49,900
卷的不行了

368
00:12:49,900 --> 00:12:50,800
卷的不行了

369
00:12:50,900 --> 00:12:52,700
记得一键三连加关注哦

370
00:12:52,700 --> 00:12:56,300
所有的内容都会开源在下面这条链接里面

371
00:12:56,300 --> 00:12:56,900
拜拜

