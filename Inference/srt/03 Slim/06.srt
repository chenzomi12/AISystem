1
00:00:00,000 --> 00:00:06,403
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:06,403 --> 00:00:07,800
Hello大家好，我是ZOMI

3
00:00:07,800 --> 00:00:09,560
因为普通话咬字不清楚

4
00:00:09,560 --> 00:00:14,720
所以录一门课 我一般都会NG很多遍 然后重复很多遍 

5
00:00:14,720 --> 00:00:19,680
那今天来到推理引擎模型压缩的 倒数第二个内容 

6
00:00:19,680 --> 00:00:22,880
就是知识蒸馏 Knowledge Distillation 

7
00:00:22,880 --> 00:00:25,720
那好了现在来到知识蒸馏这个环节

8
00:00:25,720 --> 00:00:30,840
看一下今天知识蒸馏要给大家汇报哪几个内容 

9
00:00:30,840 --> 00:00:33,720
首先就是知识蒸馏的背景 Background

10
00:00:33,720 --> 00:00:35,240
然后可以看一下

11
00:00:35,240 --> 00:00:37,720
因为需要对知识进行蒸馏

12
00:00:37,720 --> 00:00:39,640
这里面的知识就很重要了

13
00:00:39,640 --> 00:00:42,760
了解一下所谓的知识的形式

14
00:00:42,760 --> 00:00:44,840
就是Knowledge的Format

15
00:00:44,840 --> 00:00:47,720
然后可能会分开两个视频来看

16
00:00:47,720 --> 00:00:50,760
因为一个视频可能讲完这些内容有点长 

17
00:00:50,760 --> 00:00:55,800
第二个视频的内容里面 会讲讲 Distillation就是蒸馏的具体的方法 

18
00:00:55,800 --> 00:00:59,680
上面的内容更多的聚焦于Knowledge 

19
00:00:59,680 --> 00:01:03,320
下面的内容 更多的聚焦于Distillation 

20
00:01:03,320 --> 00:01:07,680
Hiton最经典的一个知识蒸馏的算法进行解读

21
00:01:07,680 --> 00:01:13,280
因为Hiton他写了一篇文章才正式的去把这个知识点明确下来 

22
00:01:14,280 --> 00:01:16,640
下面看一下所在的位置

23
00:01:16,640 --> 00:01:19,040
知识蒸馏还是在模型压缩的模块

24
00:01:19,040 --> 00:01:21,320
但实际上很多知识蒸馏的算法

25
00:01:21,320 --> 00:01:26,440
包括量化算法 还有剪枝的算法 包括二次化的算法 都是在AI框架的一个特性 

26
00:01:26,440 --> 00:01:29,040
真正在做完模型转换之后

27
00:01:29,040 --> 00:01:30,400
变成自己的IR

28
00:01:30,400 --> 00:01:32,400
然后用模型压缩

29
00:01:33,000 --> 00:01:34,120
去对AI框架

30
00:01:34,120 --> 00:01:36,920
关于模型压缩的特性进行处理

31
00:01:36,920 --> 00:01:39,680
才得到一个小型化的模型

32
00:01:39,680 --> 00:01:43,000
得到小型化的模型 再给真正的推理的 

33
00:01:44,200 --> 00:01:48,680
注意一下 推进引擎架构里面的压缩模块  它是有限定的 

34
00:01:48,680 --> 00:01:51,840
它只是做一个模型的小型化的工作

35
00:01:52,080 --> 00:01:54,920
很多内容都是在AI框架里面去处理的 

36
00:01:59,335 --> 00:02:04,304
下面看一个比较重要的内容就是知识蒸馏 Knowledge Distillation KD 

37
00:02:04,304 --> 00:02:06,914
后面都可以简称KD

38
00:02:06,914 --> 00:02:11,335
最初是Hitton在这篇文章里面去发表的

39
00:02:11,335 --> 00:02:14,655
里面就提出了soft label和hard label

40
00:02:14,655 --> 00:02:20,036
然后知识蒸馏使用soft label来训练教师网络和学生网络 

41
00:02:21,804 --> 00:02:25,324
通过教师网络把知识交给学生

42
00:02:26,045 --> 00:02:28,565
它的算法原理一句话比较简单

43
00:02:28,565 --> 00:02:33,935
就是把教师网络学习到的知识蒸馏或者交给学生网络 

44
00:02:34,880 --> 00:02:39,510
像教师网络一般都会比较大 学生网络一般都会比较小 

45
00:02:41,880 --> 00:02:44,640
两者之间是相互竞争的关系

46
00:02:44,640 --> 00:02:47,760
直到学生可以学到跟老师网络

47
00:02:47,760 --> 00:02:50,480
差不多的网络模型的精度

48
00:02:51,480 --> 00:02:55,360
下面这个图就很清楚的说明这个情况了

49
00:02:55,360 --> 00:02:56,600
首先有三个内容

50
00:02:56,600 --> 00:02:58,800
一个是老师的网络

51
00:02:58,800 --> 00:03:03,520
老师网络应该会比较大 因为老师他知识量比较丰富 

52
00:03:04,360 --> 00:03:07,440
这里面很重要的一个概念就是知识

53
00:03:07,440 --> 00:03:08,640
通过知识的转换之后

54
00:03:08,760 --> 00:03:11,720
得到student model学生网络

55
00:03:12,320 --> 00:03:15,880
学生网络一般都会默认比教师网络要小

56
00:03:15,880 --> 00:03:20,560
所以说这个可以通过知识蒸馏的方式做一个模型的压缩 

57
00:03:20,560 --> 00:03:23,240
从大模型变成小模型

58
00:03:23,240 --> 00:03:25,840
从老师模型变成学生模型

59
00:03:25,840 --> 00:03:30,720
而知识怎么转换就取决于蒸馏的算法 

60
00:03:32,000 --> 00:03:34,240
不过有一点注意的就是所使用的

61
00:03:34,240 --> 00:03:36,160
数据都是相同的

62
00:03:36,160 --> 00:03:37,680
数据都是一样的

63
00:03:37,680 --> 00:03:41,360
有可能还会增加一些数据给学生网络

64
00:03:42,480 --> 00:03:46,000
这也是整体知识蒸馏的算法架构

65
00:03:47,000 --> 00:03:49,400
在刚才的一个知识蒸馏的算法架构

66
00:03:49,400 --> 00:03:52,840
再进一步的抽象成为三个内容

67
00:03:52,840 --> 00:03:56,520
第一个就是蒸馏的知识knowledge

68
00:03:56,520 --> 00:04:00,880
第二个就是蒸馏的算法distillate algorithm

69
00:04:00,880 --> 00:04:04,360
第三个就是师生网络的架构

70
00:04:04,360 --> 00:04:08,080
就是老师跟学生之间的关系架构组成

71
00:04:10,040 --> 00:04:13,840
下面看一下第二个内容 知识蒸馏的方式 

72
00:04:13,880 --> 00:04:18,640
这个方式其实参考这一篇文章 Knowledge Distillation: A Survey 

73
00:04:18,640 --> 00:04:23,520
通常一些survey或者white paper 都是一个很好的综述和参考 

74
00:04:25,520 --> 00:04:29,760
下面来聚焦一下一个点 就是knowledge知识的方式 

75
00:04:30,360 --> 00:04:32,960
说白了虽然知识的方式说得很抽象

76
00:04:32,960 --> 00:04:37,120
就是我这些知识我要蒸馏的知识从哪里来？怎么来？ 

77
00:04:38,280 --> 00:04:40,000
这个问题要解开的

78
00:04:40,000 --> 00:04:45,400
它有三种方式response-based, Feature-based, relation-based还有architecture-based 

79
00:04:45,400 --> 00:04:47,920
一般来说architecture based 确实用的比较少 

80
00:04:47,920 --> 00:04:49,880
更多的是前三种

81
00:04:51,720 --> 00:04:54,040
逐一种来去打开

82
00:04:54,040 --> 00:04:56,720
首先就是response-based

83
00:04:57,440 --> 00:05:04,280
它主要是指teacher model教师模型输出层的特征 就是最后的一个Feature map的特征 

84
00:05:05,080 --> 00:05:10,200
希望让学生模型直接学习到教师模型预测的结果 

85
00:05:10,200 --> 00:05:13,640
真正的直接把结果给学生去学习

86
00:05:14,320 --> 00:05:16,080
通俗的来讲就是一个知识点

87
00:05:16,080 --> 00:05:20,240
老师充分的学习完了 然后把结论告诉学生就好了 

88
00:05:20,240 --> 00:05:22,520
这个就是最终的输出

89
00:05:22,520 --> 00:05:24,720
下面看一下具体的公式

90
00:05:24,920 --> 00:05:27,960
假设Zt就是教师模型的输出

91
00:05:27,960 --> 00:05:30,480
而Zs就是学生模型的输出

92
00:05:30,480 --> 00:05:33,200
这里面的输出都是指最后一层的输出

93
00:05:33,830 --> 00:05:39,790
而response-based knowledge 蒸馏的形式就可以用下面这条公式来去描述 

94
00:05:41,200 --> 00:05:44,640
回到这个图 看一下response-based在哪里 

95
00:05:44,640 --> 00:05:45,560
在这儿

96
00:05:45,920 --> 00:05:49,480
这就证明最后一层的模型的输出的知识

97
00:05:49,480 --> 00:05:53,960
就是老师已经学完了 然后把这个知识传授给学生 

98
00:05:55,520 --> 00:05:59,280
下面看一下response-based的一个学习流程图 

99
00:05:59,280 --> 00:06:01,040
红色代表的是老师

100
00:06:01,040 --> 00:06:02,960
绿色代表的是学生

101
00:06:02,960 --> 00:06:06,680
会把teacher model输出的特征

102
00:06:06,680 --> 00:06:09,920
给到学生网络模型去学习

103
00:06:10,040 --> 00:06:12,360
这里面会拿出最后一层的特征

104
00:06:12,360 --> 00:06:14,720
然后通过distillation loss

105
00:06:14,720 --> 00:06:19,200
让学生最后一层的特征去学习老师的输出的特征 

106
00:06:19,200 --> 00:06:21,680
老师的输出特征应该是比较固定的

107
00:06:21,680 --> 00:06:24,840
而学生它是不太固定需要去学习的 

108
00:06:24,840 --> 00:06:27,840
于是通过一个损失函数去模拟

109
00:06:27,840 --> 00:06:30,200
去减少去学习

110
00:06:30,200 --> 00:06:34,120
使得两个的logist越小越好

111
00:06:34,960 --> 00:06:40,240
第二种就是Feature-based knowledge

112
00:06:40,240 --> 00:06:44,520
实际上网络模型有很多的层

113
00:06:44,520 --> 00:06:46,880
每一层都会有对应的Feature map

114
00:06:46,880 --> 00:06:48,960
有非常多的特征

115
00:06:48,960 --> 00:06:51,920
有非常多的知识可以让去学习

116
00:06:51,920 --> 00:06:54,840
所以Feature-based knowledge

117
00:06:54,840 --> 00:06:59,520
主要是把教授和学生的特征激活连接起来 

118
00:06:59,520 --> 00:07:02,320
这种方式可以用下面这条公式来表示

119
00:07:02,720 --> 00:07:08,080
看一下知识的图 像Feature-based knowledge就在这里面 

120
00:07:09,640 --> 00:07:15,480
会把网络模型中间学习到的激活还有权重 把它作为一个Feature 

121
00:07:15,480 --> 00:07:19,720
然后通过这个Feature建立学生跟老师之间的关系 

122
00:07:19,720 --> 00:07:22,120
这种就是Feature-based knowledge

123
00:07:23,760 --> 00:07:29,160
下面看一下Feature-based knowledge的一个整体的或者工作的流程图 

124
00:07:30,160 --> 00:07:33,480
这里面的distillation loss更多是创建在teacher model 

125
00:07:33,480 --> 00:07:35,440
跟student model的中间层

126
00:07:35,440 --> 00:07:38,440
通过中间层去创建一个连接关系

127
00:07:39,560 --> 00:07:42,080
这种算法的好处就是老师网络

128
00:07:42,080 --> 00:07:46,240
可以给学生网络提供非常大量的有用的参考信息 

129
00:07:46,240 --> 00:07:51,360
但是怎么有效的从老师网络 这里面这么多提示层去给到学生 

130
00:07:51,360 --> 00:07:54,720
因为student model的网络层数会比teacher model要少 

131
00:07:55,400 --> 00:07:59,880
应该从teacher model里面提取哪几层有效的作为引导层 

132
00:07:59,880 --> 00:08:02,440
这一步是前沿的研究

133
00:08:03,000 --> 00:08:07,160
缺点就是提示层和引导层的大小存在明显的差异 

134
00:08:07,160 --> 00:08:12,360
说白了就是teacher model 跟student model的网络层数 中间的Feature map差异比较大 

135
00:08:12,360 --> 00:08:14,800
如何正确的匹配两者之间的关系

136
00:08:14,800 --> 00:08:17,320
也是需要一个探索和研究的

137
00:08:17,320 --> 00:08:20,000
而业界在整体上面还没有成熟

138
00:08:21,640 --> 00:08:25,160
最后一种就是relation-based knowledge

139
00:08:25,960 --> 00:08:31,760
基于关系的知识主要是进一步探索不同层或者数据样本之间的一个关系 

140
00:08:31,760 --> 00:08:34,240
从而提出一些相关的知识

141
00:08:34,680 --> 00:08:38,680
看一下这个图relation-based knowledge就在这里面 

142
00:08:39,520 --> 00:08:41,080
希望通过研究不同的样本

143
00:08:41,080 --> 00:08:42,720
不同的输出层之间的关系

144
00:08:43,080 --> 00:08:46,640
从而创建整个知识蒸馏的知识点

145
00:08:47,560 --> 00:08:50,800
下面又来到算法的架构图里面

146
00:08:50,800 --> 00:08:52,280
有相同的数据

147
00:08:52,280 --> 00:08:54,400
然后有一个teacher model和student model

148
00:08:54,720 --> 00:08:59,880
distillation loss就不仅仅是去学习刚才讲到的网络模型中间的特征 

149
00:08:59,880 --> 00:09:00,960
还有最后一层的特征

150
00:09:00,960 --> 00:09:05,160
而且他会学习数据样本还有网络模型层之间的一个关系 

151
00:09:06,280 --> 00:09:08,680
好了 今天给大家汇报了两个知识 

152
00:09:08,680 --> 00:09:13,000
一个就是知识蒸馏的背景 为什幺要去蒸馏 蒸馏的好处 

153
00:09:13,000 --> 00:09:15,600
接着看一下 蒸馏的知识的形态 

154
00:09:15,600 --> 00:09:19,800
就是更多的关注于knowledge怎么提取 knowledge从哪里来  

155
00:09:19,800 --> 00:09:21,880
好 谢谢各位 拜了个拜 

156
00:09:22,400 --> 00:09:25,520
卷的不行了 卷的不行了 记得一键三连加关注 

157
00:09:25,880 --> 00:09:29,120
所有的内容都会开源在下面这条链接里面 

158
00:09:29,480 --> 00:09:30,440
拜了个拜

