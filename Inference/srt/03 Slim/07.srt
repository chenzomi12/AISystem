1
00:00:00,000 --> 00:00:04,480
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:04,480 --> 00:00:08,360
哈喽大家好 我们来到知识蒸馏的下集

3
00:00:08,360 --> 00:00:10,360
这里是ZOMI的电台

4
00:00:11,360 --> 00:00:12,240
在上一期里面呢

5
00:00:12,240 --> 00:00:13,880
我们已经充分地去了解了

6
00:00:13,880 --> 00:00:16,320
知识蒸馏的一个算法提出的背景

7
00:00:16,320 --> 00:00:19,520
还有知识蒸馏的知识的形态

8
00:00:19,520 --> 00:00:21,920
今天呢我给大家去汇报的内容呢

9
00:00:21,920 --> 00:00:24,960
主要是下面两个具体的知识蒸馏的方法

10
00:00:24,960 --> 00:00:28,080
还有Hinton经典的知识蒸馏的论文去解读

11
00:00:29,080 --> 00:00:32,880
第一个内容是知识蒸馏的方法

12
00:00:32,880 --> 00:00:37,280
同样出自于知识蒸馏的综述这篇论文里面

13
00:00:37,280 --> 00:00:40,880
知识蒸馏呢其实主要分为三个方法

14
00:00:40,880 --> 00:00:45,760
第一个呢就是离线蒸馏 offline distillation 

15
00:00:45,760 --> 00:00:49,720
第二种呢就是在线的蒸馏 online distillation

16
00:00:49,720 --> 00:00:52,880
第三种呢就是自蒸馏的方式

17
00:00:52,880 --> 00:00:54,120
一共有三种

18
00:00:54,120 --> 00:00:57,280
这里面呢则有两种颜色

19
00:00:57,280 --> 00:00:59,520
一种是红色一种是绿色

20
00:00:59,520 --> 00:01:03,120
红色呢就代表预训练的模型已经训练好的

21
00:01:03,120 --> 00:01:08,120
另外一种呢绿色就代表我们将要去训练的模型

22
00:01:09,320 --> 00:01:17,120
下面我们来打开看一看第一种方法就是 offline distillation  离线的蒸馏的方式 

23
00:01:17,120 --> 00:01:22,200
其实呢现在来看大部分的算法都采用offline的这种方式 

24
00:01:22,200 --> 00:01:23,640
就是离线蒸馏

25
00:01:23,640 --> 00:01:25,720
而蒸馏的过程分为两个阶段

26
00:01:25,760 --> 00:01:31,040
第一个阶段就是左边的 蒸馏前我们先训练一个教师的模型teachmodel 

27
00:01:31,040 --> 00:01:32,560
先把它训练好

28
00:01:33,440 --> 00:01:36,400
第二步呢就是把教师网络的模型的知识

29
00:01:36,400 --> 00:01:38,360
蒸馏给学生网络

30
00:01:38,360 --> 00:01:40,480
就进行一个知识迁移的过程

31
00:01:41,200 --> 00:01:43,160
因此offline distillation呢

32
00:01:43,160 --> 00:01:47,360
就是离线蒸馏的这种方式呢更侧重于知识的迁移部分 

33
00:01:47,760 --> 00:01:52,240
怎么去迁移 迁移什么知识 这个点非常重要 

34
00:01:52,880 --> 00:01:54,560
这里面呢值得注意的就是

35
00:01:54,560 --> 00:01:58,080
在步骤一 蒸馏前教师网络模型进行一个预训练 

36
00:01:58,080 --> 00:02:00,720
就提前训练我们的教师模型

37
00:02:00,720 --> 00:02:02,000
那这个教师模型呢

38
00:02:02,000 --> 00:02:04,480
一般来说教师模型的参数量呢会比较大

39
00:02:04,480 --> 00:02:06,920
而且训练的时间呢也会比较长

40
00:02:07,720 --> 00:02:13,520
现在呀好多好多的大模型都是采用这种方式去得到一个小模型的 

41
00:02:13,520 --> 00:02:19,000
例如BERT网络模型就推出了tinyBERT也就是这种蒸馏的方式 

42
00:02:19,000 --> 00:02:23,080
好像VIT也推出了tinyViT这种蒸馏的方式 

43
00:02:23,120 --> 00:02:25,440
同样都是采用offline distillation

44
00:02:26,720 --> 00:02:29,480
不过这种方式呢也有自身的弊端

45
00:02:29,920 --> 00:02:32,920
通常这种模式呢就是学生网络模型呢

46
00:02:32,920 --> 00:02:35,920
过度的依赖于我们的教师网络模型

47
00:02:37,000 --> 00:02:41,920
下面我们看一下第二种蒸馏的方式 online distillation 

48
00:02:43,480 --> 00:02:45,240
虽然在线蒸馏跟离线蒸馏啊

49
00:02:45,240 --> 00:02:47,040
都是左边一个教师模型

50
00:02:47,040 --> 00:02:48,720
右边一个学生模型

51
00:02:48,840 --> 00:02:53,040
不过有一点比较大的区别就是 这两个模型都是绿的 

52
00:02:53,080 --> 00:02:55,200
这不代表老师和学生都绿了

53
00:02:55,200 --> 00:02:57,480
而是代表老师和学生

54
00:02:57,480 --> 00:02:59,880
都是一个在线学习的过程

55
00:02:59,880 --> 00:03:01,560
就是一起去学习

56
00:03:07,120 --> 00:03:11,000
教师模型和学生的模型的参数同时更新 

57
00:03:11,465 --> 00:03:16,600
整个知识蒸馏的算法呢就变成一体一个端到端可训练可学习的方案 

58
00:03:16,600 --> 00:03:18,920
而不是教师模型自己先学

59
00:03:18,920 --> 00:03:21,320
学习完之后再把知识蒸馏给学生

60
00:03:21,840 --> 00:03:23,960
这种属于离线offline的方式

61
00:03:23,960 --> 00:03:27,640
教师跟学生同时学习属于online的方式 

62
00:03:27,640 --> 00:03:30,960
那online的方式呢有一个比较大的缺点就是 

63
00:03:31,440 --> 00:03:33,000
在一个在线的环境里面呢

64
00:03:33,000 --> 00:03:37,120
很难去获得一个参数量又大精度又好的教师模型 

65
00:03:38,320 --> 00:03:42,520
第三个知识蒸馏的方式呢 叫做self distillation 

66
00:03:42,520 --> 00:03:46,880
就是教师模型和学生模型呢其实就是一个模型 

67
00:03:46,880 --> 00:03:49,360
它进行一个自学习的过程

68
00:03:49,480 --> 00:03:52,080
端到端的可训练可学习的方案

69
00:03:53,000 --> 00:03:54,280
其实这种方式呢

70
00:03:54,280 --> 00:03:56,200
也属于online distillation

71
00:03:56,200 --> 00:03:59,080
就是在线蒸馏的其中一个特例

72
00:04:01,280 --> 00:04:02,640
现在呢我们总结一下

73
00:04:02,640 --> 00:04:05,080
刚才讲到的三种知识蒸馏的方法

74
00:04:05,080 --> 00:04:06,440
三种知识蒸馏的方法呢

75
00:04:06,440 --> 00:04:10,080
可以看作三种不同的学习过程

76
00:04:10,080 --> 00:04:11,720
那第一种就是刚才讲到的

77
00:04:11,720 --> 00:04:13,160
offline distillation

78
00:04:13,160 --> 00:04:15,560
最常用的离线蒸馏方法

79
00:04:16,200 --> 00:04:18,120
主要是指一个知识渊博的老师

80
00:04:18,120 --> 00:04:19,360
他自己已经学完了

81
00:04:19,360 --> 00:04:22,720
然后直接把他学到的知识传授给学生

82
00:04:22,720 --> 00:04:23,720
那现在第二种呢

83
00:04:23,720 --> 00:04:25,480
就是online distillation

84
00:04:25,480 --> 00:04:26,800
在线的知识蒸馏

85
00:04:26,800 --> 00:04:31,040
是指老师和学生一起学习一起成长 

86
00:04:31,040 --> 00:04:33,080
第三种就是self distillation

87
00:04:33,080 --> 00:04:36,400
就是学生自己学习自己成长 

88
00:04:39,520 --> 00:04:41,360
下面呢我们以一个最经典的算法

89
00:04:41,360 --> 00:04:44,440
应该是16年到17年的时候

90
00:04:44,440 --> 00:04:49,000
Hinton第一次提出知识蒸馏这个概念的这篇文章 

91
00:04:49,000 --> 00:04:51,360
那下面这篇文章呢比较粗暴 

92
00:04:51,360 --> 00:04:55,920
名字呢叫做 Distilling the Knowledge in a Neural Network 

93
00:04:57,680 --> 00:04:59,680
在正式了解这篇文章之前呢

94
00:04:59,680 --> 00:05:02,200
我们要提前去看看两个概念

95
00:05:02,200 --> 00:05:03,680
一个叫做hard target

96
00:05:03,680 --> 00:05:05,960
一个叫做soft target

97
00:05:05,960 --> 00:05:08,480
在传统神经网络模型当中的训练

98
00:05:08,480 --> 00:05:10,680
其实我们首先定一个损失函数

99
00:05:10,680 --> 00:05:12,240
然后定一个优化器

100
00:05:12,240 --> 00:05:15,040
优化器在不断地去优化我们的损失函数

101
00:05:15,040 --> 00:05:19,120
目的是使得我们的预测值呢尽可能地接近于我们的真实值 

102
00:05:19,120 --> 00:05:20,720
就是Y等于Y label

103
00:05:20,720 --> 00:05:22,560
那这个就是我们的目标

104
00:05:23,480 --> 00:05:24,920
而为了产生这个目标呢

105
00:05:24,920 --> 00:05:26,960
损失函数就是使我们神经网络的

106
00:05:26,960 --> 00:05:30,400
损失值和真实值呢之间尽可能地少 

107
00:05:30,400 --> 00:05:32,720
我们看看下面左边的这个图

108
00:05:32,720 --> 00:05:36,320
假设我现在有10个数字 0 1 3 4 5 6 7 8 9 10 

109
00:05:36,880 --> 00:05:39,240
我输进去一张图片是2

110
00:05:39,240 --> 00:05:42,440
我们希望预测是2的概率越高越好

111
00:05:42,960 --> 00:05:45,120
这种方式呢 在数学或者统计方面呢 

112
00:05:45,120 --> 00:05:49,360
就是对我们的ground truth真实的数据求极大的似然值 

113
00:05:49,360 --> 00:05:51,640
我们看一下右边的这个图

114
00:05:52,280 --> 00:05:53,520
在知识蒸馏里面呢

115
00:05:53,520 --> 00:05:55,440
我们就像右边的这个图所示

116
00:05:55,440 --> 00:05:57,320
希望能够学习到更多的

117
00:05:57,320 --> 00:05:59,240
其他额外相关的知识

118
00:05:59,240 --> 00:06:01,360
就是我假设输进去的一个数字

119
00:06:01,360 --> 00:06:04,040
手写数字可能长得像3

120
00:06:04,040 --> 00:06:05,800
这个时候呢 我们希望神经网络呢 

121
00:06:05,800 --> 00:06:07,960
学到更多的冗余的信息

122
00:06:07,960 --> 00:06:10,760
当然了他告诉我这个肯定是2 那肯定是最好的 

123
00:06:10,760 --> 00:06:13,320
把手写数字的字体长得有点像3

124
00:06:13,360 --> 00:06:15,880
这个冗余的信息也告诉出来

125
00:06:15,880 --> 00:06:17,360
那这个肯定是最好的

126
00:06:17,360 --> 00:06:20,560
因此呢我们看一下左边的这个图跟右边的这个图所示 

127
00:06:20,560 --> 00:06:22,520
左边的这个呢我们叫做Hard Target

128
00:06:22,520 --> 00:06:25,120
右边的这个呢我们叫做Soft Target

129
00:06:25,120 --> 00:06:27,720
带有一些其他冗余的信息

130
00:06:28,240 --> 00:06:30,120
下面我们再了解另外一个概念

131
00:06:30,120 --> 00:06:33,040
叫做SoftMax with Temperature

132
00:06:33,600 --> 00:06:37,400
在SoftMax函数里面呢 增加了一个温度系数 

133
00:06:38,280 --> 00:06:41,560
下面我们看一下这条公式的几个数字的含义 

134
00:06:41,560 --> 00:06:44,640
qi呢就是指第i个类别的输出的概率

135
00:06:44,640 --> 00:06:48,640
而zi呢就是指第i个类别输出的logits

136
00:06:49,280 --> 00:06:52,440
下面我们需要对每个类别输出的logist

137
00:06:52,440 --> 00:06:54,560
进行一个指数的求和

138
00:06:54,560 --> 00:06:57,120
那就得到了我们softmax的函数了

139
00:06:57,760 --> 00:07:00,520
下面呢我们进行了一个修改

140
00:07:00,520 --> 00:07:04,040
把刚才的Soft Target就是Soft Label的信息 

141
00:07:04,280 --> 00:07:06,080
给到我们的softmax函数

142
00:07:06,080 --> 00:07:09,480
这里面呢就增加了一个温度的系数T

143
00:07:09,480 --> 00:07:12,280
zi除以T zj除以T 

144
00:07:12,800 --> 00:07:14,800
当温度等于1的时候

145
00:07:14,800 --> 00:07:15,960
其实大家看到没有

146
00:07:15,960 --> 00:07:18,400
其实等于我们标准的softmax的函数

147
00:07:18,760 --> 00:07:23,520
T的数字越高呢 softmax输出的概率的分布呢 就越平滑 

148
00:07:23,840 --> 00:07:25,640
分布的信息熵呢也就越大

149
00:07:25,640 --> 00:07:30,640
所以负标签所携带的 一些额外的冗余的信息呢 也会相对的放大 

150
00:07:31,160 --> 00:07:33,120
这个时候呢我们网络模型的训练呢

151
00:07:33,120 --> 00:07:36,360
就更关注于我们一些负标签的冗余的信息 

152
00:07:36,360 --> 00:07:38,080
就是我们刚才讲到的这个图

153
00:07:38,400 --> 00:07:41,960
Soft Target里面的一些冗余的信息额外的信息 

154
00:07:41,960 --> 00:07:43,320
也把它记录下来

155
00:07:43,320 --> 00:07:45,360
就是通过简单的设置一个

156
00:07:45,360 --> 00:07:47,560
温度系数来控制

157
00:07:49,560 --> 00:07:52,400
那下面我们再看一个比较明确的图

158
00:07:53,520 --> 00:07:56,280
随着T的增加呢 T从小到大 

159
00:07:56,280 --> 00:07:59,560
我们可以看到softmax输出的分布呢就会越平滑 

160
00:07:59,760 --> 00:08:03,240
信息熵呢也就会越大 信息的差异呢也就会越少 

161
00:08:03,640 --> 00:08:05,800
当然了怎幺找到一个合理的T

162
00:08:05,800 --> 00:08:07,840
得到我们中间这种图呢

163
00:08:08,080 --> 00:08:10,320
是很关键的一步

164
00:08:11,080 --> 00:08:15,120
下面我们就来探讨一下如何选择这个T 

165
00:08:17,120 --> 00:08:20,920
实际上呢选择温度T呢主要是下面一种情况 

166
00:08:21,280 --> 00:08:24,920
假设想从负标签里面呢 学习到更多的有用的知识 

167
00:08:24,920 --> 00:08:26,920
有用的信息或者一些额外的参数 

168
00:08:26,920 --> 00:08:28,520
我们的温度T呢

169
00:08:28,520 --> 00:08:31,400
就是适当的去调高一点点

170
00:08:31,920 --> 00:08:34,040
但是呢当我们想减少负标签

171
00:08:34,040 --> 00:08:35,440
对我们整个神经网络

172
00:08:35,440 --> 00:08:37,280
或者对我们的预测值的干扰的时候呢

173
00:08:37,280 --> 00:08:41,600
我们的温度T呢就适当的往低去调整就好了 

174
00:08:42,480 --> 00:08:44,760
当然了T的大小应该是指为多少

175
00:08:44,760 --> 00:08:46,760
我们需要根据我们实际的情况

176
00:08:46,760 --> 00:08:49,080
实际的任务进行设定的

177
00:08:49,320 --> 00:08:53,000
在分类任务和检测任务 我们的T的选择也是不同的 

178
00:08:56,080 --> 00:08:59,160
接下来呢就正式的回到知识蒸馏这个算法里面 

179
00:09:00,000 --> 00:09:02,640
首先去了解一下 知识蒸馏算法的训练流程呢 

180
00:09:02,640 --> 00:09:06,000
跟传统的训练流程的一个不一样的区别 

181
00:09:07,400 --> 00:09:09,760
第一个呢就是传统的训练流程

182
00:09:09,760 --> 00:09:11,600
传统训练流程我们刚才讲到了

183
00:09:11,600 --> 00:09:13,320
就是我们最大的目标呢

184
00:09:13,320 --> 00:09:20,120
就是训练我们的hard target 对ground truth真实的样本呢 求极大的似然softmax的值 

185
00:09:20,480 --> 00:09:23,520
但是呢在知识蒸馏的训练过程当中呢

186
00:09:23,520 --> 00:09:27,480
我们更多的是希望学习到很多soft target 

187
00:09:28,040 --> 00:09:31,840
利用教师模型的分类的概率呢作为soft target 

188
00:09:32,320 --> 00:09:35,520
就像ZOMI给大家去汇报AI系统的这个相关的知识呢 

189
00:09:35,520 --> 00:09:37,600
我一般来说都不会照着字来念

190
00:09:37,960 --> 00:09:41,560
而是插入了很多我自己的理解或者额外的知识一样 

191
00:09:45,400 --> 00:09:48,040
毫不意外的就是这篇文章的知识蒸馏呢

192
00:09:48,040 --> 00:09:50,800
采用了一个offline的离线的蒸馏方式

193
00:09:50,800 --> 00:09:55,680
然后呢结构上面呢就采用了经典的师生的网络模型 

194
00:09:55,920 --> 00:09:57,600
teacher呢就是知识的输出者

195
00:09:57,600 --> 00:10:00,080
student呢就是知识的接受者握

196
00:10:01,960 --> 00:10:04,640
整个知识蒸馏呢分为两个阶段

197
00:10:04,760 --> 00:10:06,040
注意是两个

198
00:10:06,040 --> 00:10:09,120
第一个就是训练我们的教师模型

199
00:10:09,800 --> 00:10:14,040
第二个就是学生模型进行蒸馏就是学习 

200
00:10:14,840 --> 00:10:19,200
下面呢我就分开两个给大家进行一个汇报

201
00:10:21,760 --> 00:10:25,960
现在呢更多的是一个字面和概念的意义的了解

202
00:10:25,960 --> 00:10:28,600
后面我们会展开一个图让大家看得更清楚 

203
00:10:28,840 --> 00:10:30,520
首先我们训练teacher model的时候呢

204
00:10:30,520 --> 00:10:32,120
teacher model我们叫做Net-T

205
00:10:32,400 --> 00:10:35,200
特点就是这个网络模型呢 相对来说比较复杂 

206
00:10:35,520 --> 00:10:39,520
像transformer之类的大模型呢确实更加适合教师模型 

207
00:10:39,520 --> 00:10:44,440
那唯一的要求就是对于输入的X呢 它都能输出一个Y

208
00:10:44,440 --> 00:10:51,000
那这个Y呢经过softmax的映射呢 能够输出对应概率的一个预测的概率的类别值 

209
00:10:51,600 --> 00:10:54,880
接着第二步就是学生模型进行蒸馏

210
00:10:54,880 --> 00:10:56,760
那学生模型进行蒸馏了说白了

211
00:10:56,760 --> 00:11:00,720
就是我们需要训练一个student model 我们叫做Net-S 

212
00:11:01,040 --> 00:11:06,080
它的特点就是参数量呢相对来说比我们的teacher model要少 

213
00:11:06,080 --> 00:11:08,880
我们的模型结构呢也相对来说简单

214
00:11:08,880 --> 00:11:13,520
同样的有个要求就是对于输入的X呢 我们都能够输出Y 

215
00:11:13,520 --> 00:11:19,600
而Y呢经过softmax的映射后呢 能够与NetT分别对应起来 

216
00:11:19,600 --> 00:11:20,880
用大白话来说呢

217
00:11:20,880 --> 00:11:23,840
就是teacher model我要预测1000个分类

218
00:11:23,840 --> 00:11:26,760
我的student model也需要预测1000个分类

219
00:11:26,760 --> 00:11:28,440
而不是变成500个分类

220
00:11:28,440 --> 00:11:30,200
那这是没办法做映射了

221
00:11:32,920 --> 00:11:33,920
在预训练阶段呢

222
00:11:33,920 --> 00:11:36,560
会训练一个性能比较好的teacher model

223
00:11:36,560 --> 00:11:38,520
所以呢我们会把teacher model的信息呢

224
00:11:38,520 --> 00:11:41,080
给到我们的student model去学习

225
00:11:41,080 --> 00:11:43,880
下面我们来看一下真正的一个算法流程

226
00:11:43,880 --> 00:11:45,960
首先第一步我需要训练一个teacher model

227
00:11:45,960 --> 00:11:47,400
就上面的这个teacher model

228
00:11:47,400 --> 00:11:49,120
我先把它训练出来

229
00:11:49,960 --> 00:11:52,480
接着呢我利用一个高温的T呢

230
00:11:52,480 --> 00:11:54,960
就是这个T的值呢适得比较大

231
00:11:54,960 --> 00:11:59,520
产生一个soft target 把很多冗余的信息呢保留起来 

232
00:12:00,320 --> 00:12:01,880
接着呢就是第二步了

233
00:12:01,880 --> 00:12:05,640
第二步就是我们的student model的学习和蒸馏的过程

234
00:12:05,640 --> 00:12:08,360
那有两个点特别是需要注意的

235
00:12:08,360 --> 00:12:10,120
就是在我们的第三步

236
00:12:10,120 --> 00:12:11,240
第三步的时候呢

237
00:12:11,240 --> 00:12:12,120
这里面呢说白了

238
00:12:12,120 --> 00:12:17,720
就使用一个soft target还有hard target同时训练student model 

239
00:12:17,720 --> 00:12:18,800
那可以看到呢

240
00:12:18,800 --> 00:12:20,520
我们上面有一部分

241
00:12:20,520 --> 00:12:21,400
就这里面呢

242
00:12:21,400 --> 00:12:24,200
有两个损失函数拼在一起

243
00:12:24,200 --> 00:12:26,160
作为一个总的损失函数

244
00:12:26,160 --> 00:12:27,120
这个损失函数呢

245
00:12:27,120 --> 00:12:29,320
就是我们的T呢会比较高

246
00:12:29,320 --> 00:12:30,800
跟上面的可以保持一致

247
00:12:30,840 --> 00:12:32,800
然后呢训练一个soft target

248
00:12:32,800 --> 00:12:36,040
通过distillation loss呢去进行一个学习

249
00:12:36,800 --> 00:12:39,440
那第二个呢就是我们的hard target

250
00:12:39,440 --> 00:12:42,360
通过一个通用的或者普通的一个损失函数

251
00:12:42,360 --> 00:12:44,480
去训练学生网络模型

252
00:12:44,480 --> 00:12:46,720
最后一步就是推理了

253
00:12:46,720 --> 00:12:47,400
推理的时候呢

254
00:12:47,400 --> 00:12:49,320
我们把T视之为1

255
00:12:49,320 --> 00:12:52,000
进行一个学生模型的在线推理

256
00:12:52,000 --> 00:12:55,720
所以student model的训练或者蒸馏的过程呢 会相对来说复杂一点 

257
00:12:55,720 --> 00:13:00,560
它有两个损失 一个是distillation loss 一个是student loss 

258
00:13:00,560 --> 00:13:04,560
下面呢我们来看一下具体的一个loss的情况

259
00:13:06,080 --> 00:13:08,880
这个L呢就是对应的总的损失函数

260
00:13:08,880 --> 00:13:11,160
这里面刚才讲到了有两个损失函数

261
00:13:11,160 --> 00:13:14,200
一个损失函数呢 就是针对一个soft label的损失函数 

262
00:13:14,200 --> 00:13:17,840
一个损失函数呢 就是对应hard label的一个损失函数 

263
00:13:17,840 --> 00:13:21,000
而soft label的损失函数呢 就是来自于我们的teacher model 

264
00:13:21,000 --> 00:13:24,320
hard label呢 就是来自于我们真实的数据标签 

265
00:13:24,320 --> 00:13:27,440
通过两种方式或者通过两个损失函数 

266
00:13:27,440 --> 00:13:31,880
把它合在一起变成一个总的损失函数来去学习 

267
00:13:32,600 --> 00:13:36,240
所以来说我们只需要看懂了这个图的流程

268
00:13:36,240 --> 00:13:39,560
基本上你就明白这个算法是怎么去实现了

269
00:13:40,760 --> 00:13:42,800
好了今天的内容呢就到此为止

270
00:13:42,800 --> 00:13:43,920
如果大家有兴趣的话

271
00:13:43,920 --> 00:13:47,160
也可以看一下相关的文献和内容

272
00:13:47,160 --> 00:13:49,000
谢谢各位 拜了个拜 

273
00:13:49,000 --> 00:13:50,680
卷的不行了卷的不行了

274
00:13:50,680 --> 00:13:52,520
记得一键三连加关注哦

275
00:13:52,520 --> 00:13:55,600
所有的内容都会开源在下面这条链接里面

276
00:13:56,120 --> 00:13:56,920
拜了个拜

