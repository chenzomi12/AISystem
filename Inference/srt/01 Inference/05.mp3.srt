1
00:00:00,000 --> 00:00:04,560
巴巴巴巴巴巴巴巴巴巴

2
00:00:06,560 --> 00:00:07,480
哈喽大家好

3
00:00:07,480 --> 00:00:11,640
我是那个饭后摆步走两腿疼一宿的ZOMI

4
00:00:12,080 --> 00:00:13,800
每次跟着那群架构师吃饭

5
00:00:14,080 --> 00:00:18,760
他们吃完晚饭之后都要散步快半小时到一个小时的

6
00:00:18,760 --> 00:00:19,680
我实在受不了了

7
00:00:19,680 --> 00:00:22,840
所以以后的吃饭我就不跟着他们去散步了

8
00:00:23,120 --> 00:00:27,600
今天我们主要是来到了推理系统这个系列里面的

9
00:00:27,600 --> 00:00:29,600
我个人觉得比较重要的一个章节

10
00:00:29,920 --> 00:00:31,960
就是推理引擎

11
00:00:31,960 --> 00:00:34,240
那我们会在后面的模型小进化

12
00:00:34,240 --> 00:00:35,760
离线优化压缩

13
00:00:35,760 --> 00:00:37,280
部署和运行优化

14
00:00:37,280 --> 00:00:42,920
这些内容都是围绕着我们整个推理引擎去展开的

15
00:00:42,920 --> 00:00:46,040
而推理系统我们不会再深入的去介绍了

16
00:00:46,040 --> 00:00:49,000
因为推理系统更多的是平台相关的工作

17
00:00:49,880 --> 00:00:51,840
这在传统的云服务服务器

18
00:00:52,040 --> 00:00:53,760
还有相关的一些工作

19
00:00:53,920 --> 00:00:57,240
其实已经有非常多的书籍视频

20
00:00:57,680 --> 00:01:01,000
而我更多的是聚焦于AI系统

21
00:01:01,000 --> 00:01:05,720
这个AI系统里面推理引擎是其中占的比较大的一块

22
00:01:05,720 --> 00:01:10,240
我们今天主要是给大家分开两个内容来去汇报的

23
00:01:10,240 --> 00:01:13,280
第一个就是去看看推理引擎的一个主要的特点

24
00:01:13,280 --> 00:01:14,400
有了这些特点

25
00:01:14,480 --> 00:01:17,240
我们看一下推理引擎的一个技术的挑战

26
00:01:17,240 --> 00:01:19,440
这些纯粹是吹牛逼的工作

27
00:01:19,440 --> 00:01:22,520
让大家了解一下具体的概念就好了

28
00:01:22,520 --> 00:01:25,320
就你可能会比较系统的去了解一下

29
00:01:25,520 --> 00:01:28,040
接着我们会在下个内容里面

30
00:01:28,040 --> 00:01:30,840
去给大家汇报一个整体的架构

31
00:01:30,840 --> 00:01:33,400
因为我们知道推理引擎有哪些模块

32
00:01:33,400 --> 00:01:34,200
有哪些架构

33
00:01:34,200 --> 00:01:35,640
它应该注意哪些功能

34
00:01:36,640 --> 00:01:38,960
我们就可以站在一个比较宏观的概念

35
00:01:38,960 --> 00:01:42,640
去了解整个推理引擎应该长成什幺样子

36
00:01:42,640 --> 00:01:44,880
面对我们自身自己的业务

37
00:01:44,880 --> 00:01:47,040
我应该设计一个怎幺样的推理引擎

38
00:01:47,040 --> 00:01:49,320
最后我们去简单的去看看

39
00:01:49,320 --> 00:01:51,240
整个推理引擎的工作流程

40
00:01:51,240 --> 00:01:52,520
它到底是怎幺样的

41
00:01:52,640 --> 00:01:54,880
这两个工作应该比较重要的

42
00:01:54,880 --> 00:01:57,800
或者内容我会去在下一节给大家汇报

43
00:01:57,800 --> 00:01:59,560
今天我们来吹吹牛逼

44
00:01:59,560 --> 00:02:02,160
去看看推理引擎的特点和技术挑战

45
00:02:05,720 --> 00:02:07,480
下面我们看看推理引擎

46
00:02:07,480 --> 00:02:11,040
我总结的有四个比较重要的特点

47
00:02:11,040 --> 00:02:12,640
第一个就是轻量化

48
00:02:12,640 --> 00:02:14,320
第二个你要做到通用

49
00:02:14,320 --> 00:02:17,320
第三个你除了通用你还要易用

50
00:02:17,320 --> 00:02:19,480
因为推理引擎其实易用性

51
00:02:19,480 --> 00:02:21,240
很多人去忽略掉的

52
00:02:21,280 --> 00:02:24,800
也是我们一开始去追求极致的高效

53
00:02:24,800 --> 00:02:27,320
就是高性能的时候去忽略掉的

54
00:02:27,320 --> 00:02:29,760
所以说在这幺多特点里面

55
00:02:29,880 --> 00:02:32,000
我觉得最重要的就是高效

56
00:02:32,000 --> 00:02:34,160
还有易用两个点

57
00:02:35,160 --> 00:02:38,600
接下来我们每一个点去打开

58
00:02:41,760 --> 00:02:45,440
首先第一个就是高性能就高效的问题

59
00:02:47,040 --> 00:02:49,440
推理引擎其实要适配非常多的

60
00:02:49,480 --> 00:02:51,680
不同的架构和操作系统

61
00:02:51,680 --> 00:02:53,760
我们希望在单线层下面

62
00:02:53,760 --> 00:02:56,040
不管我们到时候跑的是一个并行还是什幺

63
00:02:56,040 --> 00:02:58,120
希望尽可能的在单线层下面

64
00:02:58,480 --> 00:03:01,000
整个模型的运行效率

65
00:03:01,000 --> 00:03:04,600
是占满我们整个设备的计算的峰值

66
00:03:04,600 --> 00:03:06,240
不管是理论峰值还是实际峰值

67
00:03:06,240 --> 00:03:08,840
我们希望跑得越快越好

68
00:03:09,360 --> 00:03:11,800
第二个点就是针对对应的一个加速芯片

69
00:03:11,920 --> 00:03:14,400
我们希望能够做到一个深度的调油

70
00:03:14,400 --> 00:03:17,200
做一个极致性能的优化

71
00:03:17,320 --> 00:03:19,360
我们可能会在OpenCL或者Win卡上面

72
00:03:19,720 --> 00:03:22,000
去做不同的一些写不同的算子

73
00:03:22,000 --> 00:03:23,160
写不同的kernel

74
00:03:23,160 --> 00:03:26,480
可能甚至我们会编写一些会议编的代码

75
00:03:26,480 --> 00:03:28,600
或者SIMT的一些代码

76
00:03:28,600 --> 00:03:30,960
充分的去发挥我们的算力

77
00:03:31,320 --> 00:03:33,520
说白了就是越快越好

78
00:03:33,520 --> 00:03:35,360
不管我用什幺方式去实现

79
00:03:35,360 --> 00:03:38,040
反正你得给我实现的越快越好

80
00:03:39,000 --> 00:03:42,760
最后我们需要支持不同精度的一个计算

81
00:03:42,760 --> 00:03:45,760
在不同的架构上面去进行一个适配的

82
00:03:45,800 --> 00:03:48,920
这个就是高性能的一个具体的特点

83
00:03:49,240 --> 00:03:52,320
可以看到其实我们不同不管是哪个框架

84
00:03:53,200 --> 00:03:55,080
这些都是一些推理的框架

85
00:03:55,080 --> 00:03:56,800
大家可以简单的去了解一下

86
00:03:56,800 --> 00:03:58,480
有NCAN腾讯的

87
00:03:58,480 --> 00:04:00,720
还有MAC小米的

88
00:04:00,960 --> 00:04:02,120
TFLite谷歌的

89
00:04:02,120 --> 00:04:03,040
还有QML

90
00:04:03,040 --> 00:04:04,600
苹果的MMN的

91
00:04:04,600 --> 00:04:05,480
阿里的单压

92
00:04:05,600 --> 00:04:07,680
还有华为的Mathbot Lite

93
00:04:08,760 --> 00:04:10,520
华为的产品其实我也不知道为啥

94
00:04:10,520 --> 00:04:13,120
大家很少把它做一个对标的例子

95
00:04:13,520 --> 00:04:15,720
可以看到了不管是哪个情况

96
00:04:15,760 --> 00:04:18,840
我们都会在不同的设备上面去做一个比较

97
00:04:18,840 --> 00:04:20,680
这些比较就是比较性能

98
00:04:21,480 --> 00:04:23,320
性能才是第一位的

99
00:04:23,320 --> 00:04:25,000
性能才是我们推理引擎

100
00:04:25,000 --> 00:04:27,160
所聚焦的最重要的一个点

101
00:04:29,800 --> 00:04:32,160
接着我们看一下轻量化

102
00:04:33,160 --> 00:04:35,240
因为我们的推理引擎

103
00:04:35,360 --> 00:04:38,520
要部署到不同的硬件设备上面

104
00:04:38,520 --> 00:04:40,240
这个就是华为

105
00:04:40,840 --> 00:04:45,240
从2012年到2019年的相关旗舰手机

106
00:04:45,360 --> 00:04:46,400
我们的推理引擎

107
00:04:46,480 --> 00:04:49,000
要部署在不同的一个手机上面

108
00:04:49,000 --> 00:04:50,080
不同的设备上面

109
00:04:50,080 --> 00:04:51,200
对我们的要求

110
00:04:51,200 --> 00:04:53,080
轻量化的要求是非常高的

111
00:04:53,080 --> 00:04:56,560
第二个可能我们还会部署在一些手表

112
00:04:56,560 --> 00:04:57,960
手机耳环上面

113
00:04:58,480 --> 00:05:01,080
这个时候这些包括我们的手机

114
00:05:01,280 --> 00:05:03,560
包括我们的耳机降噪功能也好

115
00:05:03,560 --> 00:05:06,800
我们都会有对应的一些AI的系统

116
00:05:06,800 --> 00:05:07,560
AI的设备

117
00:05:07,560 --> 00:05:09,080
推理的引擎

118
00:05:09,280 --> 00:05:11,120
这个时候就需要我们进一步的

119
00:05:11,120 --> 00:05:13,560
去降低我们整个包的大小

120
00:05:14,440 --> 00:05:15,520
简单的来说

121
00:05:15,720 --> 00:05:18,200
我们在手机上面去部署的推理引擎

122
00:05:18,200 --> 00:05:19,840
可能在我们手表

123
00:05:19,840 --> 00:05:22,520
还有耳机上面部署的推理引擎

124
00:05:22,520 --> 00:05:23,520
是不同的

125
00:05:23,720 --> 00:05:25,400
我们举个具体的例子

126
00:05:25,400 --> 00:05:27,160
我们现在华为手机上面

127
00:05:27,280 --> 00:05:28,760
部署的是Mathball Lite

128
00:05:28,960 --> 00:05:29,880
可能在手表上面

129
00:05:29,960 --> 00:05:31,480
我们部署的是Mathball的

130
00:05:31,480 --> 00:05:32,920
Macro的版本

131
00:05:32,920 --> 00:05:34,640
所以说它不同的版本

132
00:05:34,640 --> 00:05:36,640
对轻量化的要求是不一样的

133
00:05:36,640 --> 00:05:37,440
不同的设备

134
00:05:37,480 --> 00:05:39,440
对轻量化的要求也不一样

135
00:05:41,360 --> 00:05:45,160
接着我们看一个通用性的问题

136
00:05:45,160 --> 00:05:46,160
作为一个推理引擎

137
00:05:46,280 --> 00:05:48,400
我需要支持非常多不同的框架

138
00:05:48,400 --> 00:05:50,200
训练出来的一个格式

139
00:05:50,240 --> 00:05:52,920
而且我还要支持很多不同的

140
00:05:52,920 --> 00:05:54,720
主流的网络模型结构

141
00:05:54,720 --> 00:05:57,480
所以说为什幺做系统的人要懂算法

142
00:05:57,480 --> 00:05:59,400
我们现在其实有很多系统的

143
00:05:59,400 --> 00:06:00,960
工程师是不懂算法的

144
00:06:00,960 --> 00:06:02,240
或者从其他产品线

145
00:06:02,240 --> 00:06:04,920
或者传统的一些优先过来的

146
00:06:05,360 --> 00:06:07,040
懂算法很重要

147
00:06:07,040 --> 00:06:09,560
懂业务也很重要

148
00:06:09,800 --> 00:06:10,760
接着我们可以看到

149
00:06:10,800 --> 00:06:12,080
其实我们通用性

150
00:06:12,080 --> 00:06:14,120
会遇到很大的一个挑战

151
00:06:14,120 --> 00:06:16,280
这个也是会在后面去讲

152
00:06:16,280 --> 00:06:17,440
有什幺解决方案的

153
00:06:17,440 --> 00:06:19,880
第一个就是支持多输入的多输出

154
00:06:19,880 --> 00:06:22,080
还有任意维度的输入输出

155
00:06:22,080 --> 00:06:24,560
可能还会有动态的batch

156
00:06:24,560 --> 00:06:27,800
另外可能还会支持带空滋流的模型

157
00:06:27,800 --> 00:06:31,520
这些都是非常大的一些挑战和特点

158
00:06:32,640 --> 00:06:34,120
而在中米的眼中

159
00:06:34,320 --> 00:06:35,720
可能动态的输入

160
00:06:35,880 --> 00:06:37,800
是比较大的一个挑战

161
00:06:37,800 --> 00:06:39,880
对我们的一个推进引擎来说

162
00:06:39,880 --> 00:06:42,160
因为我们在处理一些NLP的样术

163
00:06:42,280 --> 00:06:43,160
例如BERT的时候

164
00:06:43,160 --> 00:06:45,400
我们的输入是一个变长的串行

165
00:06:45,400 --> 00:06:48,000
变长的串行就需要支持动态的输入

166
00:06:48,000 --> 00:06:49,960
对我们的一个引擎来说

167
00:06:50,080 --> 00:06:52,080
推进引擎是一个很大的挑战

168
00:06:52,080 --> 00:06:53,080
接着我们看一下

169
00:06:53,080 --> 00:06:54,480
可能我们还要支持服务器

170
00:06:54,600 --> 00:06:56,720
跟电脑还有不同的操作系统

171
00:06:56,720 --> 00:06:58,640
可以看到我讲的是举个例子

172
00:06:58,640 --> 00:07:00,080
现在华为的设备

173
00:07:00,200 --> 00:07:01,920
我都是来用华为作为例子

174
00:07:02,440 --> 00:07:04,160
虽然我不在终端产品线

175
00:07:04,840 --> 00:07:06,920
像华为它自己就有非常多的

176
00:07:06,920 --> 00:07:07,720
不同的设备

177
00:07:07,720 --> 00:07:08,960
包括笔记本的显示器

178
00:07:09,080 --> 00:07:10,440
还有平板智能屏

179
00:07:10,480 --> 00:07:11,600
还有可穿戴的设备

180
00:07:11,600 --> 00:07:13,680
包括现在还推出了一个打印机

181
00:07:14,760 --> 00:07:16,000
像这些设备

182
00:07:16,480 --> 00:07:18,800
都是用不同的操作系统的

183
00:07:19,160 --> 00:07:22,200
而且它都有不同的枪式的一个接口

184
00:07:22,200 --> 00:07:25,320
所以我们需要支持非常多的一个设备

185
00:07:25,320 --> 00:07:26,440
还有操作系统

186
00:07:26,440 --> 00:07:27,520
对于通用性来说

187
00:07:27,720 --> 00:07:29,440
挑战是非常大的

188
00:07:32,200 --> 00:07:34,480
最后我们看一下易用性

189
00:07:34,720 --> 00:07:36,560
易用性对于普通用户来说

190
00:07:36,560 --> 00:07:37,280
可能不感知

191
00:07:37,280 --> 00:07:39,280
但是对于开发者来说

192
00:07:39,280 --> 00:07:41,480
是非常重要的一个内容

193
00:07:41,480 --> 00:07:43,280
我们可能会使用很多

194
00:07:43,280 --> 00:07:44,000
lumpi的算子

195
00:07:44,000 --> 00:07:45,880
去做一些常用的一个计算

196
00:07:46,080 --> 00:07:46,560
这个时候

197
00:07:46,880 --> 00:07:48,800
AI推理框架怎幺跟它混用呢

198
00:07:50,600 --> 00:07:51,080
另外的话

199
00:07:51,080 --> 00:07:51,680
我们的任务

200
00:07:51,840 --> 00:07:54,480
可能更多的是聚焦CV或者NLP

201
00:07:54,480 --> 00:07:55,280
常用的任务

202
00:07:55,280 --> 00:07:57,080
我们希望不需要拧入大量的

203
00:07:57,080 --> 00:07:58,600
OpenCV这种三方的包

204
00:07:58,600 --> 00:08:01,360
而是怎幺样的快速的给用户

205
00:08:01,400 --> 00:08:03,840
给开发者提供相关的API

206
00:08:03,840 --> 00:08:06,720
最后可能会支持很多平台的模型训练

207
00:08:06,720 --> 00:08:07,960
还有丰富的API

208
00:08:07,960 --> 00:08:10,920
中米觉得第一点和第二点是比较重要的

209
00:08:10,920 --> 00:08:12,560
而我只有第一点和第二点

210
00:08:12,720 --> 00:08:14,760
其实提过了一些相关的想法

211
00:08:14,760 --> 00:08:17,280
自己当时候也手撸了一把

212
00:08:17,600 --> 00:08:19,400
确实做了非常多的相关的工作

213
00:08:19,400 --> 00:08:21,320
在这里面打了非常多的代码

214
00:08:24,480 --> 00:08:27,040
接着我们看一下技术的挑战

215
00:08:27,040 --> 00:08:28,240
我们的challenge

216
00:08:29,600 --> 00:08:30,480
技术的挑战

217
00:08:30,520 --> 00:08:32,320
其实了解完刚才的一些特点

218
00:08:32,440 --> 00:08:34,440
我们就可以总结了一些矛盾点

219
00:08:34,440 --> 00:08:36,840
第一个就是我们的AI的需求很复杂

220
00:08:36,840 --> 00:08:39,000
那进程的大小有限吗

221
00:08:40,120 --> 00:08:41,120
我们举几个例子

222
00:08:41,120 --> 00:08:43,520
就现在Pytorch有1200多个算子

223
00:08:43,520 --> 00:08:45,560
TensorFlow接近2000多个算子

224
00:08:45,560 --> 00:08:46,960
但是我们推理引擎

225
00:08:47,400 --> 00:08:50,240
针对每个后端都要提供这幺多算子吗

226
00:08:50,240 --> 00:08:51,400
那可不爆炸了

227
00:08:51,680 --> 00:08:52,680
针对马力的GPU

228
00:08:52,680 --> 00:08:54,120
我提供1200多个算子

229
00:08:54,560 --> 00:08:55,640
针对骁龙的芯片

230
00:08:55,640 --> 00:08:57,200
我们提供1200多个算子

231
00:08:57,200 --> 00:08:58,160
那还得了

232
00:08:58,160 --> 00:08:59,120
我们克隆开发算子

233
00:08:59,120 --> 00:09:00,680
开发的同时自己累死了

234
00:09:00,680 --> 00:09:01,760
天天加班三班

235
00:09:01,760 --> 00:09:02,880
倒都搞不定了

236
00:09:03,720 --> 00:09:05,040
所以说这个时候

237
00:09:05,280 --> 00:09:07,440
我们的需求是非常复杂的

238
00:09:07,440 --> 00:09:09,040
但是我们的进程大小有限

239
00:09:09,040 --> 00:09:11,480
我们的人员开发工作量也有限

240
00:09:11,760 --> 00:09:12,840
接着我们看一下

241
00:09:13,160 --> 00:09:14,200
我们的AI

242
00:09:14,400 --> 00:09:15,880
其实除了我们的模型推理

243
00:09:15,880 --> 00:09:18,760
我们还包括很多的千维域处理的问题

244
00:09:18,760 --> 00:09:20,800
不希望引入大量的三方依赖

245
00:09:20,800 --> 00:09:23,120
这个时候就需要有限的支持

246
00:09:23,480 --> 00:09:26,600
所以说这里面两个都是一个矛盾的点

247
00:09:26,920 --> 00:09:28,600
接着我们的算力需求

248
00:09:28,600 --> 00:09:30,160
跟我们的资源碎片化

249
00:09:30,440 --> 00:09:31,800
是一个矛盾的点

250
00:09:32,160 --> 00:09:33,520
我们的AI模型

251
00:09:33,760 --> 00:09:36,560
往往都是需要非常大量的计算量的

252
00:09:36,840 --> 00:09:37,800
但是我们推理引擎

253
00:09:37,880 --> 00:09:40,800
大部分都在一个IoT的设备

254
00:09:40,800 --> 00:09:42,960
或者一些边缘的设备进行一个推理的

255
00:09:43,280 --> 00:09:46,600
这个时候就会引入一个很大的矛盾

256
00:09:46,600 --> 00:09:48,720
我需要大量的一个计算资源

257
00:09:48,720 --> 00:09:51,600
但是我的硬件计算资源有限

258
00:09:51,880 --> 00:09:52,480
怎幺协调

259
00:09:52,840 --> 00:09:53,440
怎幺综合

260
00:09:54,040 --> 00:09:55,680
第二个就是我的计算资源

261
00:09:56,120 --> 00:09:58,080
我就以一个手机作为例子

262
00:09:58,240 --> 00:10:00,120
手机它是一款SoC

263
00:10:00,520 --> 00:10:03,120
SoC里面就包括用了ARM的CPU

264
00:10:03,400 --> 00:10:05,360
可能会用了马力的GPU

265
00:10:05,680 --> 00:10:07,800
可能会有自己的一个DSP

266
00:10:08,120 --> 00:10:09,800
麒麟就会有自己的DSP

267
00:10:10,600 --> 00:10:14,240
可能华为麒麟里面还有一个自己的NPU

268
00:10:16,640 --> 00:10:17,880
这个时候可以看到

269
00:10:17,880 --> 00:10:19,840
就算一款手指甲大小

270
00:10:20,120 --> 00:10:23,280
放在手机里面的一款芯片

271
00:10:23,640 --> 00:10:26,560
里面的计算资源都是极度的碎片化的

272
00:10:26,960 --> 00:10:28,840
每一个不同的硬件

273
00:10:28,840 --> 00:10:30,360
每一个不同的IP

274
00:10:30,520 --> 00:10:32,240
我们CPU可能当一个IP

275
00:10:32,240 --> 00:10:33,560
GPU当一个IP

276
00:10:33,800 --> 00:10:36,200
它都会有自己的一个编程的体系

277
00:10:36,680 --> 00:10:39,920
这个时候会使我们的进程极度的膨胀

278
00:10:40,240 --> 00:10:41,880
这是两个比较大的矛盾

279
00:10:45,120 --> 00:10:49,960
最后一个就是执行效率跟模型精度的一个矛盾

280
00:10:50,320 --> 00:10:53,800
我们希望我们的模型的效率跑得越快越好

281
00:10:54,040 --> 00:10:55,360
而且精度越高越好

282
00:10:55,680 --> 00:10:57,960
但是我们的模型的精度

283
00:10:57,960 --> 00:10:59,120
模型变小了

284
00:10:59,120 --> 00:11:01,240
模型的精度就会下降

285
00:11:01,640 --> 00:11:03,400
这是个不可调和的矛盾

286
00:11:03,880 --> 00:11:06,280
于是引入了很多这种量化的技术

287
00:11:06,280 --> 00:11:07,200
压缩的技术

288
00:11:07,200 --> 00:11:08,400
就希望模型小一点

289
00:11:08,400 --> 00:11:10,800
但是保持我们的原模型的状态

290
00:11:10,800 --> 00:11:14,920
还有很多做一个模型创新小模型的研究

291
00:11:15,360 --> 00:11:17,720
接着第二点就是云测的模型的精度

292
00:11:17,800 --> 00:11:20,360
我们希望在训练的时候精度越高越好

293
00:11:20,600 --> 00:11:22,920
转移到端测的时候越小越好

294
00:11:22,920 --> 00:11:24,440
但精度你不能给我掉

295
00:11:24,760 --> 00:11:28,600
例如我们现在在做一个大模型

296
00:11:29,200 --> 00:11:32,800
大模型它可能会有千亿的规模的参数量

297
00:11:32,800 --> 00:11:36,360
千亿规模可能动辄就上几百兆或者一个G

298
00:11:36,680 --> 00:11:39,520
一个G的一个模型你推到我们手机上面

299
00:11:39,520 --> 00:11:40,680
那不卡死了吗

300
00:11:40,680 --> 00:11:42,520
手机里面就两个G的内存

301
00:11:42,520 --> 00:11:45,040
你还希望它塞你一个模型进一个G吗

302
00:11:45,040 --> 00:11:46,000
这不可能的

303
00:11:46,840 --> 00:11:50,240
所以我们希望对我们的大模型进行一个100倍的压缩

304
00:11:50,440 --> 00:11:55,240
保持可能0.5%或者千分之五的一个进步的损失

305
00:11:55,720 --> 00:11:56,520
这个时候怎幺做

306
00:11:56,720 --> 00:11:58,720
这是一个很大的挑战

307
00:12:01,440 --> 00:12:03,920
好了今天的内容就到这里为止

308
00:12:03,920 --> 00:12:06,960
我们今天主要是讲了推进引擎的一个主要的特点

309
00:12:06,960 --> 00:12:10,920
四个特点效率通用性应用性轻量化

310
00:12:11,280 --> 00:12:15,800
最后我们了解了一下一些不可避免的一些技术的挑战

311
00:12:15,800 --> 00:12:18,120
主要是有几个矛盾所引起的

312
00:12:18,400 --> 00:12:20,120
自由的碎片化算定的需求大

313
00:12:20,440 --> 00:12:22,520
模型的精度高模型小

314
00:12:24,000 --> 00:12:26,520
那今天我们了解完这些基础的概念

315
00:12:26,520 --> 00:12:28,600
可能我的语速会稍微快一点没关系

316
00:12:28,600 --> 00:12:30,720
这是只是一些简单的concept

317
00:12:31,080 --> 00:12:32,840
再往下一节内容里面

318
00:12:32,960 --> 00:12:35,560
我们就会讲整体的架构和工作流程了

319
00:12:35,560 --> 00:12:36,760
好了谢谢各位

320
00:12:36,760 --> 00:12:37,640
拜拜

321
00:12:38,120 --> 00:12:39,680
卷的不行了卷的不行了

322
00:12:39,680 --> 00:12:41,120
记得一键三连加关注

323
00:12:41,560 --> 00:12:44,600
所有的内容都会开源在下面这条链接里面

324
00:12:45,200 --> 00:12:45,880
拜拜

