1
00:00:00,000 --> 00:00:04,864
字幕生成：Galaxies     字幕校对：NaHS

2
00:00:04,864 --> 00:00:07,880
Hello大家好，我是好久没有更新的周米,

3
00:00:07,880 --> 00:00:11,400
最近确实实在是太忙了，忙着ChatGPT这个项目,

4
00:00:11,400 --> 00:00:15,960
这个项目就导致我经常加班,晚上回来就有了惰性。

5
00:00:15,960 --> 00:00:20,240
今天我们还是来到了推理引擎的kernel优化,

6
00:00:20,240 --> 00:00:24,880
在整个kernel优化里面，我们来到了第一个比较简单的内容,

7
00:00:24,880 --> 00:00:29,440
就是我们看看卷积优化的整体的原理，卷积需要做哪些优化

8
00:00:29,440 --> 00:00:32,920
下面我们看一下整个kernel优化,或者我们的卷积优化里面,

9
00:00:32,920 --> 00:00:32,945
我们还在第一个阶段,算法的优化,

10
00:00:32,945 --> 00:00:35,720
我们还在第一个阶段,算法的优化,

11
00:00:35,720 --> 00:00:39,920
而这里面的算法主要还是讲卷积的优化。

12
00:00:39,920 --> 00:00:42,160
接着我们来再深入一下,

13
00:00:42,160 --> 00:00:46,720
kernel优化主要是围绕着kernel层去实现的,

14
00:00:46,720 --> 00:00:49,560
而我们之前其实已经给大家重复讲过了,

15
00:00:49,560 --> 00:00:52,240
我们对一个简单的卷积算子,

16
00:00:52,240 --> 00:00:56,360
在CPU里面可能会使用NEON指令集去实现

17
00:00:56,360 --> 00:00:56,365
也可能会使用X86的AVX指令集去实现

18
00:00:56,365 --> 00:01:00,040
也可能会使用X86的AVX指令集去实现

19
00:01:00,040 --> 00:01:04,360
在一个推进引擎里面的实现的方式就有非常多种,

20
00:01:04,360 --> 00:01:08,215
因为我们的推进引擎要支持CPU，也要支持GPU

21
00:01:08,215 --> 00:01:08,240
而CPU就有两种不同的实现

22
00:01:08,240 --> 00:01:10,815
而CPU就有两种不同的实现

23
00:01:10,840 --> 00:01:14,640
可能在GPU上面我们就有更多种不同的实现

24
00:01:14,640 --> 00:01:18,025
可能我们会用CUDA实现、用OpenCL、用Vulkan

25
00:01:18,040 --> 00:01:21,360
可能还会用OpenGL，还有Meta来去实现。

26
00:01:21,400 --> 00:01:26,360
最终实现完的，我们会把它封装成一个高性能的算子库,

27
00:01:26,360 --> 00:01:28,600
也可能直接提供kernel层。

28
00:01:28,600 --> 00:01:30,680
废话和前期知识有点多,

29
00:01:30,770 --> 00:01:36,000
我们再往下看一下，在整个kernel优化里面我们会涉及到哪些内容。

30
00:01:36,000 --> 00:01:39,080
首先我们会去看一下什么是卷积

31
00:01:39,080 --> 00:01:43,300
那如果大家都懂的，这个概念可以跳过这一节，来去看下一节。

32
00:01:43,300 --> 00:01:49,280
下一节我们就会讲讲Caffe里面用的最多的img2col这种优化的算法,

33
00:01:49,280 --> 00:01:49,305
它是一种卷积的具体的实现方法

34
00:01:49,305 --> 00:01:51,480
它是一种卷积的具体的实现方法

35
00:01:51,480 --> 00:01:56,960
把卷积的操作变成GEMM(General Matrix Multiplication)这种运算的方式。

36
00:01:56,960 --> 00:01:59,200
然后我们会有一个空间组合的优化

37
00:01:59,200 --> 00:02:00,880
空间组合优化其实比较简单

38
00:02:00,880 --> 00:02:03,760
跟我们编译里面的其实是比较相似的

39
00:02:03,760 --> 00:02:07,535
也是结合了img2col的思想来去实现的

40
00:02:07,535 --> 00:02:07,560
在后面的两节里面就有点意思了

41
00:02:07,560 --> 00:02:10,495
在后面的两节里面就有点意思了

42
00:02:10,520 --> 00:02:14,320
里面我们会讲到Winograd这种优化的算法

43
00:02:14,320 --> 00:02:18,840
Winograd这个优化算法是在1980几年的时候已经提出来了

44
00:02:18,840 --> 00:02:23,200
但是当时候因为算力的问题没有很好的得到一个重用

45
00:02:23,200 --> 00:02:29,180
现在应该是在2011年之后Winpgrad开始慢慢的起来了

46
00:02:29,200 --> 00:02:32,720
确实它在做一些小卷积核的计算的时候

47
00:02:32,720 --> 00:02:34,520
效果性能还是很好的

48
00:02:34,520 --> 00:02:39,585
然后再到QNNPACK，间接卷积优化这种方式

49
00:02:39,640 --> 00:02:43,200
去看看我们卷积怎幺做不同的优化。

50
00:02:43,215 --> 00:02:45,680
现在我们来到了第一个内容,

51
00:02:45,680 --> 00:02:48,200
就是卷积的基础概念

52
00:02:48,240 --> 00:02:50,720
我们来了解一下什幺是卷积

53
00:02:50,720 --> 00:02:53,640
现在我们看一下什么是卷积

54
00:02:53,640 --> 00:02:57,760
实际上卷积是神经网络里面的内核计算单元之一。

55
00:02:57,760 --> 00:02:59,040
为什么叫做之一呢?

56
00:02:59,040 --> 00:03:02,560
因为在神经网络里面的最核心的几种计算单元

57
00:03:02,560 --> 00:03:07,040
有卷积CNN,也有LSTM,还有Transformer。

58
00:03:07,040 --> 00:03:10,720
最后一个就是最常用的MatMul,矩阵相乘。

59
00:03:10,720 --> 00:03:15,080
实际上卷积在前几年应该是非常非常的火,

60
00:03:15,080 --> 00:03:17,480
基本上所有的CPU算法都离不开它。

61
00:03:17,520 --> 00:03:20,360
包括我们常用的Restnet,MobileNet,EfficientNet,

62
00:03:20,360 --> 00:03:24,560
这些都是使用了或者大量的使用了卷积的计算。

63
00:03:24,560 --> 00:03:30,000
但是实际上卷积的变种是非常非常的丰富和多样的。

64
00:03:30,000 --> 00:03:31,880
我们除了通用的卷积计算之外,

65
00:03:31,880 --> 00:03:34,600
我们还会在卷积里面加上Bias

66
00:03:34,600 --> 00:03:37,400
我们可能会对卷积进行空洞卷积

67
00:03:37,400 --> 00:03:39,840
可能还会进行一个Device的卷积

68
00:03:39,840 --> 00:03:42,760
所以我们会说卷积的变种非常丰富

69
00:03:42,760 --> 00:03:45,560
而且它的计算还是非常的复杂

70
00:03:45,600 --> 00:03:49,120
我们的神经网络或者在一个网络模型当中

71
00:03:49,120 --> 00:03:52,800
大部分时间都消耗在我们的卷积计算

72
00:03:52,800 --> 00:03:57,880
所以如何对我们的卷积进行优化就变得非常重要

73
00:03:57,880 --> 00:04:02,320
这也是我们这个系列或者在Kernel优化里面重点去讲解的

74
00:04:02,320 --> 00:04:05,160
当然Transform也是一种很好的例子

75
00:04:05,160 --> 00:04:07,000
但是Transform相关的优化

76
00:04:07,000 --> 00:04:09,960
说实话其实并不是说非常的多

77
00:04:09,960 --> 00:04:11,000
没有像卷积那样

78
00:04:11,000 --> 00:04:14,800
而且卷积在端侧推理引擎里面是非常的成熟

79
00:04:14,800 --> 00:04:17,440
而Transform在端侧推理引擎里面

80
00:04:17,440 --> 00:04:21,240
现在来看应用的不是非常多

81
00:04:21,240 --> 00:04:24,120
我们在前面其实已经介绍过Transform这个结构

82
00:04:24,120 --> 00:04:28,680
在端侧推理引擎里面现在还在慢慢的引入阶段

83
00:04:28,680 --> 00:04:30,520
还没有等到大规模成熟

84
00:04:30,520 --> 00:04:32,760
而且随着时间技术的发展

85
00:04:32,760 --> 00:04:37,440
研究员就提出了多种的关于卷积的优化方法

86
00:04:37,440 --> 00:04:39,240
包括img2col还有Winograd

87
00:04:39,240 --> 00:04:43,080
下面我们来看一下什么为之卷积

88
00:04:43,280 --> 00:04:46,720
下面是卷积的一些我在网上找到的

89
00:04:46,720 --> 00:04:48,120
维基百科的概念

90
00:04:48,120 --> 00:04:50,080
说实话我看的不是很懂

91
00:04:50,080 --> 00:04:52,880
如果大家想深入了解什么为之卷积

92
00:04:52,880 --> 00:04:54,520
卷积跟傅里叶之间的关系

93
00:04:54,520 --> 00:04:56,200
还有卷积的具体的原理,

94
00:04:56,200 --> 00:04:58,760
大家可以看一下网上相关的介绍,

95
00:04:58,760 --> 00:05:00,080
这也是非常的多,

96
00:05:00,080 --> 00:05:01,880
我们简单的去给大家念一念,

97
00:05:01,880 --> 00:05:04,360
卷积主要是通过两个函数,

98
00:05:04,360 --> 00:05:06,120
一个F,一个是G,

99
00:05:06,120 --> 00:05:10,880
通过两个函数生成第三个函数的一种具体的数学运算,

100
00:05:10,880 --> 00:05:14,080
我们可以把它变成等于H(x)

101
00:05:14,080 --> 00:05:16,720
它的本质是一种特殊的积分变换,

102
00:05:16,720 --> 00:05:20,240
所以我们可以看到在这里面有个积分的符号,

103
00:05:20,240 --> 00:05:22,680
这里面就表示了表征函数,

104
00:05:22,680 --> 00:05:24,360
F是一个表征函数,

105
00:05:24,360 --> 00:05:25,840
G也是一个表征函数,

106
00:05:25,840 --> 00:05:29,280
经过翻转和平均重叠的部分的函数的乘积,

107
00:05:29,280 --> 00:05:32,320
里面的一个长度的积分,

108
00:05:32,320 --> 00:05:35,360
那幺可以看到有两个框框,

109
00:05:35,360 --> 00:05:36,840
第一个是蓝色的框框,

110
00:05:36,840 --> 00:05:40,360
第二个就是这里面移动的黄色的框框,

111
00:05:40,360 --> 00:05:42,560
蓝色的框框是我们的f(τ)

112
00:05:42,560 --> 00:05:45,680
而红色的框框是我们的g(t-τ)

113
00:05:45,680 --> 00:05:50,680
而我们的三角形经过的三角形就是F跟G两个表征函数,

114
00:05:50,680 --> 00:05:52,560
而得到的一个积分的概念,

115
00:05:52,560 --> 00:05:55,480
说实话积分的概念我不明白为什幺要这幺做,

116
00:05:55,480 --> 00:05:57,960
现在我们看一下积分的公式,

117
00:05:57,960 --> 00:06:01,000
我们可以看到积分公式里面有几个比较重要的元素,

118
00:06:01,000 --> 00:06:02,200
第一个就是T,

119
00:06:02,200 --> 00:06:03,360
第二个就是τ

120
00:06:03,360 --> 00:06:04,880
第三个就是T-τ

121
00:06:04,880 --> 00:06:07,640
我们把它组成一个新的公式,

122
00:06:07,640 --> 00:06:12,320
现在我们可以看到新的公式就是T=τ+(t-τ)

123
00:06:12,320 --> 00:06:15,560
我们把它再去分裂成两边两条公式,

124
00:06:15,560 --> 00:06:17,640
第一条就是X=τ

125
00:06:17,640 --> 00:06:20,560
第二个就是Y=T-τ

126
00:06:20,560 --> 00:06:23,400
然后我们把这两个公式再组合起来,

127
00:06:23,400 --> 00:06:25,320
变成X+Y=n

128
00:06:25,320 --> 00:06:28,800
这时候可以看到这是一条线性的公式,

129
00:06:28,800 --> 00:06:30,800
我们可以往下再看一看,

130
00:06:30,800 --> 00:06:33,760
这条线性的公式把它在二维空间里面画出来了,

131
00:06:33,760 --> 00:06:36,000
就是像右边的所示,

132
00:06:36,000 --> 00:06:39,840
我们把N不断的去放成不同的一种数值,

133
00:06:39,840 --> 00:06:44,960
然后可以看到这条公式类似于在我们的平面当中不断的去划过,

134
00:06:44,960 --> 00:06:47,120
如果编辑刚才上面这条直线,

135
00:06:47,120 --> 00:06:48,960
就好像我们的毛巾,

136
00:06:48,960 --> 00:06:50,560
把毛巾卷起来,

137
00:06:50,560 --> 00:06:52,680
然后变成一个新的概念,

138
00:06:52,680 --> 00:06:54,240
或者一种新的形态一样,

139
00:06:54,240 --> 00:06:57,080
这种就是在网上比较通俗,

140
00:06:57,080 --> 00:06:59,320
或者比较容易理解的一种概念,

141
00:06:59,320 --> 00:07:01,360
现在讲不清明白没关系,

142
00:07:01,360 --> 00:07:02,720
大家可以不用看这个视频,

143
00:07:02,720 --> 00:07:05,200
这个视频其实确实没什幺太多的内容,

144
00:07:05,200 --> 00:07:09,120
我们可以看到现在很多时候我们会把积分,

145
00:07:09,120 --> 00:07:13,360
积分是对连续的数据或者无穷的数据进行一个计算的,

146
00:07:13,360 --> 00:07:15,440
这个就是信号处理当中卷积,

147
00:07:15,440 --> 00:07:19,640
把我们的连续变成一个离散的形式的表示,

148
00:07:19,640 --> 00:07:21,520
现在我们把这条公式,

149
00:07:21,520 --> 00:07:23,480
再拓展到二维的空间,

150
00:07:23,480 --> 00:07:26,000
就得到了我们神经网络里面的卷积,

151
00:07:26,000 --> 00:07:28,280
我们可以看到这里面的公式非常多,

152
00:07:28,280 --> 00:07:30,720
关于里面的参数量的非常多,

153
00:07:30,720 --> 00:07:32,040
我们逐个来打开一下,

154
00:07:32,040 --> 00:07:35,040
首先S就是我们的卷积核,

155
00:07:35,040 --> 00:07:37,040
或者我们的卷积最后的输出,

156
00:07:37,080 --> 00:07:39,360
而I就是我们的卷积的输入

157
00:07:39,360 --> 00:07:41,800
假设你当它作为一张图片就行了,

158
00:07:41,800 --> 00:07:44,240
而K就是我们的卷积核,

159
00:07:44,240 --> 00:07:46,840
右边就是具体的计算公式,

160
00:07:46,840 --> 00:07:48,640
我们再往下看一看,

161
00:07:48,640 --> 00:07:52,960
这里面有一个非常好的一个可视化的网站,

162
00:07:52,960 --> 00:07:54,720
我们打开去看看,

163
00:07:56,360 --> 00:08:00,320
这个就是GitHub里面一个非常好的一个可视化的方式,

164
00:08:00,320 --> 00:08:04,360
我们可以看到卷积形式方法非常多,

165
00:08:04,360 --> 00:08:05,960
我们有标准的卷积,

166
00:08:05,960 --> 00:08:07,120
有反卷积,

167
00:08:07,120 --> 00:08:08,560
有分组卷积,

168
00:08:08,560 --> 00:08:09,760
有可分离卷积,

169
00:08:09,760 --> 00:08:12,560
还有分组卷积的方式非常多,

170
00:08:12,560 --> 00:08:16,600
下面蓝色的比较深颜色的就是我们的卷积核,

171
00:08:16,600 --> 00:08:20,480
而下面底的蓝色就是我们的输入的图片,

172
00:08:20,480 --> 00:08:23,480
卷积核跟图片进行滑窗之后,

173
00:08:23,480 --> 00:08:25,520
就得到一个新的输出的结果,

174
00:08:25,520 --> 00:08:26,720
而新的输出的结果,

175
00:08:26,720 --> 00:08:31,280
这就是上面我们绿色的对应于我们公式里面的S,

176
00:08:31,280 --> 00:08:33,480
这个就是卷积的具体的方式,

177
00:08:33,520 --> 00:08:38,200
我们其实卷积更多的一开始没有应用到神经网络里面,

178
00:08:38,200 --> 00:08:41,480
在图像处理的时候用的特别的多,

179
00:08:41,480 --> 00:08:45,360
下面这个就是我们整体的卷积的一个计算,

180
00:08:45,360 --> 00:08:47,960
这个就是我们的卷积的计算核,

181
00:08:47,960 --> 00:08:49,920
这个就是我们的图片,

182
00:08:49,920 --> 00:08:52,720
图片跟卷积核进行一个叉乘

183
00:08:52,720 --> 00:08:56,120
就得到我们的最终的结果-3,

184
00:08:56,120 --> 00:08:58,520
具体的公式就比较简单,

185
00:08:58,520 --> 00:09:03,440
这里面的卷积和每一个元素可以看到-1、0、1

186
00:09:03,440 --> 00:09:09,160
-1、0、1，然后跟上面的每一个元素进行相乘,

187
00:09:09,160 --> 00:09:12,560
-1乘以3，0乘以0，1乘以1

188
00:09:12,560 --> 00:09:15,000
然后再把它进行求和,

189
00:09:15,000 --> 00:09:17,000
跟我们刚才的公式是一模一样的,

190
00:09:17,000 --> 00:09:20,200
最终得到我们-3这个结果,

191
00:09:20,200 --> 00:09:22,240
这个就是卷积的计算,

192
00:09:22,240 --> 00:09:25,240
既然提到卷积在图像处理里面用的非常多,

193
00:09:25,240 --> 00:09:26,880
我们现在打开PhotoShop,

194
00:09:26,880 --> 00:09:30,600
看一下卷积在PhotoShop里面最常用的一些功能,

195
00:09:31,120 --> 00:09:33,560
假设我们现在有这么一张水彩画,

196
00:09:33,560 --> 00:09:35,160
然后点击滤镜,

197
00:09:35,160 --> 00:09:37,480
滤镜里面的模糊,

198
00:09:37,480 --> 00:09:39,520
对，模糊里面用的非常多,

199
00:09:39,520 --> 00:09:41,080
首先就是高斯模糊,

200
00:09:41,080 --> 00:09:45,440
高斯模糊就是我们的卷积核是符合高斯的分布,

201
00:09:45,440 --> 00:09:49,680
而这里面平均的半径就是我们的卷积核的大小,

202
00:09:49,680 --> 00:09:51,880
可以看到通过这里面的空字拖动,

203
00:09:51,880 --> 00:09:54,200
我们可以控制我们的卷积核的大小,

204
00:09:54,200 --> 00:09:56,560
卷积核的大小作用于整个图片,

205
00:09:56,560 --> 00:09:59,480
就使得我们的图片越来越模糊,

206
00:09:59,480 --> 00:10:01,520
像里面的很多滤镜的方式,

207
00:10:01,520 --> 00:10:03,920
就使用了不同的卷积的组成方式,

208
00:10:03,920 --> 00:10:06,840
去对我们的图片产生作用的,

209
00:10:06,840 --> 00:10:09,640
好不容易终于讲完了自己不熟悉的,

210
00:10:09,640 --> 00:10:12,280
或者不是说非常理解的概念给大家,

211
00:10:12,280 --> 00:10:14,040
说实话实在惭愧,

212
00:10:14,040 --> 00:10:16,680
下面又回到了我最熟悉的概念,

213
00:10:16,680 --> 00:10:18,240
还是系统的优化,

214
00:10:18,240 --> 00:10:19,840
还有算法的优化,

215
00:10:19,840 --> 00:10:22,120
也没有办法给大家讲得很透彻,

216
00:10:22,120 --> 00:10:23,680
所以希望大家能够谅解,

217
00:10:23,680 --> 00:10:26,160
简单的听一听或者不要听就行了,

218
00:10:26,280 --> 00:10:29,720
下面我们看一下卷积对于 Tensor 的一个优化,

219
00:10:29,720 --> 00:10:31,240
首先可以看到了张量,

220
00:10:31,240 --> 00:10:32,640
这个 Tensor 就我们的张量,

221
00:10:32,640 --> 00:10:33,880
张量在内存里面,

222
00:10:33,880 --> 00:10:39,080
一般我们现在假设以 NHWC 这种方式作为一个布局,

223
00:10:39,080 --> 00:10:42,120
可以看到下面这一扎公式,

224
00:10:42,120 --> 00:10:45,720
就是卷积对于张量的一种运算,

225
00:10:45,720 --> 00:10:48,960
我们可以看到这边有非常多的 for,

226
00:10:48,960 --> 00:10:52,320
我们逐个的看看这些 for 有什幺作用,

227
00:10:52,320 --> 00:10:55,480
首先我们有外层三层的 for,

228
00:10:55,520 --> 00:10:58,120
里面又有三层的 for,

229
00:10:58,120 --> 00:11:01,960
外面的三层的 for 就是对于我们 nhw 里面的 h,

230
00:11:01,960 --> 00:11:04,800
我们对于 h 的这个信道进行遍历

231
00:11:04,800 --> 00:11:06,840
然后对于 w 这个信道进行遍历

232
00:11:06,840 --> 00:11:10,160
最后对于我们的 c 的信道进行遍历

233
00:11:10,160 --> 00:11:12,920
最后才拿到我们第一个数据 c,

234
00:11:12,920 --> 00:11:14,400
0h 0w 0c,

235
00:11:14,400 --> 00:11:15,840
然后把它赋一个值,

236
00:11:15,840 --> 00:11:18,440
接着我们把这个值,

237
00:11:18,440 --> 00:11:19,960
因为我们拿到内存的地址,

238
00:11:19,960 --> 00:11:21,760
内存的地址里面存的是什幺字,

239
00:11:21,760 --> 00:11:22,720
我们是不确定的,

240
00:11:22,720 --> 00:11:24,640
所以我们首先对它进行赋值

241
00:11:24,640 --> 00:11:27,280
然后有三个内层的 for,

242
00:11:27,280 --> 00:11:30,840
三个内层的 for 就是我们的卷积核了,

243
00:11:30,840 --> 00:11:33,040
这个卷积核就是 kernel 的 h,

244
00:11:33,040 --> 00:11:34,160
kernel 的 w,

245
00:11:34,160 --> 00:11:36,360
还有 input channel,

246
00:11:36,360 --> 00:11:39,040
通过这种方式对它进行一个组织,

247
00:11:39,040 --> 00:11:44,000
然后就是我们刚才的那条求和公式里面的进行一个计算,

248
00:11:44,000 --> 00:11:50,680
具体就是这里面卷积核跟这个原始的图片进行加权求和

249
00:11:50,680 --> 00:11:52,200
加权求和之后,

250
00:11:52,240 --> 00:11:56,360
就给我们的 c[0h][0w][0c]进行计算, 

251
00:11:56,360 --> 00:12:01,080
可以看到一个简单的卷积的数学计算是非常的复杂,

252
00:12:01,080 --> 00:12:03,040
有非常多的嵌套的循环,

253
00:12:03,040 --> 00:12:06,960
为了去让我们的整个嵌套的循环没有那么深,

254
00:12:06,960 --> 00:12:10,080
我们会做很多的循环的优化,

255
00:12:10,080 --> 00:12:13,440
循环的展开、分块、重排、融合、拆分

256
00:12:13,440 --> 00:12:15,280
那这些所有的概念,

257
00:12:15,280 --> 00:12:19,920
我们在之前 AI 编译器里面其实是讲过的,

258
00:12:19,960 --> 00:12:21,960
在我们的 AI 编译器后端优化,

259
00:12:21,960 --> 00:12:23,080
就算子的优化,

260
00:12:23,080 --> 00:12:26,680
循环的优化里面给大家详细的去介绍过,

261
00:12:26,680 --> 00:12:28,720
但是这里面是推理引擎的概念,

262
00:12:28,920 --> 00:12:32,520
所以我们不需要通过编译器或者不需要引入编译器的概念,

263
00:12:32,520 --> 00:12:34,560
直接用人工的方式,

264
00:12:34,560 --> 00:12:37,960
用手排的方式对循环进行展开,

265
00:12:37,960 --> 00:12:40,520
这也是Kernel优化工程师所做的工作,

266
00:12:40,520 --> 00:12:42,480
另外我们还有指令的优化,

267
00:12:42,480 --> 00:12:44,480
对我们的数据进行向量化,

268
00:12:44,480 --> 00:12:47,120
对我们的数据进行张量化,

269
00:12:47,120 --> 00:12:49,200
这些概念也是比较好理解的,

270
00:12:49,240 --> 00:12:52,960
我们现在假设 C 只有四个通道,

271
00:12:52,960 --> 00:12:57,040
这个时候我们就可以完全把它进行一个向量化的操作,

272
00:12:57,040 --> 00:13:01,680
不用每次都执行四次相关的累加操作,

273
00:13:01,680 --> 00:13:04,320
下面我们最后还有存储的优化,

274
00:13:04,320 --> 00:13:07,120
存储优化主要是包括我们的访存的延迟,

275
00:13:07,120 --> 00:13:09,000
还有存储的分配,

276
00:13:09,000 --> 00:13:11,520
可以往上面这一坨公式里面看到

277
00:13:11,520 --> 00:13:15,440
我们这里面其实有大量的访问内存的方式,

278
00:13:15,440 --> 00:13:18,120
怎幺对它进行优化是一个很大的概念,

279
00:13:18,120 --> 00:13:22,280
这些我们写 kernel 的工程师非常之在行的,

280
00:13:22,280 --> 00:13:23,800
那更多相关的操作,

281
00:13:23,800 --> 00:13:26,760
更多相关的原理也可以去到我之前讲到的

282
00:13:26,760 --> 00:13:29,520
AI编译器后端优化里面的相关的内容,

283
00:13:29,520 --> 00:13:33,120
这里面就不会介绍太多相关的原理知识,

284
00:13:33,120 --> 00:13:35,440
今天的内容确实太不专业了,

285
00:13:35,440 --> 00:13:36,240
就到这里为止,

286
00:13:36,240 --> 00:13:36,840
谢谢各位,

287
00:13:36,840 --> 00:13:37,600
拜了个拜!

288
00:13:38,520 --> 00:13:39,320
卷的不行了,

289
00:13:39,320 --> 00:13:40,160
卷的不行了,

290
00:13:40,160 --> 00:13:42,000
记得一键三连加关注哦,

291
00:13:42,000 --> 00:13:45,240
所有的内容都会开源在下面这条链接里面,

292
00:13:45,240 --> 00:13:46,520
拜拜!

