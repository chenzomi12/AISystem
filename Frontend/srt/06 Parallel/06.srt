1
00:00:00,140 --> 00:00:04,700
字幕生成: 粟君杰 字幕校对: 粟君杰
2
00:00:04,820 --> 00:00:05,870
嗨，大家好
3
00:00:05,870 --> 00:00:06,980
我是ZOMI
3
00:00:06,980 --> 00:00:09,560
今天我们迎来了一个新的内容
4
00:00:09,860 --> 00:00:11,720
混合并行
5
00:00:11,720 --> 00:00:13,000
我们可以看到
6
00:00:13,000 --> 00:00:15,670
其实深度学习迎来了大模型之后
7
00:00:15,670 --> 00:00:18,490
从2016年或者17年的时候
8
00:00:18,490 --> 00:00:20,270
出现Transformer之后呢
9
00:00:20,270 --> 00:00:24,810
整个网络模型的参数量形成了一个断代的局面
10
00:00:25,570 --> 00:00:27,540
想要大模型训练起来
11
00:00:27,540 --> 00:00:30,440
不仅仅需要大量的ai集群或者服务器
12
00:00:30,440 --> 00:00:32,960
我们还需要很多分布式训练的策略
13
00:00:32,960 --> 00:00:34,400
或者一些算法
14
00:00:34,400 --> 00:00:36,020
我们今天的内容呢
15
00:00:36,020 --> 00:00:39,300
主要是来分享一下大模型的混合并行
16
00:00:39,300 --> 00:00:41,560
那混合并行呢，主要是混合
17
00:00:41,560 --> 00:00:46,120
我们在上一节的时候去介绍的模型并行
18
00:00:46,120 --> 00:00:46,780
张量并行
19
00:00:46,780 --> 00:00:47,380
数据并行
20
00:00:47,380 --> 00:00:48,520
还有Pipeline并行
21
00:00:48,520 --> 00:00:51,300
把这几种并行的方式混合起来
22
00:00:51,300 --> 00:00:53,820
而在我们的算法结构里面呢
23
00:00:53,820 --> 00:00:55,560
又有两种最著名的算法
24
00:00:55,560 --> 00:00:57,720
第一种呢，就是推荐大模型
25
00:00:57,720 --> 00:01:00,140
主要以Embending层为主的
26
00:01:00,140 --> 00:01:04,910
另外一种呢，就是LLM large language model
27
00:01:04,910 --> 00:01:06,880
大规模语言模型
28
00:01:06,880 --> 00:01:10,390
今天我们来分别介绍两种模型
29
00:01:10,390 --> 00:01:11,680
推荐模型呢
30
00:01:11,680 --> 00:01:17,900
我们以2019年facebook发表的deep learning recommendations model
31
00:01:17,900 --> 00:01:21,160
DLRM作为一个经典的例子
32
00:01:21,160 --> 00:01:24,620
去讲讲混合并行在这里面是怎么去实现的
33
00:01:24,620 --> 00:01:28,700
在正式进入到推荐大模型的混合并行的策略
34
00:01:28,700 --> 00:01:29,350
之前呢
35
00:01:29,350 --> 00:01:32,320
我们回顾一下CTR网络模型
36
00:01:32,320 --> 00:01:34,820
CTR呢，就是Click-Through-Read
37
00:01:34,820 --> 00:01:37,670
中文呢，叫做点击率预估
38
00:01:37,670 --> 00:01:38,900
简单的来说呢
39
00:01:38,900 --> 00:01:41,150
就是我有一系列的用户的信息
40
00:01:41,150 --> 00:01:43,860
还有一些物品的信息
41
00:01:43,860 --> 00:01:47,820
然后去预测用户去点击这个物品
42
00:01:47,820 --> 00:01:49,620
购买的一个可能性
43
00:01:49,620 --> 00:01:52,500
所以叫做点击率预估的模型
44
00:01:52,500 --> 00:01:55,800
点击率预估模型呢，有两个很重要的feature
45
00:01:55,800 --> 00:01:56,840
就是我们的特征
46
00:01:56,840 --> 00:01:59,560
第一个特征我们叫做连续性的特征
47
00:01:59,560 --> 00:02:01,840
主要是讲一些用户本身的特征
48
00:02:01,840 --> 00:02:03,100
包括用户的年龄啊
49
00:02:03,100 --> 00:02:03,760
性别啊
50
00:02:03,760 --> 00:02:06,820
还有用户历史的购买的一些记录
51
00:02:06,820 --> 00:02:09,980
都另外一种特征呢，我们叫做Category Feature
52
00:02:09,980 --> 00:02:12,380
就是在排序的物品的特征
53
00:02:12,380 --> 00:02:14,680
那物品的特征就会非常的丰富了
54
00:02:14,680 --> 00:02:16,180
不仅仅包括物品的ID
55
00:02:16,180 --> 00:02:18,760
还有物品的非常丰富的图片的信息
56
00:02:18,760 --> 00:02:19,690
文本的信息
57
00:02:19,690 --> 00:02:22,390
还有物品历史被点击过的次数
58
00:02:22,390 --> 00:02:24,600
物品被购买的记录的信息
59
00:02:24,600 --> 00:02:28,020
哪些用户喜欢这些物品相关的所有的信息
60
00:02:28,020 --> 00:02:32,010
所以我们可以看到主要有两个信息feature去组成
61
00:02:32,010 --> 00:02:36,380
回到facebook的这篇DLRM的网络模型里面呢
62
00:02:36,380 --> 00:02:39,680
我们的网络模型结构呢，就如图里面所示
63
00:02:39,680 --> 00:02:42,560
首先我们用户的一些连续性的特征呢
64
00:02:42,560 --> 00:02:43,860
叫做Numerical Features
65
00:02:43,860 --> 00:02:47,520
我们用灰色来代表连续性的这些特征呢
66
00:02:47,520 --> 00:02:50,180
用mp网络模型作为一个表示的
67
00:02:50,180 --> 00:02:54,110
而一些物体的Category的特征呢可能会非常的多
68
00:02:54,110 --> 00:02:56,400
而且非常大量的习俗化
69
00:02:56,400 --> 00:02:59,460
所以文章当中呢，就使用Embeding层作为表征
70
00:02:59,460 --> 00:03:02,840
然后再通过点成把刚才的用户特征
71
00:03:02,840 --> 00:03:05,220
还有Category的特征合起来
72
00:03:05,220 --> 00:03:07,380
然后再给我们的MVP网络模型
73
00:03:07,380 --> 00:03:09,180
做一个点击率的预估
74
00:03:09,180 --> 00:03:11,680
就是Click Possibility
75
00:03:11,680 --> 00:03:14,560
回顾分布式训练里面的张量并行
76
00:03:14,560 --> 00:03:16,800
对于Embeding层呢，有两种
77
00:03:16,800 --> 00:03:18,840
一种是Table-wise的切换方式
78
00:03:18,840 --> 00:03:20,940
一种是Colum-wise的切换方式
79
00:03:20,940 --> 00:03:22,770
那Table-wise的切分方式呢
80
00:03:22,770 --> 00:03:25,260
主要是以目录特征作为一个切分
81
00:03:25,260 --> 00:03:28,000
我把不同的特征切分到不同的机器
82
00:03:28,000 --> 00:03:32,050
而Colum-wise呢就是我把所有的特征都合成一个大表
83
00:03:32,050 --> 00:03:34,860
然后每个特征呢，按列的进行切分
84
00:03:34,860 --> 00:03:38,400
例如特征0的会切分0~63列
85
00:03:38,400 --> 00:03:39,300
在Gpu 0
86
00:03:39,300 --> 00:03:42,720
然后把剩下的64~127列的切分
87
00:03:42,720 --> 00:03:44,720
在我们的Gpu 1
88
00:03:44,720 --> 00:03:47,510
当我的Category Future的时候非常的大
89
00:03:47,510 --> 00:03:51,560
可能一个Embeding层不能够完全的放在Gpu 0里面
90
00:03:51,560 --> 00:03:54,890
所以呢Table-wise的这种方式呢是不推荐采用的
91
00:03:54,890 --> 00:03:57,980
而更多的是采用Colum-wise的这种方式
92
00:03:57,980 --> 00:04:01,190
在facebook DLRM这篇文章里面呢
93
00:04:01,190 --> 00:04:05,600
就采用了刚才我们介绍的Colum-wise的切分方式
94
00:04:05,600 --> 00:04:08,240
首先第一个做的就是数据的并行
95
00:04:08,240 --> 00:04:11,780
把GPU 0，GPU 1都存放不同的category
96
00:04:11,780 --> 00:04:13,660
或者用户的目录的数据
97
00:04:13,660 --> 00:04:16,420
然后呢，去输给我们的网络模型
98
00:04:16,420 --> 00:04:17,339
第一层
99
00:04:17,339 --> 00:04:20,189
那Embedding呢，就会做一个模型的并行
100
00:04:20,189 --> 00:04:22,720
就是我们谈到的张量并行
101
00:04:22,720 --> 00:04:24,400
Colum-wise的切换方式
102
00:04:24,400 --> 00:04:27,440
接着呢，实现一个All-to-All的通讯
103
00:04:27,440 --> 00:04:29,780
把Embedding后的所有的数据
104
00:04:29,780 --> 00:04:32,020
同步到所有的机器上面
105
00:04:32,020 --> 00:04:34,820
接着呢，再进行一个数据的并行
106
00:04:34,820 --> 00:04:36,740
那这里面的数据的并行呢
107
00:04:36,740 --> 00:04:38,630
就不是训练的数据的并行
108
00:04:38,630 --> 00:04:43,700
而是我们网络模型参数里面的权重优化器啊
109
00:04:43,700 --> 00:04:44,780
激活输出啊
110
00:04:44,780 --> 00:04:49,660
这些网络模型当中衍生的数据的一个并行
111
00:04:49,660 --> 00:04:52,600
现在我们有n个GPU，GPU 0里面呢
112
00:04:52,600 --> 00:04:53,980
我们把Category Feature 0
113
00:04:53,980 --> 00:04:58,520
还有Category Feature 2拿到一部分的batch数据之后呢
114
00:04:58,520 --> 00:05:00,500
去传给我们的Model Parallel来
115
00:05:00,500 --> 00:05:03,440
GPU N里面有会存放我们的Category N
116
00:05:03,440 --> 00:05:07,370
同时我还会存放Category Future 2的一些数据
117
00:05:07,370 --> 00:05:08,780
那这里面呢，有一个差异
118
00:05:08,780 --> 00:05:11,770
就是Category 0只存放在GPU 0
119
00:05:11,770 --> 00:05:14,020
Category N存放在GPU N
120
00:05:14,020 --> 00:05:17,080
是因为有可能这台GPU里面的内存
121
00:05:17,080 --> 00:05:19,090
已经放不下太多的特征了
122
00:05:19,090 --> 00:05:21,760
所以这些特征呢，有部分是重叠
123
00:05:21,760 --> 00:05:24,160
只有这台服务器去拥有的
124
00:05:24,160 --> 00:05:27,040
这时候呢，把数据就传给我们的Embedding层
125
00:05:27,040 --> 00:05:29,380
Embedding层呢，就像我们刚才所说的
126
00:05:29,380 --> 00:05:32,230
采用的一个Colum-Wise的这种切分方式
127
00:05:32,230 --> 00:05:35,100
把不同的列切分到不同的机器
128
00:05:36,420 --> 00:05:39,140
最后的输出呢，去使用通讯集合
129
00:05:39,140 --> 00:05:41,420
All-to-All的方式进行通讯
130
00:05:41,420 --> 00:05:44,560
可以看到GPU 0对于网络模型的输入
131
00:05:44,560 --> 00:05:46,900
有了所有目录的Embedding的特征
132
00:05:46,900 --> 00:05:49,780
包括我们刚才的第0个Embedding的特征
133
00:05:49,780 --> 00:05:52,900
第n个特征和第二个Embedding的特征
134
00:05:52,900 --> 00:05:58,140
同理GPU N有了所有目录的Embedding的特征
135
00:05:58,140 --> 00:06:02,280
然后呢，再作为真正的神经网络MLP层做一个输入
136
00:06:02,280 --> 00:06:05,320
然后去做一个点击率的预估
137
00:06:05,320 --> 00:06:06,880
那这里面呢，刚才说了
138
00:06:06,880 --> 00:06:11,170
我们会做一个Data Parallelism数据的并行
139
00:06:11,170 --> 00:06:13,870
把我们的网络模型的权重参数
140
00:06:13,870 --> 00:06:15,920
网络模型的激活的输出
141
00:06:15,920 --> 00:06:20,040
还有网络模型的优化器的状态都进行一个并行
142
00:06:20,040 --> 00:06:22,770
这里面呢，ZOMI就想提出一个问题
143
00:06:22,770 --> 00:06:24,750
为什么我们需要强调
144
00:06:24,750 --> 00:06:29,000
需要对Embedding这一层单独的去做模型并行
145
00:06:29,000 --> 00:06:30,350
或者张量并行呢
146
00:06:30,350 --> 00:06:34,500
而不是去强调我后面的网络做一个并行呢
147
00:06:35,260 --> 00:06:39,660
其实这个问题呢，是跟推荐网络模型是相关的
148
00:06:39,660 --> 00:06:44,070
由于Embedding层在深度学习推荐模型当中呢
149
00:06:44,070 --> 00:06:46,220
起着非常重要的地位
150
00:06:46,220 --> 00:06:49,520
一般呢，它们用于将我们的输入的用户的数据
151
00:06:49,520 --> 00:06:53,100
或者离散的特征映射到一些高维的向量
152
00:06:53,100 --> 00:06:55,900
以便于后面我们的神经网络去处理
153
00:06:55,900 --> 00:06:56,980
而Embedding层呢
154
00:06:56,980 --> 00:06:59,890
通常是构成我们深度学习推荐模型的
155
00:06:59,890 --> 00:07:01,340
大部分的参数
156
00:07:01,340 --> 00:07:05,030
可能一个Embedding长就可以达到了TB的级别
157
00:07:05,030 --> 00:07:06,920
所以在训练的期间
158
00:07:06,920 --> 00:07:10,070
很难把它们放在单个GPU的内存
159
00:07:10,070 --> 00:07:14,610
所以呢，现代的推荐系统或者深度学习推荐系统
160
00:07:14,610 --> 00:07:15,600
一般来说
161
00:07:15,600 --> 00:07:19,840
需要模型并行和数据并行两种方式进行组合
162
00:07:19,840 --> 00:07:23,110
才能够更好地去训练我们的网络模型
163
00:07:23,110 --> 00:07:26,340
再往下呢，这个图就会更加复杂一点
164
00:07:26,340 --> 00:07:27,300
这些用户的特征
165
00:07:27,300 --> 00:07:30,780
连续的特征呢，使用的是Data Parallelism数据并行
166
00:07:30,780 --> 00:07:33,300
而用户的这些一般连续性的特征呢
167
00:07:33,300 --> 00:07:35,360
使用MLP层
168
00:07:35,360 --> 00:07:39,140
MLP层对权重参数有反向的梯度进行并行
169
00:07:39,840 --> 00:07:42,560
最后就是DLRM的实验结果
170
00:07:42,560 --> 00:07:44,420
硬件呢，用了两个AMD
171
00:07:44,420 --> 00:07:45,740
还有A100-80G
172
00:07:45,740 --> 00:07:49,220
还有DGXA100 三种硬件进行对比
173
00:07:49,220 --> 00:07:52,460
而在没有进行任何并行的加速的时候呢
174
00:07:52,460 --> 00:07:57,490
可能效果是每秒钟只能处理17.7k的数据
175
00:07:57,490 --> 00:08:01,030
而经过我们刚才的hybrid parallelism
176
00:08:01,030 --> 00:08:04,240
把数据并行和模型并行混合在一起
177
00:08:04,240 --> 00:08:07,840
能够得到一个接近700倍的性能的提升
178
00:08:07,840 --> 00:08:10,450
所以混合并行还是非常有用的
179
00:08:10,450 --> 00:08:14,480
可接下来呢我们以Megatron-LM大规模语言模型
180
00:08:14,480 --> 00:08:15,140
作为一个例子
181
00:08:15,140 --> 00:08:18,020
去讲讲如何做一个混合并行
182
00:08:18,020 --> 00:08:19,520
这次的混合并行呢
183
00:08:19,520 --> 00:08:22,020
比刚才的推荐模型
184
00:08:22,020 --> 00:08:25,440
使用数据并行和模型并行更复杂一点
185
00:08:25,440 --> 00:08:28,670
它融合了流水线并行的方式
186
00:08:28,670 --> 00:08:32,180
下面这个图呢，我们简单的去理解一下
187
00:08:32,180 --> 00:08:34,340
一个大规模语言模型里面呢
188
00:08:34,340 --> 00:08:35,840
主要每一层绿色的
189
00:08:35,840 --> 00:08:38,180
就是代表一个Transformer的结构
190
00:08:38,180 --> 00:08:41,690
可以看到Transformer的结构层数会非常的多
191
00:08:41,690 --> 00:08:44,780
通过大量的Transformer进行一个堆叠
192
00:08:45,420 --> 00:08:47,840
对于大量的Transformer的网络模型结构
193
00:08:47,840 --> 00:08:49,900
我们想到的一种最好的方式
194
00:08:49,900 --> 00:08:52,150
就是对它进行一个流水线并行
195
00:08:52,150 --> 00:08:55,880
把不同的Transformer放在不同的stage里面
196
00:08:55,880 --> 00:08:59,120
然后再给不同的机器进行一个计算
197
00:08:59,120 --> 00:09:00,080
GPU 1
198
00:09:00,080 --> 00:09:04,200
这个stage呢，就拥有了两个全收网的网络模型结构
199
00:09:04,200 --> 00:09:06,840
然后再给我们的GPU 2进行计算
200
00:09:06,840 --> 00:09:08,340
通过这种方式呢
201
00:09:08,340 --> 00:09:10,300
我们叫做流水线并行
202
00:09:10,300 --> 00:09:10,780
当然了
203
00:09:10,780 --> 00:09:13,120
我们今天要讲的就是Megatron-LM
204
00:09:13,120 --> 00:09:16,740
在2022年去发表的最新的文章
205
00:09:16,740 --> 00:09:19,620
Efficient Large-Scale Language
206
00:09:19,620 --> 00:09:22,859
Model Training on GPU Cluster Using Megatron-LM
207
00:09:22,859 --> 00:09:25,709
是发表了两篇文章的
208
00:09:25,709 --> 00:09:27,980
第一篇文章主要是讲
209
00:09:28,680 --> 00:09:32,060
对于Transformer结构如何进行并行
210
00:09:32,060 --> 00:09:36,170
第二篇文章呢，就是讲混合并行加上流水线并行
211
00:09:36,170 --> 00:09:38,060
如何做加速的
212
00:09:38,100 --> 00:09:40,340
所以回到这个图里面呢
213
00:09:40,340 --> 00:09:41,420
我们可以看到
214
00:09:41,420 --> 00:09:43,280
除了利用不同的机器呢
215
00:09:43,280 --> 00:09:46,310
进行一个流水线并行之外呢
216
00:09:46,310 --> 00:09:48,260
每一层Transformer之间呢
217
00:09:48,260 --> 00:09:50,740
还会进行一个张量的并行
218
00:09:50,740 --> 00:09:52,420
把我们的网络模型
219
00:09:52,420 --> 00:09:56,580
把我们的Transformer的结构进行一个纵向的切分
220
00:09:56,580 --> 00:10:00,620
现在我们来回顾一下张量并行的一个概念
221
00:10:00,620 --> 00:10:03,200
在Transformer的MLP层里面呢
222
00:10:03,200 --> 00:10:06,230
我们对权重A呢，进行一个列的切分
223
00:10:06,230 --> 00:10:09,200
我们对权重B呢，进行一个行的切分
224
00:10:09,200 --> 00:10:12,080
使得我们的网络模型的输入输出
225
00:10:12,080 --> 00:10:15,230
只需要经过两次通讯就可以了
226
00:10:18,140 --> 00:10:20,400
第一次呢，就是对X进行一个广播，split到不同的机器
227
00:10:20,400 --> 00:10:21,960
最后计算完之后呢
228
00:10:21,960 --> 00:10:24,740
通过g执行一次All-Reduce的通讯
229
00:10:24,740 --> 00:10:27,710
就完成了一个MLP层的张量并行
230
00:10:27,710 --> 00:10:31,270
同理在Self-Attention层也是这种方式
231
00:10:31,270 --> 00:10:35,100
我的q k v作为权重来进行一个列的切分
232
00:10:35,100 --> 00:10:37,380
B这个权重呢，进行一个行的切分
233
00:10:37,380 --> 00:10:40,110
f进行一次split的通讯
234
00:10:40,110 --> 00:10:42,940
而g呢，执行一次All-Gather的通讯
235
00:10:42,940 --> 00:10:46,480
这就完成了Self-Attention层的张量并行
236
00:10:46,480 --> 00:10:49,960
那接下来的内容呢可能会稍微复杂一点
237
00:10:49,960 --> 00:10:53,290
上面的这个图呢，就是张量并行
238
00:10:53,290 --> 00:10:54,740
tensor parallelism
239
00:10:54,740 --> 00:10:57,680
但同时呢，引入了一个流水线的并行
240
00:10:57,680 --> 00:10:59,480
这是Transformer1的结构
241
00:10:59,480 --> 00:11:01,520
这是第二层Transformer的结构
242
00:11:01,520 --> 00:11:04,960
不同的Transformer放在不同的机器里面
243
00:11:04,960 --> 00:11:06,760
在流水线并行的时候呢
244
00:11:06,760 --> 00:11:08,770
采用的是GPipline的方式
245
00:11:08,770 --> 00:11:11,200
GPipline呢，我们上一节里面已经讲过了
246
00:11:11,200 --> 00:11:12,680
我们简单的去回忆一下
247
00:11:12,680 --> 00:11:14,180
GPipline最重要的特征
248
00:11:14,180 --> 00:11:17,760
就是把我们的batch呢分成很多个Micro-Batch
249
00:11:17,760 --> 00:11:21,510
所以每个Device上面都有非常多的Micro-Batch
250
00:11:21,510 --> 00:11:22,650
从1~8
251
00:11:22,650 --> 00:11:26,720
然后再进行反向的时候会做一个重计算
252
00:11:26,720 --> 00:11:28,550
所以反向的计算的时间
253
00:11:28,550 --> 00:11:31,080
会比正向的计算的时间要多一倍
254
00:11:31,080 --> 00:11:31,800
这样做呢
255
00:11:31,800 --> 00:11:34,950
就是为了极大的去减少我们的动态的内存
256
00:11:34,950 --> 00:11:37,260
以计算去换我们的空间
257
00:11:37,260 --> 00:11:39,360
计算完一次反向之后呢
258
00:11:39,360 --> 00:11:41,360
在这里面进行一个同步
259
00:11:41,360 --> 00:11:42,380
同步完之后呢
260
00:11:42,380 --> 00:11:45,440
再往下进行下一次Batch的计算
261
00:11:45,440 --> 00:11:48,180
这篇文章了还没有这么简单
262
00:11:48,180 --> 00:11:51,570
那上面呢，就是PipeDream的具体的实现方式
263
00:11:51,570 --> 00:11:55,500
PipeDream最主要的特征就是引入了权重隐藏
264
00:11:55,500 --> 00:11:56,550
weight shading
265
00:11:56,550 --> 00:11:58,140
在稳定状态的时候
266
00:11:58,140 --> 00:12:00,300
每次进行一个前向计算的时候
267
00:12:00,300 --> 00:12:04,230
使用的是距离它最近最新的一个反向的计算
268
00:12:04,230 --> 00:12:07,880
同时去记录里面正向权重的版本维护
269
00:12:07,880 --> 00:12:09,980
一个权重的生命周期
270
00:12:12,200 --> 00:12:16,840
用的是5正向时候算出来的权重梯度
271
00:12:16,840 --> 00:12:18,500
Megatron-LM这篇文章里面呢
272
00:12:18,500 --> 00:12:22,040
对pipeline并行又提出了一个新的并行方式
273
00:12:22,040 --> 00:12:23,750
叫做virtual pipeline
274
00:12:23,750 --> 00:12:25,250
虚拟的pipeline
275
00:12:25,250 --> 00:12:29,210
我们在下面这个图呢，看一下几个重要的特征
276
00:12:29,210 --> 00:12:30,830
第一个呢，就是Bubble的时间
277
00:12:30,830 --> 00:12:34,590
进一步的比刚才上面的PipeDream要缩短了
278
00:12:34,590 --> 00:12:37,890
第二个点就是Mini-Batch的1 2 3是更小了
279
00:12:37,890 --> 00:12:41,050
但是出现了一个浅蓝色的1 2 3 4
280
00:12:41,050 --> 00:12:43,060
同理在反向的时候呢
281
00:12:43,060 --> 00:12:46,120
我除了有浅绿色的1 2 3 4之外呢
282
00:12:46,120 --> 00:12:48,700
我还有深绿色的1 2 3 4
283
00:12:48,700 --> 00:12:52,110
那这几个浅色和深色的有什么区别呢
284
00:12:52,110 --> 00:12:54,870
我们以下面这个图作为例子
285
00:12:54,870 --> 00:12:57,390
左边的这个图呢，就是刚才讲到的
286
00:12:57,390 --> 00:12:59,700
对Transformer进行不断的堆叠
287
00:12:59,700 --> 00:13:01,860
形成的语言大模型结构
288
00:13:01,860 --> 00:13:05,520
那中间的这个方式呢就是GPipe或者PipeDream
289
00:13:05,520 --> 00:13:08,340
在一个设备上面的放置多个box
290
00:13:08,340 --> 00:13:10,260
可以看到这里面呢，有两个box
291
00:13:10,260 --> 00:13:12,880
两个传送门层放在GPU 0里面
292
00:13:12,880 --> 00:13:15,220
GPU 1呢，就放了三个box
293
00:13:15,220 --> 00:13:17,500
三个Transformer的层
294
00:13:17,500 --> 00:13:19,560
而Virtual Pipline呢，就反道而其行
295
00:13:19,560 --> 00:13:24,060
首先呢，我GPU 0每一台设备上面呢，只存放一个box
296
00:13:24,060 --> 00:13:25,320
一个Transformer的结构
297
00:13:25,320 --> 00:13:27,240
GPU 1呢，也只存放一个
298
00:13:27,240 --> 00:13:29,060
GPU 1呢，也存放一个
299
00:13:29,060 --> 00:13:30,800
但是再往下了
300
00:13:30,800 --> 00:13:33,680
这个就是刚才的浅色和深色的区别了
301
00:13:33,680 --> 00:13:37,980
GPU 0重新放回一个Transformer的box
302
00:13:37,980 --> 00:13:41,190
这种方式就实现了数量不变的情况
303
00:13:41,190 --> 00:13:44,020
分出了更多的Pipeline stage
304
00:13:44,020 --> 00:13:47,620
用更多的通讯量去换取我们的Bubble的
305
00:13:47,620 --> 00:13:48,900
比例的降低
306
00:13:48,900 --> 00:13:51,120
所以我们可以看到这里面呢，有浅色
307
00:13:51,120 --> 00:13:51,810
有深色
308
00:13:51,810 --> 00:13:53,700
但是呢，带来的好处就是
309
00:13:53,700 --> 00:13:57,320
中间的bubble空载的时间减少了
310
00:13:57,320 --> 00:14:00,440
我们不能说在所有的ai集群里面呢
311
00:14:00,440 --> 00:14:02,740
这种的切分方式是最优的
312
00:14:02,740 --> 00:14:06,430
但是呢，在英伟达的DGX A100集群里面呢
313
00:14:06,430 --> 00:14:10,160
作者呢，就证明了这种tensor Parallelism来加上Data Parallelism
314
00:14:10,160 --> 00:14:12,440
还有Pipline Parallelism的这种方式
315
00:14:12,440 --> 00:14:16,340
实现了整个通讯的加速比是非常好的
316
00:14:16,340 --> 00:14:20,000
并行的算法的内容在这里呢，基本上就结束了
317
00:14:20,000 --> 00:14:22,820
我们今天了解了一个推荐的大模型
318
00:14:22,820 --> 00:14:24,980
主要是由Embendding层去组成的
319
00:14:24,980 --> 00:14:29,080
而Embendding里面呢，我们更多的是采用Model Parallelism来算
320
00:14:29,080 --> 00:14:31,000
在网络模型结构阶段呢
321
00:14:31,000 --> 00:14:33,670
更多的是采用一个Data Parallelism
322
00:14:33,670 --> 00:14:35,500
对于语言大模型呢
323
00:14:35,500 --> 00:14:38,460
我们基本上所有的并行的方式
324
00:14:38,460 --> 00:14:40,560
包括数据并行
325
00:14:40,560 --> 00:14:43,860
张量并行和流水线并行都会混合在一起用
326
00:14:43,860 --> 00:14:46,440
那这种方式呢我们叫做混合并行
327
00:14:46,440 --> 00:14:48,580
最后我稍微打一个广告
328
00:14:48,580 --> 00:14:52,900
就是在MindSpore里面为了实现更多的并行方式
329
00:14:52,900 --> 00:14:54,520
提出了一个新的特性
330
00:14:54,520 --> 00:14:56,640
叫做自动混合并行
331
00:14:56,640 --> 00:15:00,180
因为刚才我们提到的很多的混合并行方式啊
332
00:15:00,180 --> 00:15:01,880
需要人工地去设计
333
00:15:01,880 --> 00:15:03,560
人工地去调测
334
00:15:03,560 --> 00:15:05,540
或者找到一种很好的规律
335
00:15:05,540 --> 00:15:09,520
而MindSpore直接使用了自动混合并行
336
00:15:09,520 --> 00:15:12,400
解决了很多用户去尝试去了解
337
00:15:12,400 --> 00:15:15,340
去深入到这个ai系统的细节里面
338
00:15:15,340 --> 00:15:16,540
让我们的用户呢
339
00:15:16,540 --> 00:15:19,750
或者开发者更加聚焦于自己的算法
340
00:15:19,750 --> 00:15:22,880
他们自己的网络模型结构的搭建理念哦
341
00:15:22,900 --> 00:15:24,380
卷的不行了
342
00:15:24,380 --> 00:15:25,280
卷的不行了
343
00:15:25,280 --> 00:15:26,900
记得一键三连加关注哦
344
00:15:26,900 --> 00:15:30,320
所有的内容都会开源在下面这条链接里面
345
00:15:30,320 --> 00:15:31,360
拜了个拜