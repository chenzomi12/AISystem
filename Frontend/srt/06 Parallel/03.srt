1
00:00:00,140 --> 00:00:04,000
字幕生成: 粟君杰 字幕校对: 粟君杰
2
00:00:04,670 --> 00:00:05,300
hello
3
00:00:05,300 --> 00:00:05,780
大家好
4
00:00:05,780 --> 00:00:07,040
我是ZOMI
5
00:00:07,040 --> 00:00:09,980
今天我们来到大模型与分布式训练里面的
6
00:00:09,980 --> 00:00:12,740
张量并行这个内容里面
7
00:00:12,740 --> 00:00:14,540
那之前隔了一段时间呢
8
00:00:14,540 --> 00:00:16,340
是因为公司到年底了
9
00:00:16,340 --> 00:00:20,490
各种汇报对齐开会，占了非常多的时间
10
00:00:20,490 --> 00:00:24,540
明显更新的速率要比之前的慢了很多
11
00:00:24,540 --> 00:00:26,340
但模型要真正训练起来
12
00:00:26,340 --> 00:00:28,860
其实我们不仅仅需要非常多的money
13
00:00:28,860 --> 00:00:30,280
还有我们的服务器集群
14
00:00:30,280 --> 00:00:33,160
我们更多的是需要分布式并行
15
00:00:33,160 --> 00:00:34,960
或者分布式训练的能力
16
00:00:34,960 --> 00:00:37,900
分布式训练里面呢，又分为模型并行
17
00:00:37,900 --> 00:00:40,900
而模型并行里面有两个非常重要的
18
00:00:40,900 --> 00:00:42,980
就是我们对模型进行切分
19
00:00:42,980 --> 00:00:46,380
今天我们主要来聊聊张量并行的这个概念
20
00:00:46,380 --> 00:00:49,590
首先我们会从一个张量并行的一个原理来讲
21
00:00:49,590 --> 00:00:51,900
然后用三个算子去讲讲
22
00:00:51,900 --> 00:00:53,940
我们张量是怎么做并行的
23
00:00:53,940 --> 00:00:57,240
最后呢，去讲讲张量重排在MindSpore里面
24
00:00:57,240 --> 00:00:58,800
非常重要的一个概念
25
00:00:58,800 --> 00:01:00,240
在上一节里面呢
26
00:01:00,240 --> 00:01:01,860
我们讲了数据并行
27
00:01:01,860 --> 00:01:03,480
数据并行其实分为DP
28
00:01:03,480 --> 00:01:07,480
DDP和FSDP 3种不同的并行模式
29
00:01:07,480 --> 00:01:10,270
那之前呢，我们留了一个遗留问题
30
00:01:10,270 --> 00:01:12,700
假设我输入的图片的通道有非常多
31
00:01:12,700 --> 00:01:14,720
我输入的图片呢也是非常大
32
00:01:14,720 --> 00:01:17,750
那这时候呢，训练的数据这么大的情况下
33
00:01:17,750 --> 00:01:21,000
我们是属于数据并行吗
34
00:01:21,000 --> 00:01:24,060
诶这个问题呢，我们上一节没有回答哦
35
00:01:24,060 --> 00:01:26,000
这里面真正来回答一下
36
00:01:26,000 --> 00:01:28,190
不管是训练书的数据呢
37
00:01:28,190 --> 00:01:31,220
还是我们神经网络里面去执行的数据
38
00:01:31,220 --> 00:01:33,039
我们统一都称为张量
39
00:01:33,039 --> 00:01:35,859
所以对这个这么大的数据进行切分呢
40
00:01:35,859 --> 00:01:38,180
我们叫做张量并行
41
00:01:38,180 --> 00:01:40,880
下面我们来看看张量并行和流水线并行的
42
00:01:40,880 --> 00:01:41,780
不同之处啊
43
00:01:41,780 --> 00:01:43,580
第一个就是我们的流水线并行
44
00:01:43,580 --> 00:01:46,680
可以看到左边的这个图呢，就是流水线并行
45
00:01:46,680 --> 00:01:49,620
流水线并行是按照模型的layer进行切
46
00:01:49,620 --> 00:01:50,880
分到不同的机器的
47
00:01:50,880 --> 00:01:53,020
我第一层切换到第一台机器
48
00:01:53,020 --> 00:01:55,000
第二层切换到第二台机器
49
00:01:55,000 --> 00:01:56,620
那这种层间的并行呢
50
00:01:56,620 --> 00:01:58,890
我们叫做流水线并行
51
00:01:58,890 --> 00:02:01,999
下一个内容呢，我们会来介绍的张量并行呢
52
00:02:01,999 --> 00:02:03,379
就像右边的这个图
53
00:02:03,379 --> 00:02:06,229
我们将计算图的层内的不同的参数
54
00:02:06,229 --> 00:02:07,660
切换到不同的机器
55
00:02:07,660 --> 00:02:09,130
假设是同一层的
56
00:02:09,130 --> 00:02:12,180
我把这一层的参数或者这一层的张量
57
00:02:12,180 --> 00:02:15,270
切分成Device 1和Device 2去执行
58
00:02:15,270 --> 00:02:16,740
那这种层内的并行
59
00:02:16,740 --> 00:02:18,700
我们叫做张量并行
60
00:02:18,700 --> 00:02:19,840
张量并行呢
61
00:02:19,840 --> 00:02:22,320
实际上我们会引起两个重要的思考
62
00:02:22,320 --> 00:02:24,360
如何切分我的网络模型
63
00:02:24,360 --> 00:02:26,160
我是从这个位置进行切分呢
64
00:02:26,160 --> 00:02:28,840
还是这个位置对半切分呢
65
00:02:28,840 --> 00:02:30,760
那第二个值得思考的就是
66
00:02:30,760 --> 00:02:34,360
怎么样去保证我切分完之后的正确性呢
67
00:02:34,360 --> 00:02:35,320
Device 1
68
00:02:35,320 --> 00:02:36,340
第一层的神经元
69
00:02:36,340 --> 00:02:37,540
其实是跟Device 2
70
00:02:37,540 --> 00:02:40,260
最后一层的神经元有相关联联系的
71
00:02:40,260 --> 00:02:43,060
如何保证切分之后的正确性呢
72
00:02:43,060 --> 00:02:45,760
这两个问题都是非常值得大家去思考的
73
00:02:48,700 --> 00:02:50,680
Oh My God~
74
00:02:52,880 --> 00:02:54,400
讲完背景知识之后呢
75
00:02:54,400 --> 00:02:57,160
我们正式的来去了解一下数学原理
76
00:02:57,160 --> 00:02:58,840
数学原理其实很简单啊
77
00:02:58,840 --> 00:03:01,980
张量的切分方式呢主要分为两种
78
00:03:01,980 --> 00:03:04,050
第一种呢，就是行的切分
79
00:03:04,050 --> 00:03:06,630
假设我们现在有一个2x2的矩阵
80
00:03:06,630 --> 00:03:07,800
按行的切分呢
81
00:03:07,800 --> 00:03:10,280
很明显就是直接按行切分
82
00:03:10,280 --> 00:03:13,310
那第二种切分方式呢就按列来切分
83
00:03:13,310 --> 00:03:16,100
那第三种呢其实它不属于一种切分方式啊
84
00:03:16,100 --> 00:03:17,620
它更多是一个复制
85
00:03:17,620 --> 00:03:21,010
就把一个原始的矩阵复制到不同的机器里面
86
00:03:21,010 --> 00:03:22,060
一句话来说
87
00:03:22,060 --> 00:03:23,020
数学原理很简单
88
00:03:23,020 --> 00:03:25,200
但是组合起来就很复杂
89
00:03:25,200 --> 00:03:26,880
X乘以A等于Y
90
00:03:26,880 --> 00:03:27,540
那X呢
91
00:03:27,540 --> 00:03:30,360
我们假设是网络模型的输入或者激活
92
00:03:30,360 --> 00:03:31,560
A就是我们的权重
93
00:03:31,560 --> 00:03:32,900
Y就是我们的输出
94
00:03:32,900 --> 00:03:36,500
我们把A就是我们的权重呢，按列来进行切分
95
00:03:36,500 --> 00:03:38,160
那我就切分了乘以两一个
96
00:03:38,160 --> 00:03:38,820
一个A1 
97
00:03:38,820 --> 00:03:39,600
一个A2 
98
00:03:39,600 --> 00:03:40,320
X乘以A1 
99
00:03:40,320 --> 00:03:42,060
A2呢，就等于我们的Y
100
00:03:42,060 --> 00:03:44,700
那第二个呢，A按行的切分
101
00:03:44,700 --> 00:03:45,880
我按行来切分
102
00:03:45,880 --> 00:03:49,720
这个时候我X就必须按照列进行切分
103
00:03:49,720 --> 00:03:51,010
1x2的矩阵
104
00:03:51,010 --> 00:03:53,050
再乘以2x1的矩阵
105
00:03:53,050 --> 00:03:55,880
最后才能形成一个1x1的矩阵
106
00:03:55,880 --> 00:03:59,930
这个时候我的数呢，就被迫按照列来进行切分
107
00:03:59,930 --> 00:04:01,580
按照不同的切分方式
108
00:04:01,580 --> 00:04:04,360
会影响到我前后的输入输出
109
00:04:04,360 --> 00:04:06,820
下面呢我们正式的去一个MatMul
110
00:04:06,820 --> 00:04:10,870
就是矩阵乘或者GMMLiner这种矩阵乘的算子
111
00:04:10,870 --> 00:04:12,340
并行作为一个例子
112
00:04:12,340 --> 00:04:15,910
那同样的公式X乘以A等于Y X作为输入
113
00:04:15,910 --> 00:04:17,740
A作为算子的权重
114
00:04:17,740 --> 00:04:20,440
现在呢，我把A按照列进行切分
115
00:04:20,440 --> 00:04:21,720
就竖着来切
116
00:04:21,720 --> 00:04:24,640
这时候呢，我就分开一个A1和A2
117
00:04:24,640 --> 00:04:28,570
X乘以A1 A2 等于Y1 Y2 两个数
118
00:04:28,570 --> 00:04:30,640
最后两个数我要把它拼起来
119
00:04:30,640 --> 00:04:34,440
变成一个矩阵Y我需要通过一个All-Gather的方式
120
00:04:34,440 --> 00:04:36,420
因为Y1在Device 1
121
00:04:36,420 --> 00:04:37,740
Y2 在Device 2
122
00:04:37,740 --> 00:04:40,180
我把它拼起来的时候就需要一个通讯
123
00:04:40,180 --> 00:04:43,020
所以这里面呢，用了All-Gather的通讯方式
124
00:04:43,020 --> 00:04:46,770
第二种方式呢，就是我按照行的方式进行切分
125
00:04:46,770 --> 00:04:49,800
那我的X呢，就必须按照列进行切分
126
00:04:49,800 --> 00:04:51,300
所以我们看看下面这个图
127
00:04:53,480 --> 00:04:56,300
这时候呢，我的X就需要进行通讯
128
00:04:56,300 --> 00:04:57,140
放在两半
129
00:04:57,140 --> 00:04:58,340
放在不同的机器
130
00:04:58,340 --> 00:05:00,660
然后A呢，按照正常的进行切分
131
00:05:00,660 --> 00:05:04,200
X1乘以A1等于我们的Y1，X2乘以A2 
132
00:05:04,200 --> 00:05:05,760
等于我的Y2
133
00:05:05,760 --> 00:05:07,380
得到Y1，Y2之后呢
134
00:05:07,380 --> 00:05:09,560
现在还是在两台不同的机器
135
00:05:09,560 --> 00:05:12,660
最后我们要执行一个Reduce的通讯
136
00:05:12,660 --> 00:05:14,580
才能执行这个加号的操作
137
00:05:16,680 --> 00:05:17,940
有了MatMul的理解呢
138
00:05:17,940 --> 00:05:20,160
我们来看看Transformer的MLP
139
00:05:20,160 --> 00:05:24,000
其实Transformer的MLP呢，只是多了一个激活在这里面
140
00:05:24,000 --> 00:05:26,220
这里面呢，我们还是按照刚才的方式
141
00:05:26,220 --> 00:05:28,280
第一种就是按行进行切分
142
00:05:28,280 --> 00:05:29,480
按行的切分的方式
143
00:05:29,480 --> 00:05:33,060
意味着我的X就必须要被迫着按照列进行切分
144
00:05:33,060 --> 00:05:35,490
但是后面我们有一个激活函数
145
00:05:35,490 --> 00:05:37,080
那在执行激活函数的时候呢
146
00:05:37,080 --> 00:05:39,220
我需要进行一个加的操作
147
00:05:39,240 --> 00:05:41,860
这意味着我的Y1、Y2 
148
00:05:41,860 --> 00:05:45,740
就必须要进行一个All-Reduce Sum的通讯
149
00:05:45,740 --> 00:05:47,360
所以从右边的这个图呢
150
00:05:47,360 --> 00:05:49,160
我们这里面通讯就有两次了
151
00:05:49,160 --> 00:05:52,640
第一次就是在Split这个地方把X分成两个
152
00:05:52,640 --> 00:05:55,580
第二个呢，就是在执行GeLU之前呢
153
00:05:55,580 --> 00:05:57,200
再做一个聚合的通信
154
00:05:57,200 --> 00:06:00,520
第二种方式呢，就是A按照列的方式进行切分
155
00:06:00,520 --> 00:06:02,770
我把A切换成A1 A2 
156
00:06:02,770 --> 00:06:05,440
这个时候我的GeLU就可以独立起来
157
00:06:05,440 --> 00:06:06,680
得到Y1、Y2 
158
00:06:06,680 --> 00:06:08,840
最后我再做一个All-Gather的方式
159
00:06:08,840 --> 00:06:11,090
对Y1、Y2进行拼接起来
160
00:06:11,090 --> 00:06:13,400
但是，又但是了
161
00:06:13,400 --> 00:06:15,710
Transformer里面的MLP还没有这么简单
162
00:06:15,710 --> 00:06:18,380
它后面还会接一个Dropout
163
00:06:18,380 --> 00:06:20,480
这个时候我的Y1，Y2出来之后呢
164
00:06:20,480 --> 00:06:23,650
我们还需要乘以另外一个权重
165
00:06:23,650 --> 00:06:25,570
就是我们的B得到Z1、Z2 
166
00:06:25,570 --> 00:06:26,800
然后再合并
167
00:06:26,800 --> 00:06:30,960
那这个时候我之前的Y1、Y2呢，已经是两个了
168
00:06:30,960 --> 00:06:34,170
我最后我的B呢，只要按照行切分就可以了
169
00:06:34,170 --> 00:06:35,820
我A按照列来切分
170
00:06:35,820 --> 00:06:38,140
我的B按照行的方式来切分
171
00:06:38,140 --> 00:06:39,700
最后执行MLP呢
172
00:06:39,700 --> 00:06:42,970
只是在开始跟结尾的时候做一个通讯
173
00:06:46,040 --> 00:06:47,780
第一个呢，就是对X进行广播或者Copy，那第二个就是All-Reduce
174
00:06:47,780 --> 00:06:48,980
把它聚合起来
175
00:06:48,980 --> 00:06:50,240
然后做一个Dropout
176
00:06:50,240 --> 00:06:51,560
在集群里面
177
00:06:51,560 --> 00:06:54,560
通讯的成本会比计算的成本要高很多
178
00:06:54,560 --> 00:06:56,170
因为通讯的时间很慢
179
00:06:56,170 --> 00:06:58,540
计算我们现在已经到了5nm的
180
00:06:58,540 --> 00:07:01,060
晶体管的计算速率是非常高的
181
00:07:01,060 --> 00:07:03,760
所以我们在网络模型或者计算图里面
182
00:07:03,760 --> 00:07:05,440
对我们的张量进行切分
183
00:07:05,440 --> 00:07:06,900
张量并行的时候
184
00:07:06,900 --> 00:07:10,460
要考虑的就是我的整个网络模型怎么切分
185
00:07:10,460 --> 00:07:13,490
才能够让我们的通讯时间更短
186
00:07:13,490 --> 00:07:14,300
更少
187
00:07:14,300 --> 00:07:18,830
使得整网的训练效率更高呃
188
00:07:18,830 --> 00:07:22,280
那在Transformer里面的第二个重要的就是Self-Attention
189
00:07:22,280 --> 00:07:24,470
我们来看看Self-Attention的具体执行
190
00:07:24,470 --> 00:07:26,480
Self-Attention其实比较简单
191
00:07:26,480 --> 00:07:28,970
主要是由Q、K、V进行一个矩阵乘
192
00:07:28,970 --> 00:07:30,880
然后加个Softmax，Dropout
193
00:07:30,880 --> 00:07:31,960
得到我的Y1 
194
00:07:31,960 --> 00:07:34,570
最后呢再执行一个Dropout得到我的Z
195
00:07:34,570 --> 00:07:37,930
那这个方式呢，刚才的MLP其实类似的
196
00:07:37,930 --> 00:07:42,070
同样我的Q、K、V呢是按列的方式进行切分
197
00:07:42,070 --> 00:07:44,320
我的B呢，按行进行切分
198
00:07:44,320 --> 00:07:46,760
使得我只要一开始做一个通讯
199
00:07:46,760 --> 00:07:48,900
结尾的时候再做一次通讯
200
00:07:48,900 --> 00:07:51,120
就执行完我一个Self-Attention的
201
00:07:51,120 --> 00:07:51,600
成了
202
00:07:54,840 --> 00:07:56,580
Transformer一开始推出的时候
203
00:07:56,580 --> 00:07:58,440
主是处理NLP的
204
00:07:58,440 --> 00:08:01,300
而NLP的第一个数呢，就是我的Embedding层
205
00:08:01,300 --> 00:08:03,880
一般用来压缩我的vocabulary
206
00:08:03,880 --> 00:08:06,600
或者作为vocabulary-size的第一场输入
207
00:08:06,600 --> 00:08:09,720
在大模型GPT2里面的vocabulary-size
208
00:08:09,720 --> 00:08:11,100
就已经有5万多个了
209
00:08:11,100 --> 00:08:12,520
再加上hidden-size
210
00:08:12,520 --> 00:08:15,920
其实第一层的网络模型就已经非常的巨大了
211
00:08:15,920 --> 00:08:19,850
那这个时候我们想把它切换到不同的机器里面
212
00:08:19,850 --> 00:08:22,740
才能够塞得多更多的词汇表
213
00:08:22,740 --> 00:08:24,720
那一般的推荐的方式呢
214
00:08:24,720 --> 00:08:26,520
就是按列的方式进行切分
215
00:08:26,520 --> 00:08:29,000
就是按照词表的方式进行切分
216
00:08:29,000 --> 00:08:32,060
前2万个单词呢，我就放在Device 1
217
00:08:32,060 --> 00:08:35,720
前后后两个单词呢我就放在Device 2里面
218
00:08:35,720 --> 00:08:37,160
通过列的切分方式
219
00:08:37,160 --> 00:08:38,510
然后得到Y1，Y2 
220
00:08:38,510 --> 00:08:40,400
最后执行一个All-Gather的通讯
221
00:08:40,400 --> 00:08:41,300
得到我们的Y
222
00:08:41,300 --> 00:08:42,560
再输给我们的Transformer
223
00:08:42,560 --> 00:08:44,359
网络模型的结构层里面
224
00:08:45,059 --> 00:08:47,839
但是Embedding
225
00:08:47,839 --> 00:08:52,260
它不仅仅是用在LLM大规模语言模型里面哦
226
00:08:52,260 --> 00:08:54,960
他还用在推荐模型里面
227
00:08:54,960 --> 00:08:56,520
所以推荐模型里面呢
228
00:08:56,520 --> 00:08:58,620
又有了一个Embedding的切换方式
229
00:08:58,620 --> 00:09:00,780
那下面有两个例子
230
00:09:00,780 --> 00:09:03,420
在推荐领域呢，我们有两种切换方式
231
00:09:03,420 --> 00:09:05,700
一种是Table-wise的切换方式
232
00:09:05,700 --> 00:09:08,240
一种是Column-wise的切换方式
233
00:09:08,240 --> 00:09:10,700
假设在推荐网络模型里面呢
234
00:09:10,700 --> 00:09:13,120
我们有很多物体的输入特征
235
00:09:13,120 --> 00:09:16,780
Table-wise呢，就是按照物体的特征进行切分的
236
00:09:16,780 --> 00:09:20,900
我可能把特征4 2 0，切换到我们的GPU 0里面
237
00:09:20,900 --> 00:09:23,720
我把特征3 1切换到Device 2里面
238
00:09:23,720 --> 00:09:26,040
那这是一种Table-wise的切换方式
239
00:09:26,040 --> 00:09:29,220
第二种Column-wise是最通用最常用的
240
00:09:29,220 --> 00:09:31,950
我们会在混合并行里面详细介绍的
241
00:09:31,950 --> 00:09:33,240
Column-wise的这种方式呢
242
00:09:33,240 --> 00:09:36,729
就是我把所有的特征融合到一起
243
00:09:36,729 --> 00:09:40,480
然后把特征的Embedding表按照列的方式进行切换
244
00:09:40,480 --> 00:09:42,820
可以看到一个物体的Embedding特征呢
245
00:09:42,820 --> 00:09:45,440
我可以放在GPU 0，GPU 1里面
246
00:09:45,440 --> 00:09:48,440
这种就是按照Column-wise进行一个切分的
247
00:09:48,440 --> 00:09:51,860
接下来这个内容呢可能会稍微复杂难的一点
248
00:09:51,860 --> 00:09:53,760
就是cross entropy lost
249
00:09:53,760 --> 00:09:56,280
就是我们的损失函数的并行
250
00:09:56,280 --> 00:10:00,120
对损失函数进行并行呢，有两种场景
251
00:10:00,120 --> 00:10:01,980
第一种呢，就是在我们的大词汇表
252
00:10:01,980 --> 00:10:03,360
或者语言模型里面呢
253
00:10:03,360 --> 00:10:05,580
我的logits的规模是非常大的
254
00:10:05,580 --> 00:10:06,720
其实我们刚才讲的
255
00:10:06,720 --> 00:10:10,210
我的vocabulary-size是非常多的
256
00:10:10,210 --> 00:10:11,500
我在计算的时候
257
00:10:11,500 --> 00:10:14,080
同样需要把这个vocabulary-size呢
258
00:10:14,080 --> 00:10:15,360
传给我们的loss函数
259
00:10:15,360 --> 00:10:16,860
label也是非常多的
260
00:10:16,860 --> 00:10:18,780
因为label对应我们的词表嘛
261
00:10:18,780 --> 00:10:20,100
所以我们要考虑到
262
00:10:20,100 --> 00:10:23,840
把整个词表拆分到不同的记忆上面
263
00:10:24,720 --> 00:10:27,680
第二种场景呢，就是我的分类场景
264
00:10:27,680 --> 00:10:30,200
一些极端的高难度的挑战任务里面
265
00:10:30,200 --> 00:10:33,470
可能对图像的分类就有上万种
266
00:10:33,470 --> 00:10:34,520
上10万种
267
00:10:34,520 --> 00:10:36,320
这个时候如果类别非常的大
268
00:10:36,320 --> 00:10:38,580
也会导致我们的单卡或者
269
00:10:38,580 --> 00:10:40,860
单芯片里面呢，没法储存的计算
270
00:10:40,860 --> 00:10:42,510
我们的logit这个矩阵
271
00:10:42,510 --> 00:10:45,720
所以我们就必须要按照类别的维度进行切分
272
00:10:45,720 --> 00:10:48,060
不管按照类的方式来进行切分呢
273
00:10:48,060 --> 00:10:50,180
还是按照词表的方式进行切分
274
00:10:50,180 --> 00:10:52,880
都是按照列的切换方式
275
00:10:52,880 --> 00:10:56,220
下面一起来回顾一下交叉熵损失函数
276
00:10:56,220 --> 00:10:57,780
那在二分类的时候呢
277
00:10:57,780 --> 00:11:02,020
我们每个类别的预测概率是p和1-p
278
00:11:02,020 --> 00:11:03,820
真实的预测值呢是Y1
279
00:11:03,820 --> 00:11:06,160
让另外一个预测不到的就是1-Y1
280
00:11:06,160 --> 00:11:07,620
这是二分类的情况
281
00:11:07,620 --> 00:11:09,960
那把它拓展到多分类情况里面呢
282
00:11:09,960 --> 00:11:11,480
就有两个求和了
283
00:11:11,480 --> 00:11:12,740
对于多分类的方式
284
00:11:12,740 --> 00:11:14,300
假设我们现有m个类别
285
00:11:14,300 --> 00:11:17,420
Pic呢，就是每一个类别的预测概率
286
00:11:17,420 --> 00:11:19,820
这里面呢，以语言模型作为例子
287
00:11:19,820 --> 00:11:22,380
我们可以看到这里面有一个vocabulary-size
288
00:11:22,380 --> 00:11:24,780
第一步就是要进行数据的拆分
289
00:11:24,780 --> 00:11:28,460
我们把输入的数据按vocabulary-size进行拆分
290
00:11:28,460 --> 00:11:30,470
那假设我现在有四台机器
291
00:11:30,470 --> 00:11:34,320
我可能会把数据按照列的方式进行拆分
292
00:11:34,320 --> 00:11:36,360
而label呢就是我们的真值
293
00:11:36,360 --> 00:11:39,000
一开始它可能是一些具体的单词
294
00:11:39,000 --> 00:11:41,780
可首先会对具体的单词
295
00:11:41,780 --> 00:11:43,520
执行一个onehot的向量的操作
296
00:11:43,520 --> 00:11:46,180
然后再把它Scatter到不同的机器上面
297
00:11:46,180 --> 00:11:48,040
就变成我们右边的这个图
298
00:11:48,040 --> 00:11:50,580
logits就是我们的输入有四个部分
299
00:11:50,580 --> 00:11:53,460
然后我们的label也对应有四个部分
300
00:11:53,460 --> 00:11:56,220
第二步呢，就是最大值同步了
301
00:11:56,220 --> 00:11:57,360
最大值同步呢
302
00:12:02,140 --> 00:12:03,940
中间求最大值的时候
303
00:12:03,940 --> 00:12:05,560
就通过All Reduce(Max)
304
00:12:05,560 --> 00:12:08,680
去保证获取的是一个全局的最大值
305
00:12:08,680 --> 00:12:10,420
而不是某个机器的最大值
306
00:12:10,420 --> 00:12:15,060
那第三步呢就是exp sum和softmax的计算啊
307
00:12:15,060 --> 00:12:17,700
我们继续来打开看看第三步
308
00:12:17,700 --> 00:12:18,120
第三步
309
00:12:18,120 --> 00:12:20,040
首先我进行一个指数的运算
310
00:12:20,040 --> 00:12:21,900
然后求一个当地local的最大值
311
00:12:21,900 --> 00:12:23,929
然后再求全局的最大值
312
00:12:23,929 --> 00:12:25,820
拿到全局的最大值
313
00:12:25,820 --> 00:12:27,740
然后我再进行一个指数的求和
314
00:12:27,740 --> 00:12:29,630
得到softmax的值了
315
00:12:29,630 --> 00:12:32,700
最后一步呢，就是真正执行我们的lost
316
00:12:32,700 --> 00:12:34,680
我们会把输入的logits
317
00:12:34,680 --> 00:12:36,480
还有我们的label进行相乘
318
00:12:36,480 --> 00:12:41,010
并且求和得到我们label所对应的一个位置的值
319
00:12:41,010 --> 00:12:44,900
然后再进行一个All-Reduce Sum全区的同步
320
00:12:44,900 --> 00:12:47,120
然后去计算local softmax
321
00:12:47,120 --> 00:12:49,130
然后加上这个符号的操作
322
00:12:49,130 --> 00:12:53,090
就得到了分布式交叉熵的损失函数值了
323
00:12:53,090 --> 00:12:55,120
看到这一步为止呢
324
00:12:55,120 --> 00:12:56,080
我们可以看到啊
325
00:12:56,080 --> 00:12:58,090
只是简单的模型并行
326
00:12:58,090 --> 00:13:00,420
我们会对输入的数据进行并行
327
00:13:00,420 --> 00:13:04,420
还会对输入数据的第一层Embedding层进行并行
328
00:13:04,420 --> 00:13:08,590
中间的Embedding和MLP或者矩阵乘也会做并行
329
00:13:08,590 --> 00:13:12,460
包括我的损失函数也会做张量并行
330
00:13:12,460 --> 00:13:13,600
总的来说
331
00:13:13,600 --> 00:13:16,240
张量并行无处不在
332
00:13:16,400 --> 00:13:19,420
下面呢我们来看一个额外的知识点
333
00:13:19,420 --> 00:13:21,340
就是我们的随机控制的问题
334
00:13:21,340 --> 00:13:24,920
第一个就是参数初始化的随机性的问题
335
00:13:25,500 --> 00:13:27,920
假设我们现在只有一块卡的时候呢
336
00:13:27,920 --> 00:13:30,920
可能我只需要设置一个简单的随机种子
337
00:13:30,920 --> 00:13:33,800
然后对Device 1进行一个随机数就可以了
338
00:13:33,800 --> 00:13:36,260
但是我把相同的数据切换成两半
339
00:13:36,260 --> 00:13:37,869
放在不同的机器里面
340
00:13:37,869 --> 00:13:40,449
我两台机器的随机种子都是P的时候
341
00:13:40,449 --> 00:13:42,240
就造成了数学上不等价
342
00:13:42,240 --> 00:13:44,580
失去了真正的随机性了
343
00:13:44,580 --> 00:13:47,760
也就是可能我的E1和E2 里面
344
00:13:47,760 --> 00:13:49,640
初始化的数据都是相同的
345
00:13:49,640 --> 00:13:52,490
但是我刚才只有一台机器的时候
346
00:13:52,490 --> 00:13:56,020
两半部分的随机生成的数据肯定是不同的
347
00:13:56,020 --> 00:13:58,540
于是呢我们在做多卡切分的时候
348
00:13:58,540 --> 00:14:01,630
就会去使用不同的随机种子
349
00:14:01,630 --> 00:14:04,440
对不同的机器进行初始化
350
00:14:05,140 --> 00:14:07,020
第二个随机控制的问题呢
351
00:14:07,020 --> 00:14:09,440
就是我们算子计算的一个随机性
352
00:14:09,440 --> 00:14:10,220
可以看到啊
353
00:14:10,220 --> 00:14:13,340
神经网络里面呢不仅仅只有初始化的时候
354
00:14:13,340 --> 00:14:14,690
会引入随机问题
355
00:14:14,690 --> 00:14:15,710
我们在Dropout
356
00:14:15,710 --> 00:14:17,390
还有TruncatedNormal
357
00:14:17,390 --> 00:14:19,040
StandardNormal
358
00:14:19,040 --> 00:14:20,690
还有StandardLaplace
359
00:14:20,690 --> 00:14:23,540
还有Gamm等不同的情况下
360
00:14:23,540 --> 00:14:26,079
都会引入我们的随机种子
361
00:14:26,079 --> 00:14:30,279
这个时候呢就会造成在网络模型当中会引起的
362
00:14:30,279 --> 00:14:31,670
随机性的问题
363
00:14:31,670 --> 00:14:35,150
假设我把一个Dropout的算子进行张量并行
364
00:14:35,150 --> 00:14:38,480
那这个时候呢我可能会设置不同随机种子
365
00:14:38,480 --> 00:14:42,000
但是如果我把所有的数据都All-Gather或者All-Reduce
366
00:14:42,000 --> 00:14:45,090
这个时候我使用相同一个随机种子就可以了
367
00:14:45,090 --> 00:14:46,380
讲这么多原理呢
368
00:14:46,380 --> 00:14:48,240
实际上不是希望我们的用户
369
00:14:48,240 --> 00:14:50,580
或者我们的开发者自己去设置这些
370
00:14:50,580 --> 00:14:52,920
而是告诉大家这些内容呢
371
00:14:52,920 --> 00:14:54,220
AI框架
372
00:14:54,220 --> 00:14:57,310
AI系统都会帮我们自动的去处理好
373
00:14:57,310 --> 00:15:01,160
所以大家不用去担心或者纠结在一个八机八卡
374
00:15:01,160 --> 00:15:03,860
64块NPU的集群里面
375
00:15:03,860 --> 00:15:05,300
如果初始化的时候呢
376
00:15:05,300 --> 00:15:08,420
使用同一个随机种子网络模型
377
00:15:08,420 --> 00:15:10,920
最后训练出来的精度是没有
378
00:15:10,920 --> 00:15:11,880
我初始化的时候
379
00:15:11,880 --> 00:15:14,500
使用不同的随机种子的精度要高的
380
00:15:14,500 --> 00:15:15,760
从这个图里面呢
381
00:15:15,760 --> 00:15:18,460
我们可以看出随机性控制对我们的精度
382
00:15:18,460 --> 00:15:21,670
还有loss收敛还是非常具有影响力的
383
00:15:21,670 --> 00:15:23,380
今天的内容呢稍微多了一点
384
00:15:23,380 --> 00:15:24,960
我们来总结一下模型
385
00:15:24,960 --> 00:15:27,270
并行呢，分为张量并行和流水线并行
386
00:15:27,270 --> 00:15:29,760
而张量并行呢就是层内并行
387
00:15:29,760 --> 00:15:33,220
流水线并行呢是一个层间并行的概念
388
00:15:33,220 --> 00:15:34,900
那第二就是张量并行
389
00:15:34,900 --> 00:15:36,910
主要是对数据进行切分
390
00:15:36,910 --> 00:15:40,440
切分的方式呢有按行按列的方式进行切分
391
00:15:40,440 --> 00:15:44,560
另外最常见的张量并行就是MatMul的张量并行
392
00:15:44,560 --> 00:15:47,320
通过MatMul这个张量并行的算子原理
393
00:15:47,320 --> 00:15:50,410
我们很快的去把它扩展到了Embedding、MLP
394
00:15:50,410 --> 00:15:53,650
还有Transformer等不同的算子的并行
395
00:15:53,650 --> 00:15:57,600
可能还会涉及到损失函数的并行
396
00:15:58,120 --> 00:16:00,690
最后一个就是张量并行的时候
397
00:16:00,690 --> 00:16:03,580
要注意随机性的问题
398
00:16:03,580 --> 00:16:06,520
开发AI框架或者AI系统的时候呢
399
00:16:06,520 --> 00:16:09,500
要注意随机性种子的设置
400
00:16:09,500 --> 00:16:10,460
卷的不行了
401
00:16:10,460 --> 00:16:11,330
卷的不行了
402
00:16:11,330 --> 00:16:12,980
记得一键三连加关注哦
403
00:16:12,980 --> 00:16:14,480
所有的内容都会开源
404
00:16:14,480 --> 00:16:17,420
在下面这条链接里面拜了个拜