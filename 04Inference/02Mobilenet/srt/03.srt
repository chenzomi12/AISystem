1
00:00:00,000 --> 00:00:02,975
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:05,534 --> 00:00:06,359
Hello 大家好

3
00:00:06,359 --> 00:00:11,200
还是回到模型小型化里面的 CNN 小型化

4
00:00:11,200 --> 00:00:16,200
CNN 小型化里面主要是聚焦于网络模型结构的小型化

5
00:00:16,200 --> 00:00:19,000
可以看到其实在上一节课里面

6
00:00:19,000 --> 00:00:21,800
就给大家分享了 SqueezeNet,ShuffleNet,MobileNet

7
00:00:21,800 --> 00:00:25,800
这些网络模型结构更多的是提出了新的模型结构

8
00:00:25,800 --> 00:00:27,000
新的卷积的算法

9
00:00:28,000 --> 00:00:31,600
像 SqueezeNet 就提出了自己的一个 Squeeze 的结构

10
00:00:31,600 --> 00:00:34,600
ShuffleNet 就提出了对 Channel 进行 Shuffle 的操作

11
00:00:34,600 --> 00:00:38,400
MobileNet 就提出了一个新的卷积的压缩的方式

12
00:00:38,400 --> 00:00:41,000
而下面要讲的几个系列

13
00:00:41,000 --> 00:00:44,600
主要是 ESPNet,FBNet,EfficientNet 和 GhostNet

14
00:00:44,600 --> 00:00:49,000
四个内容,下面看一下第一个内容 ESPNet

15
00:00:51,800 --> 00:00:55,200
下面这个就是 ESPNet 的一个论文

16
00:00:55,200 --> 00:00:59,400
很有意思的就是 ESPNet 跟刚才的一些网络模型

17
00:00:59,400 --> 00:01:02,000
或者刚才介绍的一些组件网络模型不一样

18
00:01:02,000 --> 00:01:04,400
它主要是针对具体的小任务

19
00:01:04,400 --> 00:01:08,200
就是 Semantic Segmentation 图像的分割

20
00:01:08,200 --> 00:01:11,200
其实在做图像的分割有一种方法

21
00:01:11,200 --> 00:01:14,200
就是把刚才介绍的一些轻量化的模型

22
00:01:14,200 --> 00:01:16,000
作为网络模型的主干

23
00:01:16,000 --> 00:01:22,600
然后在具体的 backbone 或者 Head 就替换掉 YOLO 或者 SSD 了

24
00:01:22,600 --> 00:01:25,600
这里面就提出了两个新的概念

25
00:01:25,600 --> 00:01:27,238
Efficient Special Pyramid

26
00:01:27,238 --> 00:01:29,400
高效的金字塔的结构

27
00:01:29,400 --> 00:01:31,751
第二个就是 Hierarchical Feature Fusion

28
00:01:31,751 --> 00:01:33,800
多层的特征融合

29
00:01:33,800 --> 00:01:36,000
现在看看有什么不一样哦

30
00:01:36,000 --> 00:01:38,800
第一个左边的这个就是 Standard 卷积

31
00:01:38,800 --> 00:01:40,700
普通的卷积是不断的卷卷卷卷卷

32
00:01:40,700 --> 00:01:42,200
然后不断的卷积

33
00:01:42,200 --> 00:01:45,200
这里面这个高效的金字塔结构

34
00:01:45,200 --> 00:01:48,200
就是前面我有一个 Pointwise 的卷积

35
00:01:48,200 --> 00:01:51,200
接下来就提供了几种不同的 Kernel 大小

36
00:01:51,200 --> 00:01:53,200
进行一个空洞化的卷积

37
00:01:53,200 --> 00:01:56,000
分开两步叫做 ESP 的结构

38
00:01:56,000 --> 00:01:58,800
第二个就是 HFF 的结构

39
00:01:58,800 --> 00:02:00,400
像 HFF 的结构可以看到

40
00:02:00,400 --> 00:02:05,800
其实这里面有很多种不同的金字塔的网络模型的大小

41
00:02:05,800 --> 00:02:09,064
最后通过这种加 Sum 加 Concate 

42
00:02:09,064 --> 00:02:11,000
网络模型结构的组合

43
00:02:11,000 --> 00:02:13,000
就变成了一个 Hierarchical Feature Fusion

44
00:02:13,000 --> 00:02:17,200
通过这种方式有效的去对下游任务图像分割

45
00:02:17,200 --> 00:02:18,600
保持一定的精度

46
00:02:18,600 --> 00:02:21,600
但是降低了整个网络模型的参数量

47
00:02:24,000 --> 00:02:27,400
接下来看看第二篇文章

48
00:02:27,400 --> 00:02:30,200
这个系列的第二篇文章 ESPNet V2

49
00:02:30,200 --> 00:02:35,200
下面第二个就是 ESPNet V2 第二个版本了

50
00:02:35,200 --> 00:02:37,200
确实好多这种系列版本

51
00:02:37,200 --> 00:02:40,200
看看它的网络模型结构的组成

52
00:02:40,200 --> 00:02:43,800
下面这个就是 ESPNet 的网络模型结构

53
00:02:43,800 --> 00:02:47,800
左边的这次刚才介绍的 V1 的一个 ESP 的 Block

54
00:02:47,800 --> 00:02:50,800
右边的这个就是 ESP 的一个 A 的版本

55
00:02:50,800 --> 00:02:53,400
最后这个 C 图就是最终的版本

56
00:02:53,400 --> 00:02:54,800
看一下有什么区别

57
00:02:54,800 --> 00:02:57,800
这种就是普通的 Depth-wise 的卷积

58
00:02:57,800 --> 00:03:00,600
而作者换成 Group 的一个 Depth-wise 的卷积

59
00:03:00,600 --> 00:03:04,200
就是进一步的提升了网络模型的一个轻量化

60
00:03:04,200 --> 00:03:07,600
接着第二个改变就是把 Depth-wise 的卷积

61
00:03:07,600 --> 00:03:09,400
就是后面的一些卷积

62
00:03:09,400 --> 00:03:11,800
后面就增加了一个 1x1 的卷积

63
00:03:11,800 --> 00:03:15,800
进一步的提取它的一些特征的空间出来

64
00:03:15,800 --> 00:03:17,600
作者通过实验就发现

65
00:03:17,600 --> 00:03:19,200
再提取一层特征出来了

66
00:03:19,200 --> 00:03:22,000
其实我还不如先 Concate 之后

67
00:03:22,000 --> 00:03:24,200
再做一个 Group 的一个卷积

68
00:03:24,200 --> 00:03:27,200
那就把这种卷积的方式替换成 Group 卷积

69
00:03:27,200 --> 00:03:28,600
这种方式进行提取

70
00:03:28,600 --> 00:03:32,800
可以看到从左边的一个卷积换成 Group 的卷积

71
00:03:32,800 --> 00:03:35,200
然后增加一个 Group 特征的提取

72
00:03:35,200 --> 00:03:37,400
最后汇集起来

73
00:03:37,400 --> 00:03:40,800
这种就是 ESPNet V2 的特征

74
00:03:42,400 --> 00:03:44,800
介绍完 ESPNet 这种语义分割特征之后

75
00:03:44,800 --> 00:03:48,000
看看 FBNet 这个系列

76
00:03:48,800 --> 00:03:50,400
针对 FBNet 和 EfficientNet

77
00:03:50,400 --> 00:03:53,400
我就不单独的去展开它们的论文了

78
00:03:53,400 --> 00:03:57,200
像 FBNet 其实已经推出了 V1 V2 V3 V5

79
00:03:57,200 --> 00:03:59,200
不同的版本版本量非常多

80
00:03:59,200 --> 00:04:02,400
而 EfficientNet 同样也推出了 V1 V2 的版本

81
00:04:02,400 --> 00:04:06,600
这两个版本是发生在 2018 年和 2019 年

82
00:04:06,600 --> 00:04:09,000
这段时间最火的一个技术是什么

83
00:04:09,600 --> 00:04:11,800
山顶的朋友们请告诉我

84
00:04:11,800 --> 00:04:17,600
2018 年到 2019 年最火的一个网络模型搜索的技术是什么

85
00:04:20,400 --> 00:04:21,600
NAS 搜索

86
00:04:21,600 --> 00:04:22,000
对

87
00:04:22,000 --> 00:04:24,000
像 FBNet 和 EfficientNet

88
00:04:24,000 --> 00:04:26,200
其实都是用了 NAS 的搜索方法

89
00:04:26,200 --> 00:04:30,200
只是搜索的网络模型的结构和方向各有所不同

90
00:04:30,200 --> 00:04:33,200
所以说它们两个的方式都是基于 NAS 

91
00:04:33,200 --> 00:04:34,600
而基于 NAS 的搜索方法

92
00:04:34,600 --> 00:04:37,400
其实经过了这几年的一个延续

93
00:04:37,800 --> 00:04:41,600
发现它不是一个可持续的一个场景

94
00:04:41,600 --> 00:04:43,800
因为网络模型搜索的空间太大

95
00:04:43,800 --> 00:04:45,600
占用的资源量太多

96
00:04:45,600 --> 00:04:49,800
所以说它已经不再是现在的一个研究的重点

97
00:04:49,800 --> 00:04:51,600
那今天的最后的内容

98
00:04:51,600 --> 00:04:53,800
去读一读

99
00:04:53,800 --> 00:04:55,200
诺亚提出来

100
00:04:55,200 --> 00:04:58,000
韩凯提出来的一个 GhostNet 这篇文章

101
00:04:59,800 --> 00:05:01,800
下面看看 GhostNet 这篇文章

102
00:05:01,800 --> 00:05:04,400
GhostNet 这篇文章是由华为的韩凯

103
00:05:04,400 --> 00:05:07,400
还有王云鹤来去作为一个第一作者

104
00:05:07,400 --> 00:05:10,600
现在看看它的一个主要的网络模型结构

105
00:05:10,600 --> 00:05:14,600
那下面 A 图就是普通的一个卷积层

106
00:05:14,600 --> 00:05:16,000
这个没啥好讲

107
00:05:16,000 --> 00:05:19,000
看看 GhostModel 这一个模块

108
00:05:19,000 --> 00:05:20,800
像 GhostModel 这个模块

109
00:05:20,800 --> 00:05:23,800
可以看到输入的时候还是有一个卷积

110
00:05:23,800 --> 00:05:25,000
但是这个卷积

111
00:05:25,000 --> 00:05:26,600
它的一个 channel 数

112
00:05:26,600 --> 00:05:28,600
其实是急剧的减少

113
00:05:28,600 --> 00:05:30,800
然后减少了这个 channel 数之外

114
00:05:30,800 --> 00:05:31,800
其实没关系

115
00:05:31,800 --> 00:05:33,800
它通过一个数学的线性映射

116
00:05:33,800 --> 00:05:36,200
又产生了很多不同的 feature map

117
00:05:36,200 --> 00:05:38,600
最后把卷积得到的 feature map

118
00:05:38,600 --> 00:05:41,000
还有一些做线性映射得到的 feature map

119
00:05:41,000 --> 00:05:42,800
把它们 concate 到一起

120
00:05:42,800 --> 00:05:44,800
最后做一个统一的输出

121
00:05:44,800 --> 00:05:47,200
通过这种减少单个卷积的方式

122
00:05:47,200 --> 00:05:50,400
替换成为普通的其他的一个数学的操作

123
00:05:50,400 --> 00:05:53,400
这种方式就是剪枝单层的卷积的运算

124
00:05:53,400 --> 00:05:56,800
然后用线性的一个运算方式去替换掉

125
00:05:56,800 --> 00:05:59,400
那继续往下看看

126
00:05:59,400 --> 00:06:03,200
那这个就是 stride 等于 1 的一个的 bottleneck

127
00:06:03,200 --> 00:06:05,600
这种叫做 ghost bottleneck

128
00:06:05,600 --> 00:06:07,200
那这个方式一看这个图

129
00:06:07,200 --> 00:06:08,800
其实大家很容易联想

130
00:06:08,800 --> 00:06:12,400
它就是基于一个 resnet 的一个模型的结构

131
00:06:12,400 --> 00:06:14,400
进行一个改动

132
00:06:15,600 --> 00:06:16,200
好了

133
00:06:16,200 --> 00:06:19,600
那现在在整个轻量级的网络模型里面

134
00:06:19,600 --> 00:06:22,200
基本上就介绍完它整体的系列了

135
00:06:22,200 --> 00:06:24,400
继续往下看一看

136
00:06:24,400 --> 00:06:26,200
做一个简单的总结

137
00:06:27,400 --> 00:06:29,200
那在卷积核的大小方面

138
00:06:29,200 --> 00:06:31,800
其实会把一些大的卷积核

139
00:06:32,000 --> 00:06:35,200
用了很多个小的卷积核进行代替

140
00:06:35,200 --> 00:06:36,800
例如 3x3 的卷积核

141
00:06:36,800 --> 00:06:39,800
会用一个 3x1 加一个 1x3 的卷积核

142
00:06:39,800 --> 00:06:42,400
就是低秩分解的方式进行替换

143
00:06:42,400 --> 00:06:45,200
那第二种方式就是单一尺寸的卷积核

144
00:06:45,200 --> 00:06:47,800
使用了更多尺寸的卷积核

145
00:06:47,800 --> 00:06:49,800
进行一个替换

146
00:06:49,800 --> 00:06:53,200
那第三种方式就是使用固定的形状的卷积核

147
00:06:53,200 --> 00:06:56,200
慢慢的使用了可变形状的卷积核

148
00:06:56,200 --> 00:06:58,600
例如普通卷积变成一个 Depth-wise 卷积

149
00:06:58,600 --> 00:07:01,000
然后变成 Depth-wise 加 Point-wise 卷积

150
00:07:01,000 --> 00:07:03,200
或者变成一个 Point-wise 卷积

151
00:07:04,000 --> 00:07:08,000
第四点就是大量的去使用了 1x1 卷积核

152
00:07:08,000 --> 00:07:09,600
作为 Bottleneck 的一个结构

153
00:07:09,600 --> 00:07:12,200
其中的一个输入或者输出

154
00:07:12,200 --> 00:07:16,400
确实在卷积核方面做了很多不同的演进

155
00:07:16,400 --> 00:07:20,200
那第二点就是在卷积层方面做的一个演进方式

156
00:07:20,200 --> 00:07:23,800
就是把标准的卷积核变成一个 Depth-wise 的卷积核

157
00:07:23,800 --> 00:07:25,600
或者一个 Point-wise 的卷积核

158
00:07:25,600 --> 00:07:29,600
第二个 ShuffleNet 的这种就使用了分组的卷积 

159
00:07:29,600 --> 00:07:33,200
第三个就是分组的卷积之前或者卷积之后

160
00:07:33,200 --> 00:07:35,400
使用了一个 channel Shuffle 的一个方式

161
00:07:36,800 --> 00:07:39,400
第四个就是通道加权的计算

162
00:07:39,400 --> 00:07:43,200
例如 SqueezeNet 把 1x1 加 3x3 进行一个 concate

163
00:07:43,200 --> 00:07:45,600
那在卷积层连接方面

164
00:07:45,600 --> 00:07:48,400
其实用了更多的 skip connection

165
00:07:48,400 --> 00:07:50,800
让模型更加深

166
00:07:50,800 --> 00:07:53,600
那第二个就使用了 dense connection

167
00:07:53,600 --> 00:07:54,800
融合起来了

168
00:07:54,800 --> 00:07:57,800
然后把其他的特征融合更多的特征

169
00:07:57,800 --> 00:08:00,000
从而提升网络模型的精度

170
00:08:00,000 --> 00:08:01,200
那所以总结起来

171
00:08:01,200 --> 00:08:04,800
整个 CNN 网络模型的一个结构的轻量化

172
00:08:04,800 --> 00:08:06,400
总有三方面

173
00:08:06,400 --> 00:08:09,400
第一方面就是卷积核进行改进

174
00:08:09,400 --> 00:08:13,800
第二个就是卷积层的通道数进行改进

175
00:08:13,800 --> 00:08:18,200
第三个就是卷积层的连接的方式进行改进

176
00:08:18,200 --> 00:08:21,000
从而把网络模型变得更小

177
00:08:21,000 --> 00:08:23,000
变得精度更高

178
00:08:23,000 --> 00:08:23,400
好了

179
00:08:23,400 --> 00:08:25,200
今天的分享就到这里为止

180
00:08:25,200 --> 00:08:25,800
谢谢各位

181
00:08:25,800 --> 00:08:27,000
拜拜

182
00:08:27,000 --> 00:08:27,800
卷得不行了

183
00:08:27,800 --> 00:08:28,600
卷得不行了

184
00:08:28,600 --> 00:08:30,400
记得一键三连加关注哦

185
00:08:30,400 --> 00:08:34,000
所有的内容都会开源在下面这条链接里面

186
00:08:34,000 --> 00:08:35,400
拜拜

