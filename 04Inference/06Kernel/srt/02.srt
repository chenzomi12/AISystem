1
00:00:00,000 --> 00:00:04,864
字幕生成：Galaxies     字幕校对：NaHS

2
00:00:04,864 --> 00:00:07,880
Hello 大家好，我是好久没有更新的周米

3
00:00:07,880 --> 00:00:11,400
最近确实实在是太忙了，忙着 ChatGPT 这个课程

4
00:00:11,400 --> 00:00:15,960
这个课程就导致我经常加班,晚上回来就有了惰性

5
00:00:15,960 --> 00:00:20,240
今天还是来到了推理引擎的 Kernel 优化

6
00:00:20,240 --> 00:00:24,880
在整个 Kernel 优化里面，来到了第一个比较简单的内容

7
00:00:24,880 --> 00:00:29,440
就是看看卷积优化的整体的原理，卷积需要做哪些优化

8
00:00:29,440 --> 00:00:32,920
下面看一下整个 Kernel 优化,或者卷积优化里面

9
00:00:32,920 --> 00:00:32,945
还在第一个阶段,算法的优化

10
00:00:32,945 --> 00:00:35,720
还在第一个阶段,算法的优化

11
00:00:35,720 --> 00:00:39,920
而这里面的算法主要还是讲卷积的优化

12
00:00:39,920 --> 00:00:42,160
接着来再深入一下

13
00:00:42,160 --> 00:00:46,720
Kernel 优化主要是围绕着 Kernel 层去实现

14
00:00:46,720 --> 00:00:49,560
而之前其实已经给大家重复讲过了

15
00:00:49,560 --> 00:00:52,240
对一个简单的卷积算子

16
00:00:52,240 --> 00:00:56,360
在 CPU 里面可能会使用 NEON 指令集去实现

17
00:00:56,360 --> 00:00:56,365
也可能会使用 X86 的 AVX 指令集去实现

18
00:00:56,365 --> 00:01:00,040
也可能会使用 X86 的 AVX 指令集去实现

19
00:01:00,040 --> 00:01:04,360
在一个推进引擎里面的实现的方式就有非常多种

20
00:01:04,360 --> 00:01:08,215
因为推进引擎要支持 CPU，也要支持 GPU

21
00:01:08,215 --> 00:01:08,240
而 CPU 就有两种不同的实现

22
00:01:08,240 --> 00:01:10,815
而 CPU 就有两种不同的实现

23
00:01:10,840 --> 00:01:14,640
可能在 GPU 上面就有更多种不同的实现

24
00:01:14,640 --> 00:01:18,025
可能会用 CUDA 实现、用 OpenCL、用 Vulkan

25
00:01:18,040 --> 00:01:21,360
可能还会用 OpenGL，还有 Meta 来去实现

26
00:01:21,400 --> 00:01:26,360
最终实现完的，会把它封装成一个高性能的算子库

27
00:01:26,360 --> 00:01:28,600
也可能直接提供 Kernel 层

28
00:01:28,600 --> 00:01:30,680
废话和前期知识有点多

29
00:01:30,770 --> 00:01:36,000
再往下看一下，在整个 Kernel 优化里面会涉及到哪些内容

30
00:01:36,000 --> 00:01:39,080
首先会去看一下什么是卷积

31
00:01:39,080 --> 00:01:43,300
那如果大家都懂的，这个概念可以跳过这一节，来去看下一节

32
00:01:43,300 --> 00:01:49,280
下一节就会讲讲 Caffe 里面用的最多的 img2col 这种优化的算法

33
00:01:49,280 --> 00:01:49,305
它是一种卷积的具体的实现方法

34
00:01:49,305 --> 00:01:51,480
它是一种卷积的具体的实现方法

35
00:01:51,480 --> 00:01:56,960
把卷积的操作变成 GEMM(General Matrix Multiplication)这种运算的方式

36
00:01:56,960 --> 00:01:59,200
然后会有一个空间组合的优化

37
00:01:59,200 --> 00:02:00,880
空间组合优化其实比较简单

38
00:02:00,880 --> 00:02:03,760
跟编译里面的其实是比较相似

39
00:02:03,760 --> 00:02:07,535
也是结合了 img2col 的思想来去实现

40
00:02:07,535 --> 00:02:07,560
在后面的两节里面就有点意思了

41
00:02:07,560 --> 00:02:10,495
在后面的两节里面就有点意思了

42
00:02:10,520 --> 00:02:14,320
里面会讲到 Winograd 这种优化的算法

43
00:02:14,320 --> 00:02:18,840
Winograd 这个优化算法是在 1980 几年的时候已经提出来了

44
00:02:18,840 --> 00:02:23,200
但是当时候因为算力的问题没有很好的得到一个重用

45
00:02:23,200 --> 00:02:29,180
现在应该是在 2011 年之后 Winpgrad 开始慢慢的起来了

46
00:02:29,200 --> 00:02:32,720
确实它在做一些小卷积核的计算的时候

47
00:02:32,720 --> 00:02:34,520
效果性能还是很好

48
00:02:34,520 --> 00:02:39,585
然后再到 QNNPACK，间接卷积优化这种方式

49
00:02:39,640 --> 00:02:43,200
去看看卷积怎幺做不同的优化

50
00:02:43,215 --> 00:02:45,680
现在来到了第一个内容

51
00:02:45,680 --> 00:02:48,200
就是卷积的基础概念

52
00:02:48,240 --> 00:02:50,720
来了解一下什幺是卷积

53
00:02:50,720 --> 00:02:53,640
现在看一下什么是卷积

54
00:02:53,640 --> 00:02:57,760
实际上卷积是神经网络里面的内核计算单元之一

55
00:02:57,760 --> 00:02:59,040
为什么叫做之一呢?

56
00:02:59,040 --> 00:03:02,560
因为在神经网络里面的最核心的几种计算单元

57
00:03:02,560 --> 00:03:07,040
有卷积 CNN,也有 LSTM,还有 Transformer

58
00:03:07,040 --> 00:03:10,720
最后一个就是最常用的 MatMul,矩阵相乘

59
00:03:10,720 --> 00:03:15,080
实际上卷积在前几年应该是非常非常的火

60
00:03:15,080 --> 00:03:17,480
基本上所有的 CPU 算法都离不开它

61
00:03:17,520 --> 00:03:20,360
包括常用的 Restnet,MobileNet,EfficientNet

62
00:03:20,360 --> 00:03:24,560
这些都是使用了或者大量的使用了卷积的计算

63
00:03:24,560 --> 00:03:30,000
但是实际上卷积的变种是非常非常的丰富和多样

64
00:03:30,000 --> 00:03:31,880
除了通用的卷积计算之外

65
00:03:31,880 --> 00:03:34,600
还会在卷积里面加上 Bias

66
00:03:34,600 --> 00:03:37,400
可能会对卷积进行空洞卷积

67
00:03:37,400 --> 00:03:39,840
可能还会进行一个 Device 的卷积

68
00:03:39,840 --> 00:03:42,760
所以会说卷积的变种非常丰富

69
00:03:42,760 --> 00:03:45,560
而且它的计算还是非常的复杂

70
00:03:45,600 --> 00:03:49,120
神经网络或者在一个网络模型当中

71
00:03:49,120 --> 00:03:52,800
大部分时间都消耗在卷积计算

72
00:03:52,800 --> 00:03:57,880
所以如何对卷积进行优化就变得非常重要

73
00:03:57,880 --> 00:04:02,320
这也是这个系列或者在 Kernel 优化里面重点去讲解

74
00:04:02,320 --> 00:04:05,160
当然 Transform 也是一种很好的例子

75
00:04:05,160 --> 00:04:07,000
但是 Transform 相关的优化

76
00:04:07,000 --> 00:04:09,960
说实话其实并不是说非常的多

77
00:04:09,960 --> 00:04:11,000
没有像卷积那样

78
00:04:11,000 --> 00:04:14,800
而且卷积在端侧推理引擎里面是非常的成熟

79
00:04:14,800 --> 00:04:17,440
而 Transform 在端侧推理引擎里面

80
00:04:17,440 --> 00:04:21,240
现在来看应用的不是非常多

81
00:04:21,240 --> 00:04:24,120
在前面其实已经介绍过 Transform 这个结构

82
00:04:24,120 --> 00:04:28,680
在端侧推理引擎里面现在还在慢慢的引入阶段

83
00:04:28,680 --> 00:04:30,520
还没有等到大规模成熟

84
00:04:30,520 --> 00:04:32,760
而且随着时间技术的发展

85
00:04:32,760 --> 00:04:37,440
研究员就提出了多种的关于卷积的优化方法

86
00:04:37,440 --> 00:04:39,240
包括 img2col 还有 Winograd

87
00:04:39,240 --> 00:04:43,080
下面来看一下什么为之卷积

88
00:04:43,280 --> 00:04:46,720
下面是卷积的一些我在网上找到

89
00:04:46,720 --> 00:04:48,120
维基百科的概念

90
00:04:48,120 --> 00:04:50,080
说实话我看的不是很懂

91
00:04:50,080 --> 00:04:52,880
如果大家想深入了解什么为之卷积

92
00:04:52,880 --> 00:04:54,520
卷积跟傅里叶之间的关系

93
00:04:54,520 --> 00:04:56,200
还有卷积的具体的原理

94
00:04:56,200 --> 00:04:58,760
大家可以看一下网上相关的介绍

95
00:04:58,760 --> 00:05:00,080
这也是非常的多

96
00:05:00,080 --> 00:05:01,880
简单的去给大家念一念

97
00:05:01,880 --> 00:05:04,360
卷积主要是通过两个函数

98
00:05:04,360 --> 00:05:06,120
一个 F,一个是 G

99
00:05:06,120 --> 00:05:10,880
通过两个函数生成第三个函数的一种具体的数学运算

100
00:05:10,880 --> 00:05:14,080
可以把它变成等于 H(x)

101
00:05:14,080 --> 00:05:16,720
它的本质是一种特殊的积分变换

102
00:05:16,720 --> 00:05:20,240
所以可以看到在这里面有个积分的符号

103
00:05:20,240 --> 00:05:22,680
这里面就表示了表征函数

104
00:05:22,680 --> 00:05:24,360
F 是一个表征函数

105
00:05:24,360 --> 00:05:25,840
G 也是一个表征函数

106
00:05:25,840 --> 00:05:29,280
经过翻转和平均重叠的部分的函数的乘积

107
00:05:29,280 --> 00:05:32,320
里面的一个长度的积分

108
00:05:32,320 --> 00:05:35,360
那幺可以看到有两个框框

109
00:05:35,360 --> 00:05:36,840
第一个是蓝色的框框

110
00:05:36,840 --> 00:05:40,360
第二个就是这里面移动的黄色的框框

111
00:05:40,360 --> 00:05:42,560
蓝色的框框是 f(τ)

112
00:05:42,560 --> 00:05:45,680
而红色的框框是 g(t-τ)

113
00:05:45,680 --> 00:05:50,680
而三角形经过的三角形就是 F 跟 G 两个表征函数

114
00:05:50,680 --> 00:05:52,560
而得到的一个积分的概念

115
00:05:52,560 --> 00:05:55,480
说实话积分的概念我不明白为什幺要这幺做

116
00:05:55,480 --> 00:05:57,960
现在看一下积分的公式

117
00:05:57,960 --> 00:06:01,000
可以看到积分公式里面有几个比较重要的元素

118
00:06:01,000 --> 00:06:02,200
第一个就是 T

119
00:06:02,200 --> 00:06:03,360
第二个就是τ

120
00:06:03,360 --> 00:06:04,880
第三个就是 T-τ

121
00:06:04,880 --> 00:06:07,640
把它组成一个新的公式

122
00:06:07,640 --> 00:06:12,320
现在可以看到新的公式就是 T=τ+(t-τ)

123
00:06:12,320 --> 00:06:15,560
把它再去分裂成两边两条公式

124
00:06:15,560 --> 00:06:17,640
第一条就是 X=τ

125
00:06:17,640 --> 00:06:20,560
第二个就是 Y=T-τ

126
00:06:20,560 --> 00:06:23,400
然后把这两个公式再组合起来

127
00:06:23,400 --> 00:06:25,320
变成 X+Y=n

128
00:06:25,320 --> 00:06:28,800
这时候可以看到这是一条线性的公式

129
00:06:28,800 --> 00:06:30,800
可以往下再看一看

130
00:06:30,800 --> 00:06:33,760
这条线性的公式把它在二维空间里面画出来了

131
00:06:33,760 --> 00:06:36,000
就是像右边的所示

132
00:06:36,000 --> 00:06:39,840
把 N 不断的去放成不同的一种数值

133
00:06:39,840 --> 00:06:44,960
然后可以看到这条公式类似于在平面当中不断的去划过

134
00:06:44,960 --> 00:06:47,120
如果编辑刚才上面这条直线

135
00:06:47,120 --> 00:06:48,960
就好像毛巾

136
00:06:48,960 --> 00:06:50,560
把毛巾卷起来

137
00:06:50,560 --> 00:06:52,680
然后变成一个新的概念

138
00:06:52,680 --> 00:06:54,240
或者一种新的形态一样

139
00:06:54,240 --> 00:06:57,080
这种就是在网上比较通俗

140
00:06:57,080 --> 00:06:59,320
或者比较容易理解的一种概念

141
00:06:59,320 --> 00:07:01,360
现在讲不清明白没关系

142
00:07:01,360 --> 00:07:02,720
大家可以不用看这个视频

143
00:07:02,720 --> 00:07:05,200
这个视频其实确实没什幺太多的内容

144
00:07:05,200 --> 00:07:09,120
可以看到现在很多时候会把积分

145
00:07:09,120 --> 00:07:13,360
积分是对连续的数据或者无穷的数据进行一个计算

146
00:07:13,360 --> 00:07:15,440
这个就是信号处理当中卷积

147
00:07:15,440 --> 00:07:19,640
把连续变成一个离散的形式的表示

148
00:07:19,640 --> 00:07:21,520
现在把这条公式

149
00:07:21,520 --> 00:07:23,480
再拓展到二维的空间

150
00:07:23,480 --> 00:07:26,000
就得到了神经网络里面的卷积

151
00:07:26,000 --> 00:07:28,280
可以看到这里面的公式非常多

152
00:07:28,280 --> 00:07:30,720
关于里面的参数量的非常多

153
00:07:30,720 --> 00:07:32,040
逐个来打开一下

154
00:07:32,040 --> 00:07:35,040
首先 S 就是卷积核

155
00:07:35,040 --> 00:07:37,040
或者卷积最后的输出

156
00:07:37,080 --> 00:07:39,360
而 I 就是卷积的输入

157
00:07:39,360 --> 00:07:41,800
假设你当它作为一张图片就行了

158
00:07:41,800 --> 00:07:44,240
而 K 就是卷积核

159
00:07:44,240 --> 00:07:46,840
右边就是具体的计算公式

160
00:07:46,840 --> 00:07:48,640
再往下看一看

161
00:07:48,640 --> 00:07:52,960
这里面有一个非常好的一个可视化的网站

162
00:07:52,960 --> 00:07:54,720
打开去看看

163
00:07:56,360 --> 00:08:00,320
这个就是 GitHub 里面一个非常好的一个可视化的方式

164
00:08:00,320 --> 00:08:04,360
可以看到卷积形式方法非常多

165
00:08:04,360 --> 00:08:05,960
有标准的卷积

166
00:08:05,960 --> 00:08:07,120
有反卷积

167
00:08:07,120 --> 00:08:08,560
有分组卷积

168
00:08:08,560 --> 00:08:09,760
有可分离卷积

169
00:08:09,760 --> 00:08:12,560
还有分组卷积的方式非常多

170
00:08:12,560 --> 00:08:16,600
下面蓝色的比较深颜色的就是卷积核

171
00:08:16,600 --> 00:08:20,480
而下面底的蓝色就是输入的图片

172
00:08:20,480 --> 00:08:23,480
卷积核跟图片进行滑窗之后

173
00:08:23,480 --> 00:08:25,520
就得到一个新的输出的结果

174
00:08:25,520 --> 00:08:26,720
而新的输出的结果

175
00:08:26,720 --> 00:08:31,280
这就是上面绿色的对应于公式里面的 S

176
00:08:31,280 --> 00:08:33,480
这个就是卷积的具体的方式

177
00:08:33,520 --> 00:08:38,200
其实卷积更多的一开始没有应用到神经网络里面

178
00:08:38,200 --> 00:08:41,480
在图像处理的时候用的特别的多

179
00:08:41,480 --> 00:08:45,360
下面这个就是整体的卷积的一个计算

180
00:08:45,360 --> 00:08:47,960
这个就是卷积的计算核

181
00:08:47,960 --> 00:08:49,920
这个就是图片

182
00:08:49,920 --> 00:08:52,720
图片跟卷积核进行一个叉乘

183
00:08:52,720 --> 00:08:56,120
就得到最终的结果-3

184
00:08:56,120 --> 00:08:58,520
具体的公式就比较简单

185
00:08:58,520 --> 00:09:03,440
这里面的卷积和每一个元素可以看到-1、0、1

186
00:09:03,440 --> 00:09:09,160
-1、0、1，然后跟上面的每一个元素进行相乘

187
00:09:09,160 --> 00:09:12,560
-1 乘以 3，0 乘以 0，1 乘以 1

188
00:09:12,560 --> 00:09:15,000
然后再把它进行求和

189
00:09:15,000 --> 00:09:17,000
跟刚才的公式是一模一样

190
00:09:17,000 --> 00:09:20,200
最终得到-3 这个结果

191
00:09:20,200 --> 00:09:22,240
这个就是卷积的计算

192
00:09:22,240 --> 00:09:25,240
既然提到卷积在图像处理里面用的非常多

193
00:09:25,240 --> 00:09:26,880
现在打开 PhotoShop

194
00:09:26,880 --> 00:09:30,600
看一下卷积在 PhotoShop 里面最常用的一些功能

195
00:09:31,120 --> 00:09:33,560
假设现在有这么一张水彩画

196
00:09:33,560 --> 00:09:35,160
然后点击滤镜

197
00:09:35,160 --> 00:09:37,480
滤镜里面的模糊

198
00:09:37,480 --> 00:09:39,520
对，模糊里面用的非常多

199
00:09:39,520 --> 00:09:41,080
首先就是高斯模糊

200
00:09:41,080 --> 00:09:45,440
高斯模糊就是卷积核是符合高斯的分布

201
00:09:45,440 --> 00:09:49,680
而这里面平均的半径就是卷积核的大小

202
00:09:49,680 --> 00:09:51,880
可以看到通过这里面的空字拖动

203
00:09:51,880 --> 00:09:54,200
可以控制卷积核的大小

204
00:09:54,200 --> 00:09:56,560
卷积核的大小作用于整个图片

205
00:09:56,560 --> 00:09:59,480
就使得图片越来越模糊

206
00:09:59,480 --> 00:10:01,520
像里面的很多滤镜的方式

207
00:10:01,520 --> 00:10:03,920
就使用了不同的卷积的组成方式

208
00:10:03,920 --> 00:10:06,840
去对图片产生作用

209
00:10:06,840 --> 00:10:09,640
好不容易终于讲完了自己不熟悉

210
00:10:09,640 --> 00:10:12,280
或者不是说非常理解的概念给大家

211
00:10:12,280 --> 00:10:14,040
说实话实在惭愧

212
00:10:14,040 --> 00:10:16,680
下面又回到了我最熟悉的概念

213
00:10:16,680 --> 00:10:18,240
还是系统的优化

214
00:10:18,240 --> 00:10:19,840
还有算法的优化

215
00:10:19,840 --> 00:10:22,120
也没有办法给大家讲得很透彻

216
00:10:22,120 --> 00:10:23,680
所以希望大家能够谅解

217
00:10:23,680 --> 00:10:26,160
简单的听一听或者不要听就行了

218
00:10:26,280 --> 00:10:29,720
下面看一下卷积对于 Tensor 的一个优化

219
00:10:29,720 --> 00:10:31,240
首先可以看到了张量

220
00:10:31,240 --> 00:10:32,640
这个 Tensor 就张量

221
00:10:32,640 --> 00:10:33,880
张量在内存里面

222
00:10:33,880 --> 00:10:39,080
一般现在假设以 NHWC 这种方式作为一个布局

223
00:10:39,080 --> 00:10:42,120
可以看到下面这一扎公式

224
00:10:42,120 --> 00:10:45,720
就是卷积对于张量的一种运算

225
00:10:45,720 --> 00:10:48,960
可以看到这边有非常多的 for

226
00:10:48,960 --> 00:10:52,320
逐个的看看这些 for 有什幺作用

227
00:10:52,320 --> 00:10:55,480
首先有外层三层的 for

228
00:10:55,520 --> 00:10:58,120
里面又有三层的 for

229
00:10:58,120 --> 00:11:01,960
外面的三层的 for 就是对于 nhw 里面的 h

230
00:11:01,960 --> 00:11:04,800
对于 h 的这个信道进行遍历

231
00:11:04,800 --> 00:11:06,840
然后对于 w 这个信道进行遍历

232
00:11:06,840 --> 00:11:10,160
最后对于 c 的信道进行遍历

233
00:11:10,160 --> 00:11:12,920
最后才拿到第一个数据 c

234
00:11:12,920 --> 00:11:14,400
0h 0w 0c

235
00:11:14,400 --> 00:11:15,840
然后把它赋一个值

236
00:11:15,840 --> 00:11:18,440
接着把这个值

237
00:11:18,440 --> 00:11:19,960
因为拿到内存的地址

238
00:11:19,960 --> 00:11:21,760
内存的地址里面存的是什幺字

239
00:11:21,760 --> 00:11:22,720
是不确定

240
00:11:22,720 --> 00:11:24,640
所以首先对它进行赋值

241
00:11:24,640 --> 00:11:27,280
然后有三个内层的 for

242
00:11:27,280 --> 00:11:30,840
三个内层的 for 就是卷积核了

243
00:11:30,840 --> 00:11:33,040
这个卷积核就是 Kernel 的 h

244
00:11:33,040 --> 00:11:34,160
Kernel 的 w

245
00:11:34,160 --> 00:11:36,360
还有 input channel

246
00:11:36,360 --> 00:11:39,040
通过这种方式对它进行一个组织

247
00:11:39,040 --> 00:11:44,000
然后就是刚才的那条求和公式里面的进行一个计算

248
00:11:44,000 --> 00:11:50,680
具体就是这里面卷积核跟这个原始的图片进行加权求和

249
00:11:50,680 --> 00:11:52,200
加权求和之后

250
00:11:52,240 --> 00:11:56,360
就给 c[0h][0w][0c]进行计算

251
00:11:56,360 --> 00:12:01,080
可以看到一个简单的卷积的数学计算是非常的复杂

252
00:12:01,080 --> 00:12:03,040
有非常多的嵌套的循环

253
00:12:03,040 --> 00:12:06,960
为了去让整个嵌套的循环没有那么深

254
00:12:06,960 --> 00:12:10,080
会做很多的循环的优化

255
00:12:10,080 --> 00:12:13,440
循环的展开、分块、重排、融合、拆分

256
00:12:13,440 --> 00:12:15,280
那这些所有的概念

257
00:12:15,280 --> 00:12:19,920
在之前 AI 编译器里面其实是讲过

258
00:12:19,960 --> 00:12:21,960
在 AI 编译器后端优化

259
00:12:21,960 --> 00:12:23,080
就算子的优化

260
00:12:23,080 --> 00:12:26,680
循环的优化里面给大家详细的去介绍过

261
00:12:26,680 --> 00:12:28,720
但是这里面是推理引擎的概念

262
00:12:28,920 --> 00:12:32,520
所以不需要通过编译器或者不需要引入编译器的概念

263
00:12:32,520 --> 00:12:34,560
直接用人工的方式

264
00:12:34,560 --> 00:12:37,960
用手排的方式对循环进行展开

265
00:12:37,960 --> 00:12:40,520
这也是 Kernel 优化工程师所做的工作

266
00:12:40,520 --> 00:12:42,480
另外还有指令的优化

267
00:12:42,480 --> 00:12:44,480
对数据进行向量化

268
00:12:44,480 --> 00:12:47,120
对数据进行张量化

269
00:12:47,120 --> 00:12:49,200
这些概念也是比较好理解

270
00:12:49,240 --> 00:12:52,960
现在假设 C 只有四个通道

271
00:12:52,960 --> 00:12:57,040
这个时候就可以完全把它进行一个向量化的操作

272
00:12:57,040 --> 00:13:01,680
不用每次都执行四次相关的累加操作

273
00:13:01,680 --> 00:13:04,320
下面最后还有存储的优化

274
00:13:04,320 --> 00:13:07,120
存储优化主要是包括访存的延迟

275
00:13:07,120 --> 00:13:09,000
还有存储的分配

276
00:13:09,000 --> 00:13:11,520
可以往上面这一坨公式里面看到

277
00:13:11,520 --> 00:13:15,440
这里面其实有大量的访问内存的方式

278
00:13:15,440 --> 00:13:18,120
怎幺对它进行优化是一个很大的概念

279
00:13:18,120 --> 00:13:22,280
这些写 Kernel 的工程师非常之在行

280
00:13:22,280 --> 00:13:23,800
那更多相关的操作

281
00:13:23,800 --> 00:13:26,760
更多相关的原理也可以去到我之前讲到

282
00:13:26,760 --> 00:13:29,520
AI 编译器后端优化里面的相关的内容

283
00:13:29,520 --> 00:13:33,120
这里面就不会介绍太多相关的原理知识

284
00:13:33,120 --> 00:13:35,440
今天的内容确实太不专业了

285
00:13:35,440 --> 00:13:36,240
就到这里为止

286
00:13:36,240 --> 00:13:36,840
谢谢各位

287
00:13:36,840 --> 00:13:37,600
拜了个拜!

288
00:13:38,520 --> 00:13:39,320
卷的不行了

289
00:13:39,320 --> 00:13:40,160
卷的不行了

290
00:13:40,160 --> 00:13:42,000
记得一键三连加关注哦

291
00:13:42,000 --> 00:13:45,240
所有的内容都会开源在下面这条链接里面

292
00:13:45,240 --> 00:13:46,520
拜拜!

