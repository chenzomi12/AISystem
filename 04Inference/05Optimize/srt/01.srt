1
00:00:00,028 --> 00:00:04,193
字幕生成：qiaokai 字幕校对：A 是传奇

2
00:00:05,385 --> 00:00:06,880
Hello，大家好

3
00:00:06,880 --> 00:00:07,720
我是 ZOMI

4
00:00:07,720 --> 00:00:09,520
今天来到一个新的内容

5
00:00:09,800 --> 00:00:13,360
虽然这个内容还是在推理引擎下面

6
00:00:13,360 --> 00:00:16,720
但是来到模型转换和优化这个内容里面

7
00:00:16,720 --> 00:00:20,040
本来我在这个内容只是讲讲模型转换优化

8
00:00:20,040 --> 00:00:21,400
接下来要讲哪些内容

9
00:00:21,400 --> 00:00:24,640
后来梳理着就变成了模型转换优化

10
00:00:24,640 --> 00:00:27,360
一个整体的架构和它的流程

11
00:00:27,880 --> 00:00:31,760
在接下来在正式进入模型转换优化的这个内容里面

12
00:00:31,760 --> 00:00:33,720
我想跟大家一起去回顾一下

13
00:00:33,720 --> 00:00:35,040
之前讲的一个内容

14
00:00:35,040 --> 00:00:37,160
之前讲的推理系统的介绍的时候

15
00:00:37,320 --> 00:00:39,680
其实更关注的是推理系统的架构

16
00:00:39,680 --> 00:00:41,520
还有推理引擎的架构

17
00:00:41,520 --> 00:00:43,840
接着了解了整个大的概念之后

18
00:00:44,240 --> 00:00:47,320
就开始来到了模型的小型化

19
00:00:47,320 --> 00:00:49,240
或者叫做模型轻量化

20
00:00:49,240 --> 00:00:51,840
主要是针对 CNN 和 Transformer 两个结构

21
00:00:51,840 --> 00:00:54,440
进行一个小型化或者轻量化的工作

22
00:00:54,440 --> 00:00:56,560
然后在离线转换模块的时候

23
00:00:56,880 --> 00:00:59,680
去讲了很多离线的优化压缩

24
00:00:59,680 --> 00:01:01,640
特别是低比特的量化模型剪子

25
00:01:01,640 --> 00:01:05,080
知识蒸馏这些常见的模型压缩的功能

26
00:01:06,080 --> 00:01:07,680
在第四和第五节的内容

27
00:01:07,840 --> 00:01:10,200
更多的是聚焦于推理引擎

28
00:01:10,200 --> 00:01:12,560
真正里面的一些内核的模块

29
00:01:12,840 --> 00:01:15,640
模型转换也是作为其中一个很重要的模块

30
00:01:15,640 --> 00:01:18,680
这里面将会分开三个内容来给大家介绍

31
00:01:18,680 --> 00:01:22,440
第一个就是模型转换和优化的整体的架构和流程

32
00:01:22,880 --> 00:01:25,240
接着去看一下模型格式的转换

33
00:01:25,280 --> 00:01:28,120
最后看一下模型离线的优化

34
00:01:28,560 --> 00:01:30,600
经过了格式转换还有离线优化之后

35
00:01:30,720 --> 00:01:32,680
真正的就会进入到第五节

36
00:01:32,680 --> 00:01:34,400
Runtime 和在线的优化

37
00:01:35,160 --> 00:01:36,560
而第四个和第五个内容

38
00:01:36,680 --> 00:01:40,600
也是推理引擎一个非常重要的组成部分

39
00:01:41,440 --> 00:01:43,600
回到推理引擎架构里面

40
00:01:43,600 --> 00:01:47,400
今天更多是聚焦于模型转换工具

41
00:01:47,640 --> 00:01:49,680
这一块非常重要

42
00:01:49,680 --> 00:01:52,160
没有这一块没有办法去衔接后面

43
00:01:52,160 --> 00:01:53,280
Runtime 和 Kernel

44
00:01:53,800 --> 00:01:56,320
这个模块最终要有两个功能

45
00:01:56,320 --> 00:01:58,520
一个功能就是模型的格式转换

46
00:01:58,520 --> 00:02:01,160
第二个就是计算图的优化

47
00:02:01,160 --> 00:02:03,640
这里面我用了一条虚线去代表

48
00:02:03,640 --> 00:02:05,440
上面就是模型转换

49
00:02:05,440 --> 00:02:07,480
下面就是图优化

50
00:02:09,120 --> 00:02:10,480
在接下来的内容里面

51
00:02:10,480 --> 00:02:12,680
ZOMI 想跟大家一起去探讨一下

52
00:02:12,680 --> 00:02:16,120
转换模块的挑战和相关的目标

53
00:02:16,400 --> 00:02:18,440
首先看一下转换模块

54
00:02:18,600 --> 00:02:20,680
Converter 它其实有非常大的挑战

55
00:02:20,680 --> 00:02:22,840
这里面总结了 4 条

56
00:02:23,800 --> 00:02:25,960
第一条就是 AI 模型本身

57
00:02:26,080 --> 00:02:29,000
其实有非常多的算子

58
00:02:29,240 --> 00:02:31,400
推理引擎需要用有限的算子

59
00:02:31,400 --> 00:02:34,520
来实现不同 AI 框架所需要的算子

60
00:02:34,520 --> 00:02:37,160
因为推理引擎它需要对接很多种

61
00:02:37,160 --> 00:02:40,320
不同 AI 框架训练出来的网络模型

62
00:02:40,320 --> 00:02:43,360
不同的 AI 框架它的算子有自己的定义

63
00:02:43,360 --> 00:02:46,920
接着第二点就是有非常多的框架

64
00:02:46,920 --> 00:02:50,120
不同的框架有自己的一个模型的文档格式

65
00:02:50,520 --> 00:02:52,480
第三点就是推理引擎

66
00:02:52,600 --> 00:02:55,320
需要支持很多主流的网络模型的结构

67
00:02:55,320 --> 00:02:58,360
包括 CNN GNN 还有 Transformer

68
00:02:58,360 --> 00:03:01,920
最后一点就是相关的 DSL 的一些特性

69
00:03:02,040 --> 00:03:02,920
Domain Specific

70
00:03:02,920 --> 00:03:06,760
就是一个深度学习专用领域的一些特性

71
00:03:07,120 --> 00:03:09,880
需要支持动态设备的任意维度的输出

72
00:03:10,000 --> 00:03:11,600
还有单控制流的模型

73
00:03:13,040 --> 00:03:16,320
下面逐个的去展开看一看

74
00:03:16,520 --> 00:03:19,240
AI 模型本身包含非常多的算子

75
00:03:19,400 --> 00:03:20,280
可以看一下

76
00:03:20,440 --> 00:03:22,280
下面这个图就是总结的一个图

77
00:03:22,280 --> 00:03:23,520
虽然 Caffe 现在用的很少

78
00:03:23,520 --> 00:03:25,640
但是看一下 TensorFlow 和 PyTorch

79
00:03:25,640 --> 00:03:26,520
这两个框架

80
00:03:26,520 --> 00:03:27,960
实际上这两个框架 Self

81
00:03:28,080 --> 00:03:31,640
就是它自身本来具有非常多的算子

82
00:03:31,640 --> 00:03:32,720
不同的 AI 框架

83
00:03:32,720 --> 00:03:34,360
算子的冲突度非常的高

84
00:03:34,360 --> 00:03:37,640
但是这些算子确实定义也不太一样

85
00:03:37,640 --> 00:03:38,920
例如 PyTorch 的 Padding

86
00:03:38,920 --> 00:03:40,040
跟 TensorFlow 的 Padding

87
00:03:40,040 --> 00:03:41,440
它们虽然都叫 Padding

88
00:03:41,440 --> 00:03:45,000
但是它们 Pad 的一个方式和方向也是不同

89
00:03:45,440 --> 00:03:47,080
第二个点就是推理引擎

90
00:03:47,200 --> 00:03:49,440
虽然接下来要实现的推理引擎

91
00:03:49,440 --> 00:03:52,160
不可能把每一个框架这么多算子

92
00:03:52,240 --> 00:03:53,120
都实现一遍

93
00:03:53,120 --> 00:03:55,680
所以用有限的算子去对接

94
00:03:55,680 --> 00:03:57,480
或者实现不同 AI 框架

95
00:03:57,480 --> 00:03:59,240
训练出来的网络模型

96
00:04:00,840 --> 00:04:02,000
往下看一下

97
00:04:02,000 --> 00:04:05,120
其实历经了非常多的不同的框架

98
00:04:05,120 --> 00:04:07,520
包括 TensorFlow 有 1.0 跟 2.0

99
00:04:07,520 --> 00:04:09,760
PyTorch 有之前的 1.多版本

100
00:04:09,760 --> 00:04:11,200
到现在的 2.多版本

101
00:04:11,200 --> 00:04:12,920
所以说不同的 AI 框架

102
00:04:12,920 --> 00:04:14,400
训练出来的网络模型

103
00:04:14,400 --> 00:04:15,320
还有算子

104
00:04:15,320 --> 00:04:17,080
它之间是有差异

105
00:04:17,080 --> 00:04:18,600
而且不同版本之间

106
00:04:18,600 --> 00:04:20,560
它又会增加不同的算子

107
00:04:20,680 --> 00:04:21,520
不同的 AI 框架

108
00:04:21,520 --> 00:04:24,000
它的模型转换格式也是不一样

109
00:04:24,000 --> 00:04:26,160
所以说会遇到非常多

110
00:04:26,160 --> 00:04:28,120
工程性的问题

111
00:04:28,960 --> 00:04:31,320
针对上面一二三四个问题

112
00:04:31,320 --> 00:04:33,960
其实推理引擎都要逐一的去解决

113
00:04:33,960 --> 00:04:36,240
包括在思考整个架构的时候

114
00:04:36,240 --> 00:04:37,160
面对这些问题

115
00:04:37,160 --> 00:04:39,520
应该怎幺去设计好架构

116
00:04:39,520 --> 00:04:41,520
才能够让整个模块

117
00:04:41,520 --> 00:04:44,640
或者让整个推理引擎做得更好

118
00:04:45,760 --> 00:04:48,520
第一个点是因为算子非常的多

119
00:04:48,520 --> 00:04:49,280
不同的 AI 框架

120
00:04:49,280 --> 00:04:51,160
有不同的算子的格式的定义

121
00:04:51,160 --> 00:04:52,680
于是这里面就要求

122
00:04:52,680 --> 00:04:55,160
推理引擎需要有自己的算子的定义

123
00:04:55,160 --> 00:04:57,680
还有对应的格式

124
00:04:57,680 --> 00:04:58,520
有了这个之后

125
00:04:58,520 --> 00:04:59,840
就可以去对接到

126
00:04:59,840 --> 00:05:02,720
不同的 AI 框架的算子层了

127
00:05:03,720 --> 00:05:04,600
针对第二个问题

128
00:05:04,600 --> 00:05:07,280
需要支持非常多不同的 AI 框架

129
00:05:07,280 --> 00:05:10,280
每个 AI 框架都有自己的文档格式定义

130
00:05:10,280 --> 00:05:13,600
于是这里面就要求一个推理引擎

131
00:05:13,600 --> 00:05:16,800
需要有自己自定义的计算图的 IR

132
00:05:16,840 --> 00:05:20,000
去对接到不同的 AI 框架里面的计算图

133
00:05:21,080 --> 00:05:24,120
第三点要支持 CNN GNN Transomer 等

134
00:05:24,120 --> 00:05:25,640
主流的网络模型结构

135
00:05:25,640 --> 00:05:27,560
这个时候对推理引擎

136
00:05:27,560 --> 00:05:29,280
就要求有丰富的 Demo

137
00:05:29,280 --> 00:05:30,360
还有 Benchmark

138
00:05:30,360 --> 00:05:31,160
有了 Benchmark

139
00:05:31,160 --> 00:05:32,760
就可以提供主流模型

140
00:05:32,760 --> 00:05:34,360
性能和功能的基准

141
00:05:34,360 --> 00:05:36,240
来保证来去看护

142
00:05:36,240 --> 00:05:39,240
整个推理引擎的可用性

143
00:05:40,520 --> 00:05:42,280
最后一步是因为深度学习

144
00:05:42,280 --> 00:05:43,920
有它的特殊性

145
00:05:43,920 --> 00:05:45,600
需要支持动态的 Shape

146
00:05:45,640 --> 00:05:47,160
支持任意维度的输出

147
00:05:47,520 --> 00:05:48,760
支持控制流

148
00:05:48,760 --> 00:05:50,320
于是要求推理引擎

149
00:05:50,320 --> 00:05:52,520
要支持非常好的可扩展性

150
00:05:52,520 --> 00:05:55,240
还有 AI 的比较重要的一些相关的特性

151
00:05:55,240 --> 00:05:56,040
例如动态 Shape

152
00:05:57,120 --> 00:05:58,400
针对不同的任务

153
00:05:58,400 --> 00:06:00,680
在 CV 里面例如检测分割分类

154
00:06:00,680 --> 00:06:01,600
在 NLP 里面

155
00:06:01,600 --> 00:06:02,960
Mask

156
00:06:02,960 --> 00:06:06,800
这些需要做大量的集成测试和验证

157
00:06:06,800 --> 00:06:09,240
保证确实能够处理很多

158
00:06:09,240 --> 00:06:12,040
不同类型的网络模型

159
00:06:12,040 --> 00:06:13,240
特别是像动态 Shape

160
00:06:13,400 --> 00:06:15,000
可能在分类里面是没有

161
00:06:15,040 --> 00:06:16,720
但是当遇到一些

162
00:06:17,040 --> 00:06:19,040
但是当遇到一些分割的场景

163
00:06:19,040 --> 00:06:20,880
可能会用到很多的动态 Shape

164
00:06:22,960 --> 00:06:24,640
把其他 AI 框架的网络模型

165
00:06:24,800 --> 00:06:27,880
转换成为自己推理引擎的一个网络模型

166
00:06:28,000 --> 00:06:30,120
接着就需要对网络模型

167
00:06:30,120 --> 00:06:32,200
或者计算图进行优化

168
00:06:33,280 --> 00:06:34,120
而在优化之前

169
00:06:34,240 --> 00:06:35,480
需要分析一下

170
00:06:35,480 --> 00:06:37,920
到底需要优化哪些内容

171
00:06:37,920 --> 00:06:40,040
在计算图里面到底有哪些冗余

172
00:06:40,040 --> 00:06:42,800
才能更好的执行一个优化

173
00:06:42,800 --> 00:06:44,680
所以首先来分析一下

174
00:06:44,720 --> 00:06:45,640
或者总结一下

175
00:06:45,640 --> 00:06:47,560
到底有哪些优化的挑战

176
00:06:47,560 --> 00:06:49,480
这里面 ZOMI 总结了 4 条

177
00:06:49,480 --> 00:06:51,280
第一条是结构的冗余

178
00:06:51,280 --> 00:06:52,640
第二条是精度的冗余

179
00:06:52,760 --> 00:06:54,240
第三条是算法的冗余

180
00:06:54,240 --> 00:06:56,120
第四条是读写的冗余

181
00:06:56,120 --> 00:06:58,800
下面逐条的来去看一看

182
00:06:59,800 --> 00:07:01,960
首先是结构的冗余

183
00:07:01,960 --> 00:07:04,240
结构的冗余其实在 AI 编译器里面

184
00:07:04,680 --> 00:07:07,360
大量的去给大家普及过了

185
00:07:07,680 --> 00:07:09,000
这里面确实有很多

186
00:07:09,000 --> 00:07:11,280
跟 AI 编译器相关的一些内容

187
00:07:11,280 --> 00:07:13,200
神经网络模型里面

188
00:07:13,240 --> 00:07:14,600
有非常大量

189
00:07:15,280 --> 00:07:17,360
没有效果或者没有用的计算节点

190
00:07:17,360 --> 00:07:19,200
还有很多重复计算的词图

191
00:07:19,200 --> 00:07:20,480
还有相同的结构

192
00:07:20,920 --> 00:07:22,760
都可以在保留相同

193
00:07:22,760 --> 00:07:24,600
计算图语义的情况下

194
00:07:25,120 --> 00:07:27,600
去去掉这些冗余的结构

195
00:07:27,600 --> 00:07:30,600
说白了就是我怎么改这个图都好

196
00:07:30,600 --> 00:07:32,880
我保证计算图的语义

197
00:07:33,040 --> 00:07:34,440
它的执行的方式

198
00:07:34,440 --> 00:07:37,080
跟用户的期望是相同

199
00:07:38,000 --> 00:07:38,720
所以就引出了

200
00:07:38,720 --> 00:07:40,600
在计算图优化的过程当中

201
00:07:40,600 --> 00:07:42,040
需要执行一些算子的融合

202
00:07:42,360 --> 00:07:43,120
算子的替换

203
00:07:43,440 --> 00:07:46,960
常量的折叠等常用的优化的功能

204
00:07:46,960 --> 00:07:49,640
去对结构冗余进行优化

205
00:07:51,240 --> 00:07:54,040
第二个点就是精度冗余

206
00:07:54,040 --> 00:07:55,520
实际上在推进引擎

207
00:07:55,720 --> 00:07:58,480
大部分存的数据都是张量

208
00:07:58,680 --> 00:08:02,080
一般以 FP32 浮点数来去一个存储

209
00:08:02,080 --> 00:08:03,960
但是在某些情况下

210
00:08:03,960 --> 00:08:04,640
特别是分类

211
00:08:04,800 --> 00:08:05,880
确实可以压到

212
00:08:05,880 --> 00:08:08,760
FP16 和 INT8 甚至更低比特

213
00:08:09,040 --> 00:08:11,040
数据中可能存在大量的零

214
00:08:11,040 --> 00:08:12,480
或者重复的数据

215
00:08:13,640 --> 00:08:15,120
这个时候针对精度冗余

216
00:08:15,320 --> 00:08:16,840
确实可以做很多

217
00:08:16,840 --> 00:08:19,120
模型压缩相关的工作

218
00:08:19,720 --> 00:08:20,200
这个功能

219
00:08:20,320 --> 00:08:22,240
其实在上一个内容里面

220
00:08:22,240 --> 00:08:23,640
给大家详细的介绍过

221
00:08:23,640 --> 00:08:24,800
做一些低比特的量化

222
00:08:24,920 --> 00:08:26,400
剪枝和蒸馏

223
00:08:28,040 --> 00:08:30,880
第三个就是算法的冗余

224
00:08:31,400 --> 00:08:33,880
算法的冗余听上去有点虚

225
00:08:33,880 --> 00:08:36,280
就是算子或者 Kernel 层面实现的算法

226
00:08:36,280 --> 00:08:39,240
本身就存在着计算的冗余

227
00:08:40,120 --> 00:08:41,120
什幺叫计算冗余

228
00:08:42,000 --> 00:08:43,440
这里面的 ZOMI 就举了一个

229
00:08:43,440 --> 00:08:44,520
比较明确的例子

230
00:08:44,520 --> 00:08:46,800
做一个均值模糊的滑窗

231
00:08:46,800 --> 00:08:49,040
还有拉普拉斯的一个滑窗的时候

232
00:08:49,600 --> 00:08:51,800
实际上这里面都是通过一个卷积的方式

233
00:08:51,800 --> 00:08:52,400
去实现

234
00:08:52,400 --> 00:08:54,800
只是这个卷积核比较特殊

235
00:08:54,800 --> 00:08:56,880
均值卷积可能它的卷积核

236
00:08:56,880 --> 00:08:59,320
是通过高斯定理来去实现

237
00:08:59,320 --> 00:09:00,520
拉普拉斯的滑窗

238
00:09:00,680 --> 00:09:02,960
就是通过拉普拉斯定理来去实现

239
00:09:02,960 --> 00:09:04,680
他们的计算原理都是一样

240
00:09:04,680 --> 00:09:07,880
这个时候就存在着计算的冗余了

241
00:09:08,320 --> 00:09:09,880
因为存在计算的冗余

242
00:09:09,880 --> 00:09:12,200
于是就要求推理引擎

243
00:09:12,400 --> 00:09:13,520
需要统一算子

244
00:09:13,520 --> 00:09:15,360
还有计算图的表达

245
00:09:15,360 --> 00:09:16,920
统一了算子计算图的表达

246
00:09:17,320 --> 00:09:20,000
就可以针对发现的计算冗余

247
00:09:20,000 --> 00:09:21,640
进行一个统一

248
00:09:21,640 --> 00:09:23,040
然后整体去提升

249
00:09:23,040 --> 00:09:24,520
Kernel 的泛化性

250
00:09:25,760 --> 00:09:28,760
第 4 点就是读写的冗余

251
00:09:28,760 --> 00:09:30,360
在计算场景当中

252
00:09:30,520 --> 00:09:32,320
确实会有大量

253
00:09:32,320 --> 00:09:34,360
存在大量的内存访问的问题

254
00:09:34,520 --> 00:09:36,040
内存是不是连续

255
00:09:36,160 --> 00:09:38,040
要不要进行大量的内存访问

256
00:09:38,040 --> 00:09:40,400
都会是一个很严重的挑战

257
00:09:41,280 --> 00:09:42,600
针对读写用于这个问题

258
00:09:42,920 --> 00:09:44,960
于是在优化模块里面

259
00:09:45,080 --> 00:09:47,160
就需要进行一些数据的排布的优化

260
00:09:47,160 --> 00:09:49,160
还有内存分配的优化

261
00:09:50,280 --> 00:09:51,360
了解完转化模块

262
00:09:51,360 --> 00:09:53,680
优化模块遇到的一些问题和挑战

263
00:09:53,680 --> 00:09:54,720
带着这些疑问

264
00:09:54,720 --> 00:09:55,800
或者带着这些目标

265
00:09:56,000 --> 00:09:57,800
就需要去设计好

266
00:09:57,800 --> 00:10:00,240
推理引擎整个离线模块的架构

267
00:10:00,240 --> 00:10:02,760
还有它的工作流程一些挑战

268
00:10:03,120 --> 00:10:04,280
现在看一下

269
00:10:04,400 --> 00:10:07,040
转化模块的整个架构

270
00:10:07,040 --> 00:10:08,440
直接看下面这个图

271
00:10:08,440 --> 00:10:11,840
转化模块分为一个图的转化

272
00:10:11,840 --> 00:10:13,600
还有图的优化

273
00:10:13,600 --> 00:10:14,800
两大个内容

274
00:10:14,800 --> 00:10:17,280
图的转化首先会遇到非常多

275
00:10:17,280 --> 00:10:18,920
不同的 AI 框架

276
00:10:18,920 --> 00:10:20,560
于是针对每个 AI 框架

277
00:10:20,560 --> 00:10:21,960
确实它有自己的 API

278
00:10:21,960 --> 00:10:23,800
所以不可能通过一个 Converter

279
00:10:23,800 --> 00:10:26,360
能够把它所有的 AI 框架都转换过来

280
00:10:26,360 --> 00:10:28,840
于是就会针对 MindSpore 这个 AI 框架

281
00:10:28,840 --> 00:10:30,880
可能有 MindSpore 单独的 Converter

282
00:10:30,880 --> 00:10:33,080
Pytorch 一般都会 export 到 onix

283
00:10:33,200 --> 00:10:36,160
针对 ONNX 有自己独立的 Converter

284
00:10:36,160 --> 00:10:37,680
通过不同的 Converter

285
00:10:37,680 --> 00:10:39,960
都统一转换成为自己推理引擎

286
00:10:39,960 --> 00:10:41,360
AI 中间表达

287
00:10:41,360 --> 00:10:42,960
有了这个中间表达了

288
00:10:42,960 --> 00:10:44,880
后面在做图优化的时候

289
00:10:44,880 --> 00:10:46,000
都是基于这个 AI

290
00:10:46,000 --> 00:10:47,840
都是基于自己定义的计算图

291
00:10:47,840 --> 00:10:51,160
进行一个改写或者修改

292
00:10:52,160 --> 00:10:54,800
AI 上面就是模型转换

293
00:10:54,800 --> 00:10:56,000
或者格式转换

294
00:10:56,000 --> 00:10:59,320
AI 下面就是图优化的模块

295
00:10:59,760 --> 00:11:00,800
在图这个模块

296
00:11:00,880 --> 00:11:02,640
要做很多的算子融合

297
00:11:03,120 --> 00:11:05,120
算子替换内存重排

298
00:11:05,120 --> 00:11:06,280
数据重排

299
00:11:06,280 --> 00:11:07,560
还有内存分配

300
00:11:07,560 --> 00:11:09,080
计算图优化的这个功能

301
00:11:09,240 --> 00:11:11,160
其实不是说非常的新

302
00:11:11,160 --> 00:11:13,120
如果大家看过了解过

303
00:11:13,120 --> 00:11:14,440
AI 编译器这个系列

304
00:11:14,600 --> 00:11:16,520
可以发现这里面有很多功能

305
00:11:16,520 --> 00:11:19,000
都类似于 AI 编译器的前端优化

306
00:11:19,000 --> 00:11:22,360
对这里面其实有很多功能可以复现

307
00:11:22,360 --> 00:11:24,160
而现在公司

308
00:11:24,160 --> 00:11:26,160
其实有一部分专家就觉得

309
00:11:26,160 --> 00:11:27,440
计算图优化这个功能

310
00:11:27,640 --> 00:11:29,480
应该通过编译器来去做

311
00:11:29,480 --> 00:11:31,720
所以希望另一个编译器的课程

312
00:11:31,760 --> 00:11:35,040
但是 ZOMI 觉得其实没有必要做的这么的重

313
00:11:35,480 --> 00:11:37,400
很多时候在推定引擎

314
00:11:37,520 --> 00:11:39,720
其实没有必要去做一个编译器

315
00:11:39,720 --> 00:11:42,040
更多的基于那个 pattern 去优化

316
00:11:42,040 --> 00:11:44,760
更多的基于规则进行优化就好了

317
00:11:44,760 --> 00:11:46,000
没有必要做那幺重

318
00:11:46,000 --> 00:11:47,120
因为一个离线模块

319
00:11:47,240 --> 00:11:48,800
可能它不需要太大

320
00:11:48,800 --> 00:11:51,480
简单的很小的几个 M 可以了

321
00:11:51,480 --> 00:11:53,080
因为推定引擎的 AI

322
00:11:53,080 --> 00:11:55,520
尽可能的设计的比较简单

323
00:11:55,520 --> 00:11:57,440
跟训练的框架是不一样

324
00:11:57,880 --> 00:11:58,760
训练的框架

325
00:11:58,880 --> 00:12:00,560
要考虑的问题非常多

326
00:12:00,600 --> 00:12:02,960
例如有三个特别重要的特性

327
00:12:03,240 --> 00:12:04,600
第一个就是自动微分

328
00:12:04,600 --> 00:12:07,400
第二个就是分布式的并行

329
00:12:07,560 --> 00:12:10,240
第三个点就是静态图和动态图的问题

330
00:12:10,480 --> 00:12:12,160
像现在这些大部分的问题

331
00:12:12,520 --> 00:12:14,760
在推定引擎没有必要考虑的太多

332
00:12:14,760 --> 00:12:16,680
所以没有必要搞一个编译器出来

333
00:12:16,680 --> 00:12:19,320
更多的去做一些 pattern 的修改就好了

334
00:12:19,920 --> 00:12:20,760
废话就不多说

335
00:12:20,760 --> 00:12:24,400
回到整个转化模块的工作流程里面

336
00:12:24,600 --> 00:12:26,080
去看一下刚才的架构图

337
00:12:26,200 --> 00:12:29,080
针对工作流程有什么不一样

338
00:12:29,200 --> 00:12:30,960
左边的这个是转化模块

339
00:12:30,960 --> 00:12:33,840
用了蓝色底来去给大家划分

340
00:12:33,840 --> 00:12:36,320
右边的这个就是优化的模块

341
00:12:36,320 --> 00:12:38,600
用了黄色底去进行划分

342
00:12:38,600 --> 00:12:41,360
可以看到确实也有很多个 Converter

343
00:12:41,360 --> 00:12:43,160
有非常多的转换器

344
00:12:43,520 --> 00:12:44,800
通过不同的转换器

345
00:12:44,800 --> 00:12:48,040
把不同的 AI 框架训练出来的一个网络模型

346
00:12:48,920 --> 00:12:51,000
转换成为推定引擎的 IR

347
00:12:51,000 --> 00:12:52,520
有了推定引擎的 IR 之后

348
00:12:52,600 --> 00:12:54,720
现在就变成一个统一的表达了

349
00:12:54,720 --> 00:12:56,600
于是在做后面的优化

350
00:12:56,720 --> 00:12:58,240
我分开了三段

351
00:12:59,080 --> 00:13:00,920
一段叫做 Pre Optimize

352
00:13:00,920 --> 00:13:03,040
一段叫做正式的 Optimize

353
00:13:03,040 --> 00:13:05,600
最后一段叫 Pos Optimize

354
00:13:05,600 --> 00:13:07,800
三个阶段有三个不同的内容

355
00:13:08,120 --> 00:13:11,640
这个也是 ZOMI 看到很多推定引擎的课程代码

356
00:13:11,640 --> 00:13:12,960
所总结出来

357
00:13:13,840 --> 00:13:17,080
首先会对转化模块传过来的一个计算图

358
00:13:17,080 --> 00:13:18,960
做一些公共表达式的消除

359
00:13:18,960 --> 00:13:19,880
死代码的消除

360
00:13:19,880 --> 00:13:21,240
还有代数简化

361
00:13:21,520 --> 00:13:24,040
常用的代数简化消除的功能

362
00:13:25,200 --> 00:13:26,640
执行一些公共的功能之后

363
00:13:26,800 --> 00:13:28,480
就正式的对计算图

364
00:13:28,480 --> 00:13:31,360
对神经网络的一些知识融进来了

365
00:13:31,560 --> 00:13:33,600
第一点就有算子融合算子替换

366
00:13:33,600 --> 00:13:34,760
还有常量折叠

367
00:13:34,760 --> 00:13:37,440
这个就是最常用的一些方式

368
00:13:37,440 --> 00:13:39,640
也是作为中间优化层

369
00:13:40,840 --> 00:13:42,520
来到了 Pos Optimize 这个阶段

370
00:13:42,640 --> 00:13:45,520
其实代表计算图基本上能换的就换了

371
00:13:46,040 --> 00:13:48,400
更多的是对数据的格式转换

372
00:13:48,480 --> 00:13:50,880
NCHW 到 NHWC 这种

373
00:13:50,880 --> 00:13:52,400
还有内存的布局计算

374
00:13:52,400 --> 00:13:55,120
另外会把一些重复的算子把它合并掉

375
00:13:55,760 --> 00:13:58,080
到这个阶段已经没有太多了

376
00:13:58,160 --> 00:14:01,040
更多的在前面这两个阶段已经把它干完了

377
00:14:01,040 --> 00:14:02,280
在最后一个阶段

378
00:14:02,600 --> 00:14:03,880
主要是对数据

379
00:14:03,880 --> 00:14:07,160
对内存进行一些管理和预管理的工作

380
00:14:09,160 --> 00:14:10,760
好了今天的内容就这幺多

381
00:14:10,760 --> 00:14:11,760
回顾一下

382
00:14:11,760 --> 00:14:14,640
这里面跟大家一起汇报了一下

383
00:14:14,640 --> 00:14:17,440
模型转换的遇到了一些挑战和目标

384
00:14:17,680 --> 00:14:21,360
接着看了一下计算图优化的一些挑战和目标

385
00:14:21,360 --> 00:14:22,600
带着这些挑战和目标

386
00:14:22,720 --> 00:14:25,600
就去设计了推理引擎的整体的架构

387
00:14:25,640 --> 00:14:28,760
然后把架构图变成工作流程图

388
00:14:28,760 --> 00:14:31,040
把每一个模块都梳理清楚

389
00:14:32,440 --> 00:14:34,080
卷的不行了

390
00:14:34,080 --> 00:14:35,920
记得一键三连加关注哦

391
00:14:35,920 --> 00:14:38,920
所有的内容都会开源在下面这条链接里面

392
00:14:39,520 --> 00:14:40,240
拜了个拜

